{"type": "text", "bbox": [49, 94, 288, 149], "res": [{"text": "In this work, we use a total number of 30 scene models", "confidence": 0.9959679245948792, "text_region": [[59.0, 92.0], [289.0, 92.0], [289.0, 105.0], [59.0, 105.0]]}, {"text": "which are all obtained from the Internet. However, most of", "confidence": 0.9910399913787842, "text_region": [[48.0, 104.0], [288.0, 104.0], [288.0, 117.0], [48.0, 117.0]]}, {"text": "these models are not free. Therefore, we are not allowed to", "confidence": 0.9794589281082153, "text_region": [[48.0, 116.0], [289.0, 116.0], [289.0, 130.0], [48.0, 130.0]]}, {"text": "share the models themselves. Instead, we list the models we", "confidence": 0.9790524840354919, "text_region": [[47.0, 127.0], [289.0, 128.0], [289.0, 142.0], [47.0, 141.0]]}, {"text": "use and their links in Tab. 6.", "confidence": 0.9730383157730103, "text_region": [[48.0, 140.0], [164.0, 140.0], [164.0, 152.0], [48.0, 152.0]]}], "img_idx": 0, "score": 0.9830024838447571}
{"type": "text", "bbox": [49, 538, 287, 665], "res": [{"text": "As we are encouraging case-sensitive (also with punctua-", "confidence": 0.9885888695716858, "text_region": [[60.0, 537.0], [287.0, 537.0], [287.0, 549.0], [60.0, 549.0]]}, {"text": "tion marks) evaluation for scene text recognition, we would", "confidence": 0.9891823530197144, "text_region": [[48.0, 548.0], [289.0, 548.0], [289.0, 561.0], [48.0, 561.0]]}, {"text": "like to provide benchmark performances on those widely", "confidence": 0.9979428648948669, "text_region": [[48.0, 560.0], [288.0, 560.0], [288.0, 573.0], [48.0, 573.0]]}, {"text": "used datasets.  We evaluate two implementations of the", "confidence": 0.9836816191673279, "text_region": [[48.0, 572.0], [289.0, 572.0], [289.0, 585.0], [48.0, 585.0]]}, {"text": "ASTER models, by Long et al.* and Baek eI al respec-", "confidence": 0.9382208585739136, "text_region": [[48.0, 584.0], [288.0, 584.0], [288.0, 598.0], [48.0, 598.0]]}, {"text": "tively. Results are summarized in Tab. 7.", "confidence": 0.9813919067382812, "text_region": [[47.0, 595.0], [214.0, 595.0], [214.0, 608.0], [47.0, 608.0]]}, {"text": "The two benchmark implementations perform compara-", "confidence": 0.998789370059967, "text_region": [[59.0, 606.0], [287.0, 608.0], [287.0, 622.0], [59.0, 620.0]]}, {"text": "bly, with Baek\u2019s better on straight text and Long\u2019s better at", "confidence": 0.9784508347511292, "text_region": [[48.0, 619.0], [289.0, 619.0], [289.0, 633.0], [48.0, 633.0]]}, {"text": "curved text. Compared with evaluation with fower case +", "confidence": 0.9729902744293213, "text_region": [[48.0, 632.0], [289.0, 632.0], [289.0, 644.0], [48.0, 644.0]]}, {"text": "digirs, the performance drops considerably for both models", "confidence": 0.9684116840362549, "text_region": [[48.0, 643.0], [288.0, 643.0], [288.0, 656.0], [48.0, 656.0]]}, {"text": "when we evaluate with all symbols. These results indicate", "confidence": 0.9921061992645264, "text_region": [[48.0, 654.0], [288.0, 654.0], [288.0, 667.0], [48.0, 667.0]]}], "img_idx": 0, "score": 0.9801274538040161}
{"type": "text", "bbox": [48, 199, 288, 457], "res": [{"text": "During the experiments of scene text recognition for En-", "confidence": 0.9973698258399963, "text_region": [[59.0, 197.0], [287.0, 197.0], [287.0, 211.0], [59.0, 211.0]]}, {"text": "glish scripts, we notice that among the most widely used", "confidence": 0.9984270334243774, "text_region": [[47.0, 208.0], [288.0, 209.0], [288.0, 223.0], [47.0, 222.0]]}, {"text": "benchmark datasets, several have incomplete annotations.", "confidence": 0.9904852509498596, "text_region": [[47.0, 220.0], [288.0, 221.0], [288.0, 235.0], [47.0, 234.0]]}, {"text": "They are IITSK, SVT, SVTP, and CUTE-80. The annota-", "confidence": 0.9885406494140625, "text_region": [[48.0, 233.0], [287.0, 233.0], [287.0, 247.0], [48.0, 247.0]]}, {"text": "tions of these datasets are case-insensitive, and ignore punc-", "confidence": 0.9900558590888977, "text_region": [[47.0, 244.0], [287.0, 245.0], [287.0, 258.0], [47.0, 257.0]]}, {"text": "tuation marks.", "confidence": 0.9966015815734863, "text_region": [[48.0, 257.0], [107.0, 257.0], [107.0, 268.0], [48.0, 268.0]]}, {"text": "The common practice for recent scene text recognition", "confidence": 0.9879217147827148, "text_region": [[60.0, 268.0], [288.0, 268.0], [288.0, 282.0], [60.0, 282.0]]}, {"text": "research is to convert both prediction and ground-truth text", "confidence": 0.9972838163375854, "text_region": [[47.0, 279.0], [287.0, 279.0], [287.0, 292.0], [47.0, 292.0]]}, {"text": "strings to lower-case and then compare them. This means", "confidence": 0.9966235160827637, "text_region": [[48.0, 291.0], [288.0, 291.0], [288.0, 305.0], [48.0, 305.0]]}, {"text": "that the current evaluation is flawed. It ignores letter case", "confidence": 0.9710134863853455, "text_region": [[48.0, 304.0], [289.0, 304.0], [289.0, 318.0], [48.0, 318.0]]}, {"text": "and punctuation marks which are crucial to the understand-", "confidence": 0.9978174567222595, "text_region": [[48.0, 317.0], [287.0, 317.0], [287.0, 330.0], [48.0, 330.0]]}, {"text": "ing of the text contents. Besides, evaluating on a much", "confidence": 0.9849061369895935, "text_region": [[48.0, 328.0], [289.0, 328.0], [289.0, 342.0], [48.0, 342.0]]}, {"text": "smaller vocabulary set results in over-optimism of the per-", "confidence": 0.9918606281280518, "text_region": [[47.0, 340.0], [288.0, 341.0], [288.0, 354.0], [47.0, 353.0]]}, {"text": "formance of the recognition models. ", "confidence": 0.9567129611968994, "text_region": [[47.0, 352.0], [196.0, 352.0], [196.0, 365.0], [47.0, 365.0]]}, {"text": "To aid further research, we use the Amazon mechan-", "confidence": 0.9952194690704346, "text_region": [[59.0, 364.0], [288.0, 364.0], [288.0, 377.0], [59.0, 377.0]]}, {"text": "ical Turk (AMT) to re-annotate the aforementioned 4", "confidence": 0.9723191857337952, "text_region": [[48.0, 376.0], [289.0, 376.0], [289.0, 389.0], [48.0, 389.0]]}, {"text": "datasets, which amount to 6837 word images in total.", "confidence": 0.9893690943717957, "text_region": [[48.0, 388.0], [288.0, 388.0], [288.0, 402.0], [48.0, 402.0]]}, {"text": "Each word image is annotated by 3 workers, and we", "confidence": 0.9771137833595276, "text_region": [[47.0, 399.0], [289.0, 400.0], [289.0, 414.0], [47.0, 413.0]]}, {"text": "manually check and correct images where the 3 an-", "confidence": 0.9758868217468262, "text_region": [[48.0, 413.0], [288.0, 413.0], [288.0, 426.0], [48.0, 426.0]]}, {"text": "notations differ.", "confidence": 0.9923914670944214, "text_region": [[49.0, 426.0], [122.0, 426.0], [122.0, 437.0], [49.0, 437.0]]}, {"text": "The annotated datasets are released", "confidence": 0.9771566390991211, "text_region": [[116.0, 423.0], [289.0, 424.0], [289.0, 438.0], [116.0, 437.0]]}, {"text": "via GitHub at https://github.com/Jyouhou/", "confidence": 0.9701640605926514, "text_region": [[48.0, 437.0], [288.0, 437.0], [288.0, 449.0], [48.0, 449.0]]}, {"text": "Case-Sensitive-Scene-Text-Recognition-Datasets.", "confidence": 0.9939378499984741, "text_region": [[48.0, 448.0], [331.0, 448.0], [331.0, 461.0], [48.0, 461.0]]}], "img_idx": 0, "score": 0.9794421792030334}
{"type": "text", "bbox": [50, 487, 285, 508], "res": [{"text": "We select some samples from the 4 datasets to demon-", "confidence": 0.998902440071106, "text_region": [[60.0, 486.0], [287.0, 486.0], [287.0, 499.0], [60.0, 499.0]]}, {"text": "strate the new annotations in Fig. 6.", "confidence": 0.9714668989181519, "text_region": [[48.0, 499.0], [194.0, 499.0], [194.0, 512.0], [48.0, 512.0]]}], "img_idx": 0, "score": 0.9556050896644592}
{"type": "text", "bbox": [308, 251, 544, 271], "res": [{"text": "that it may still be a challenge to recognize a larger vocabu-", "confidence": 0.9884766340255737, "text_region": [[307.0, 249.0], [545.0, 249.0], [545.0, 262.0], [307.0, 262.0]]}, {"text": "lary, and is worth further research.", "confidence": 0.9855563640594482, "text_region": [[307.0, 261.0], [447.0, 261.0], [447.0, 274.0], [307.0, 274.0]]}], "img_idx": 0, "score": 0.9531238675117493}
{"type": "title", "bbox": [49, 519, 189, 527], "res": [{"text": "B.2 Benchmark Performances", "confidence": 0.9758742451667786, "text_region": [[47.0, 516.0], [192.0, 518.0], [192.0, 532.0], [47.0, 530.0]]}], "img_idx": 0, "score": 0.9454983472824097}
{"type": "title", "bbox": [49, 469, 107, 479], "res": [{"text": "B.1 Samples", "confidence": 0.9722509384155273, "text_region": [[48.0, 466.0], [110.0, 468.0], [109.0, 481.0], [47.0, 479.0]]}], "img_idx": 0, "score": 0.9329555034637451}
{"type": "title", "bbox": [49, 164, 286, 188], "res": [{"text": "B. New Annotations for Scene Text Recogni-", "confidence": 0.9660708904266357, "text_region": [[47.0, 163.0], [287.0, 163.0], [287.0, 176.0], [47.0, 176.0]]}, {"text": "tion Datasets", "confidence": 0.9890623092651367, "text_region": [[48.0, 175.0], [118.0, 177.0], [117.0, 191.0], [47.0, 189.0]]}], "img_idx": 0, "score": 0.91367107629776}
{"type": "title", "bbox": [50, 73, 134, 82], "res": [{"text": "A. Scene Models", "confidence": 0.9499619603157043, "text_region": [[48.0, 72.0], [136.0, 72.0], [136.0, 85.0], [48.0, 85.0]]}], "img_idx": 0, "score": 0.8529868721961975}
{"type": "figure", "bbox": [301, 70, 548, 210], "res": [{"text": "Datase", "confidence": 0.8982248902320862, "text_region": [[310.0, 71.0], [337.0, 74.0], [335.0, 85.0], [309.0, 83.0]]}, {"text": "Sample lmage", "confidence": 0.920893669128418, "text_region": [[351.0, 72.0], [398.0, 74.0], [397.0, 85.0], [350.0, 83.0]]}, {"text": "Original Anmotation", "confidence": 0.945458173751831, "text_region": [[412.0, 73.0], [477.0, 73.0], [477.0, 83.0], [412.0, 83.0]]}, {"text": "New Annotatiot", "confidence": 0.939188539981842, "text_region": [[492.0, 74.0], [541.0, 74.0], [541.0, 84.0], [492.0, 84.0]]}, {"text": "CUTES0", "confidence": 0.9698386192321777, "text_region": [[309.0, 92.0], [340.0, 92.0], [340.0, 106.0], [309.0, 106.0]]}, {"text": "Tear", "confidence": 0.9333434104919434, "text_region": [[354.0, 91.0], [385.0, 91.0], [385.0, 102.0], [354.0, 102.0]]}, {"text": "TEAM", "confidence": 0.9996298551559448, "text_region": [[428.0, 93.0], [453.0, 93.0], [453.0, 105.0], [428.0, 105.0]]}, {"text": "Team", "confidence": 0.9918627738952637, "text_region": [[508.0, 94.0], [527.0, 94.0], [527.0, 105.0], [508.0, 105.0]]}, {"text": "IIT5K", "confidence": 0.7995277643203735, "text_region": [[311.0, 125.0], [333.0, 125.0], [333.0, 136.0], [311.0, 136.0]]}, {"text": "15%.", "confidence": 0.9635123610496521, "text_region": [[355.0, 119.0], [395.0, 119.0], [395.0, 138.0], [355.0, 138.0]]}, {"text": "15%.", "confidence": 0.9537452459335327, "text_region": [[508.0, 124.0], [526.0, 124.0], [526.0, 137.0], [508.0, 137.0]]}, {"text": "SVT", "confidence": 0.9993813037872314, "text_region": [[309.0, 156.0], [327.0, 156.0], [327.0, 167.0], [309.0, 167.0]]}, {"text": "DONALD", "confidence": 0.9940016865730286, "text_region": [[424.0, 156.0], [458.0, 156.0], [458.0, 167.0], [424.0, 167.0]]}, {"text": "Donald", "confidence": 0.9952074885368347, "text_region": [[504.0, 156.0], [529.0, 156.0], [529.0, 167.0], [504.0, 167.0]]}, {"text": "SVTP", "confidence": 0.9982485771179199, "text_region": [[309.0, 187.0], [331.0, 187.0], [331.0, 199.0], [309.0, 199.0]]}, {"text": "MARLBORO", "confidence": 0.9953808784484863, "text_region": [[419.0, 187.0], [463.0, 187.0], [463.0, 198.0], [419.0, 198.0]]}, {"text": "Marlboro", "confidence": 0.9546093940734863, "text_region": [[502.0, 188.0], [532.0, 188.0], [532.0, 199.0], [502.0, 199.0]]}], "img_idx": 0, "score": 0.9417840242385864}
{"type": "figure_caption", "bbox": [341, 217, 512, 226], "res": [{"text": "Figure 6: Examples of the new annotations.", "confidence": 0.9941007494926453, "text_region": [[337.0, 215.0], [514.0, 215.0], [514.0, 229.0], [337.0, 229.0]]}], "img_idx": 0, "score": 0.8971556425094604}
{"type": "reference", "bbox": [48, 676, 207, 712], "res": [{"text": "ICDAR201s-ArT-Recognitian-Alcheny", "confidence": 0.8912654519081116, "text_region": [[48.0, 684.0], [211.0, 684.0], [211.0, 697.0], [48.0, 697.0]]}, {"text": "deep-text-reccgnltion-benchnark", "confidence": 0.9326699376106262, "text_region": [[48.0, 702.0], [200.0, 702.0], [200.0, 715.0], [48.0, 715.0]]}, {"text": "*https://github.con/clovaai/", "confidence": 0.9409591555595398, "text_region": [[57.0, 693.0], [197.0, 693.0], [197.0, 706.0], [57.0, 706.0]]}], "img_idx": 0, "score": 0.8925058245658875}
