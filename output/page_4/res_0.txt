{"type": "text", "bbox": [307, 584, 545, 713], "res": [{"text": "In real-world, text instances are usually embedded on", "confidence": 0.9912566542625427, "text_region": [[318.0, 582.0], [548.0, 582.0], [548.0, 595.0], [318.0, 595.0]]}, {"text": "well-defined surfaces, e.g. traffic signs, to maintain good", "confidence": 0.9892337322235107, "text_region": [[307.0, 594.0], [548.0, 594.0], [548.0, 608.0], [307.0, 608.0]]}, {"text": "legibility. Previous works find suitable regions by using es-", "confidence": 0.9834616184234619, "text_region": [[306.0, 605.0], [547.0, 606.0], [547.0, 620.0], [306.0, 619.0]]}, {"text": "timated scene information, such as gPb-UCM [1] in Syn-", "confidence": 0.9900447130203247, "text_region": [[307.0, 618.0], [547.0, 618.0], [547.0, 632.0], [307.0, 632.0]]}, {"text": "thText [6] or saliency map in VISD [50] for approxima-", "confidence": 0.9795865416526794, "text_region": [[306.0, 629.0], [547.0, 630.0], [547.0, 645.0], [306.0, 643.0]]}, {"text": "tion. However, these methods are imprecise and often fail", "confidence": 0.9903744459152222, "text_region": [[307.0, 643.0], [547.0, 643.0], [547.0, 655.0], [307.0, 655.0]]}, {"text": "to find appropriate regions. Therefore, we propose to find", "confidence": 0.9930726289749146, "text_region": [[306.0, 653.0], [547.0, 653.0], [547.0, 667.0], [306.0, 667.0]]}, {"text": "text regions by probing around object meshes in 3D world.", "confidence": 0.990970253944397, "text_region": [[307.0, 666.0], [547.0, 666.0], [547.0, 680.0], [307.0, 680.0]]}, {"text": "Since inspecting all object meshes is time-consuming, we", "confidence": 0.9931823015213013, "text_region": [[307.0, 678.0], [547.0, 678.0], [547.0, 692.0], [307.0, 692.0]]}, {"text": "propose a 2-staged pipeline: (1) We retrieve ground truth", "confidence": 0.9608228206634521, "text_region": [[304.0, 689.0], [548.0, 688.0], [548.0, 705.0], [304.0, 706.0]]}, {"text": "surface normal map to generate initial text region propos-", "confidence": 0.9904035925865173, "text_region": [[306.0, 702.0], [547.0, 702.0], [547.0, 716.0], [306.0, 716.0]]}], "img_idx": 0, "score": 0.9929065704345703}
{"type": "text", "bbox": [48, 484, 287, 626], "res": [{"text": "The proposed random walk algorithm, however, is ineffi-", "confidence": 0.9844965934753418, "text_region": [[48.0, 483.0], [287.0, 483.0], [287.0, 497.0], [48.0, 497.0]]}, {"text": "cient in terms of exploration. Therefore, we manually select", "confidence": 0.9992169141769409, "text_region": [[47.0, 495.0], [289.0, 495.0], [289.0, 508.0], [47.0, 508.0]]}, {"text": "a set of N camera anchors across the 3D scenes as start-", "confidence": 0.9706918001174927, "text_region": [[47.0, 507.0], [289.0, 507.0], [289.0, 520.0], [47.0, 520.0]]}, {"text": "ing points. Afer every T steps, we reset the location of", "confidence": 0.9877819418907166, "text_region": [[45.0, 518.0], [290.0, 516.0], [290.0, 533.0], [45.0, 535.0]]}, {"text": "the camera to a randomly sampled camera anchor. We set", "confidence": 0.9864398241043091, "text_region": [[47.0, 532.0], [289.0, 532.0], [289.0, 544.0], [47.0, 544.0]]}, {"text": "N = 150-200 and 7 = 100. Note that the selection of cam-", "confidence": 0.9877101182937622, "text_region": [[46.0, 542.0], [288.0, 543.0], [288.0, 556.0], [46.0, 555.0]]}, {"text": "era anchors requires only little carefulness. We only need to", "confidence": 0.9878345131874084, "text_region": [[48.0, 555.0], [289.0, 555.0], [289.0, 568.0], [48.0, 568.0]]}, {"text": "ensure coverage over the space. It takes around 20 to 30 sec-", "confidence": 0.9900742173194885, "text_region": [[48.0, 567.0], [289.0, 567.0], [289.0, 581.0], [48.0, 581.0]]}, {"text": "onds for each scene, which is trivial and not a bottleneck of", "confidence": 0.9604992866516113, "text_region": [[48.0, 579.0], [289.0, 579.0], [289.0, 592.0], [48.0, 592.0]]}, {"text": "scalability. The manual but efficient selection of camera is", "confidence": 0.9902756810188293, "text_region": [[48.0, 591.0], [289.0, 591.0], [289.0, 604.0], [48.0, 604.0]]}, {"text": "compatible with the proposed random walk algorithm that", "confidence": 0.9953722953796387, "text_region": [[48.0, 603.0], [289.0, 603.0], [289.0, 617.0], [48.0, 617.0]]}, {"text": "generates diverse viewpoints.", "confidence": 0.9971941113471985, "text_region": [[48.0, 616.0], [168.0, 616.0], [168.0, 629.0], [48.0, 629.0]]}], "img_idx": 0, "score": 0.9895960688591003}
{"type": "text", "bbox": [49, 655, 287, 713], "res": [{"text": "To produce real-world variations such as lighting condi-", "confidence": 0.9930723905563354, "text_region": [[60.0, 654.0], [287.0, 654.0], [287.0, 667.0], [60.0, 667.0]]}, {"text": "tions, we randomly change the intensity, color, and direction ", "confidence": 0.9785512685775757, "text_region": [[47.0, 666.0], [289.0, 666.0], [289.0, 679.0], [47.0, 679.0]]}, {"text": "of all light sources in the scene. In addition to illuminations, ", "confidence": 0.9709265828132629, "text_region": [[46.0, 677.0], [288.0, 678.0], [288.0, 692.0], [46.0, 691.0]]}, {"text": "we also add fog conditions and randomly adjust its inten-", "confidence": 0.9948150515556335, "text_region": [[48.0, 690.0], [287.0, 690.0], [287.0, 704.0], [48.0, 704.0]]}, {"text": "sity. The environment randomization proves to increase the", "confidence": 0.9947381019592285, "text_region": [[48.0, 702.0], [289.0, 702.0], [289.0, 716.0], [48.0, 716.0]]}], "img_idx": 0, "score": 0.9886815547943115}
{"type": "text", "bbox": [48, 305, 287, 446], "res": [{"text": "In each step, we first randomly change the pitch and yaw", "confidence": 0.9956941604614258, "text_region": [[60.0, 304.0], [287.0, 304.0], [287.0, 318.0], [60.0, 318.0]]}, {"text": "values of the camera rotation, making the camera pointing", "confidence": 0.9874083399772644, "text_region": [[48.0, 315.0], [289.0, 316.0], [289.0, 330.0], [48.0, 329.0]]}, {"text": "to a new direction. Then, we cast a ray from the camera lo-", "confidence": 0.9887237548828125, "text_region": [[47.0, 328.0], [287.0, 328.0], [287.0, 342.0], [47.0, 342.0]]}, {"text": "cation towards the direction of the viewpoint. The ray stops", "confidence": 0.9985958337783813, "text_region": [[48.0, 341.0], [288.0, 341.0], [288.0, 353.0], [48.0, 353.0]]}, {"text": "when it hits any object meshes or reaches a fixed maximum", "confidence": 0.9844059348106384, "text_region": [[48.0, 352.0], [289.0, 352.0], [289.0, 365.0], [48.0, 365.0]]}, {"text": "length. By design, the path from the current location to the", "confidence": 0.99631667137146, "text_region": [[48.0, 363.0], [288.0, 363.0], [288.0, 376.0], [48.0, 376.0]]}, {"text": "stopping position is free of any barrier, i.e. not inside of", "confidence": 0.9825238585472107, "text_region": [[48.0, 375.0], [289.0, 375.0], [289.0, 389.0], [48.0, 389.0]]}, {"text": "any object meshes. Therefore, points along this ray path are", "confidence": 0.9874646067619324, "text_region": [[48.0, 387.0], [289.0, 387.0], [289.0, 401.0], [48.0, 401.0]]}, {"text": "all valid. Finally, we randomly sample one point between ", "confidence": 0.9782325625419617, "text_region": [[47.0, 398.0], [289.0, 399.0], [289.0, 413.0], [47.0, 412.0]]}, {"text": "the -th and -th of this path, and set it as the new location ", "confidence": 0.9565654993057251, "text_region": [[47.0, 411.0], [289.0, 411.0], [289.0, 425.0], [47.0, 425.0]]}, {"text": "of the camera, which is non-trivial. The proposed random ", "confidence": 0.9697124361991882, "text_region": [[47.0, 424.0], [289.0, 424.0], [289.0, 437.0], [47.0, 437.0]]}, {"text": "walk algorithm can generate diverse camera viewpoints.", "confidence": 0.9883609414100647, "text_region": [[48.0, 436.0], [276.0, 436.0], [276.0, 448.0], [48.0, 448.0]]}], "img_idx": 0, "score": 0.9875385761260986}
{"type": "text", "bbox": [307, 521, 545, 553], "res": [{"text": "diversity of the generated images and results in stronger de-", "confidence": 0.9931662082672119, "text_region": [[306.0, 520.0], [547.0, 520.0], [547.0, 533.0], [306.0, 533.0]]}, {"text": "tector performance. The proposed randomization can also", "confidence": 0.9830589294433594, "text_region": [[306.0, 532.0], [548.0, 532.0], [548.0, 545.0], [306.0, 545.0]]}, {"text": "benefit sim-to-real domain adaptation [41].", "confidence": 0.9909231662750244, "text_region": [[307.0, 544.0], [482.0, 544.0], [482.0, 556.0], [307.0, 556.0]]}], "img_idx": 0, "score": 0.9816579818725586}
{"type": "text", "bbox": [307, 427, 545, 496], "res": [{"text": "Figure 3: In the first row (1)-(4), we illustrate the", "confidence": 0.9665961265563965, "text_region": [[306.0, 426.0], [547.0, 426.0], [547.0, 439.0], [306.0, 439.0]]}, {"text": "physically-constrained 3D rando walk. For better visu-", "confidence": 0.9653031229972839, "text_region": [[306.0, 438.0], [547.0, 438.0], [547.0, 451.0], [306.0, 451.0]]}, {"text": "alization, we use a camera object to represent the viewpoint", "confidence": 0.9936462044715881, "text_region": [[307.0, 450.0], [547.0, 450.0], [547.0, 463.0], [307.0, 463.0]]}, {"text": "(marked with green boxes and arrows). In the second row,", "confidence": 0.9800763726234436, "text_region": [[306.0, 461.0], [547.0, 461.0], [547.0, 474.0], [306.0, 474.0]]}, {"text": "we compare viewpoints from the proposed method with ran-", "confidence": 0.97978276014328, "text_region": [[307.0, 473.0], [547.0, 473.0], [547.0, 487.0], [307.0, 487.0]]}, {"text": "domly sampled viewpoints.", "confidence": 0.9931467175483704, "text_region": [[306.0, 485.0], [421.0, 485.0], [421.0, 498.0], [306.0, 498.0]]}], "img_idx": 0, "score": 0.974494993686676}
{"type": "title", "bbox": [49, 637, 205, 645], "res": [{"text": "3.3. Environment Randomization", "confidence": 0.9732052683830261, "text_region": [[48.0, 636.0], [207.0, 636.0], [207.0, 648.0], [48.0, 648.0]]}], "img_idx": 0, "score": 0.9489011764526367}
{"type": "title", "bbox": [50, 466, 195, 474], "res": [{"text": "3.2.2 Auxiliary Camera Anchors", "confidence": 0.9691970944404602, "text_region": [[46.0, 462.0], [197.0, 463.0], [197.0, 477.0], [46.0, 476.0]]}], "img_idx": 0, "score": 0.9199057817459106}
{"type": "title", "bbox": [318, 565, 430, 574], "res": [{"text": "3.4. Text Region Generation", "confidence": 0.9834526181221008, "text_region": [[307.0, 563.0], [443.0, 563.0], [443.0, 576.0], [307.0, 576.0]]}], "img_idx": 0, "score": 0.8178195357322693}
{"type": "figure", "bbox": [305, 302, 544, 423], "res": [], "img_idx": 0, "score": 0.9524997472763062}
{"type": "figure", "bbox": [56, 67, 544, 254], "res": [{"text": "Mirarda >jstirti", "confidence": 0.8656798601150513, "text_region": [[433.0, 224.0], [505.0, 224.0], [505.0, 234.0], [433.0, 234.0]]}, {"text": "tiersFidy Eul", "confidence": 0.6810055375099182, "text_region": [[429.0, 238.0], [504.0, 236.0], [504.0, 250.0], [429.0, 253.0]]}], "img_idx": 0, "score": 0.9347797632217407}
{"type": "figure_caption", "bbox": [81, 262, 530, 293], "res": [{"text": "Figure 2: The pipeline of the proposed synthesis method. The arrows indicate the order. For simplicity, we only show one text", "confidence": 0.9821586012840271, "text_region": [[46.0, 258.0], [548.0, 259.0], [548.0, 276.0], [46.0, 275.0]]}, {"text": "region. From left to right: scene overview, diverse viewpoints, various lighting conditions (light color, intensity, shadows,", "confidence": 0.986828088760376, "text_region": [[46.0, 272.0], [547.0, 271.0], [547.0, 285.0], [46.0, 286.0]]}, {"text": "etc.), text region generation and text rendering.", "confidence": 0.9940202832221985, "text_region": [[47.0, 284.0], [238.0, 284.0], [238.0, 298.0], [47.0, 298.0]]}], "img_idx": 0, "score": 0.8866541385650635}
