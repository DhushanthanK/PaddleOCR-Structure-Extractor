{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be5f638",
   "metadata": {},
   "source": [
    "## Project Directory Structure\n",
    "\n",
    "- pdf_images/  \n",
    "  - page_1.png  \n",
    "  - page_2.png  \n",
    "  - ...\n",
    "\n",
    "- pdf_texts/  \n",
    "  - page_1_text.txt  \n",
    "  - page_2_text.txt  \n",
    "  - ...\n",
    "\n",
    "- output/\n",
    "  - annotations/\n",
    "    - page_1_annotation.png\n",
    "    - page_2_annotation.png\n",
    "    - ...\n",
    "  - json/\n",
    "    - page_1.json\n",
    "    - page_2.json\n",
    "    - ...\n",
    "  - page_1_result.jpg  \n",
    "  - page_2_result.jpg  \n",
    "  - ...\n",
    "  - page_1/  \n",
    "    - [39, 215, 553, 332]_0.jpg  \n",
    "    - [132, 44, 621, 222]_1.jpg  \n",
    "    - ...\n",
    "  - page_2/  \n",
    "    - [51, 198, 537, 345]_0.jpg  \n",
    "    - ...\n",
    "  \n",
    "- UnrealText.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e189e661-57cd-445c-b8a5-dc0a85a2c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(11901) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paddlepaddle in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (2.6.2)\n",
      "Requirement already satisfied: paddleocr in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (2.8.1)\n",
      "Requirement already satisfied: httpx in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddlepaddle) (0.27.2)\n",
      "Requirement already satisfied: numpy>=1.13 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddlepaddle) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddlepaddle) (10.4.0)\n",
      "Requirement already satisfied: decorator in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddlepaddle) (5.1.1)\n",
      "Requirement already satisfied: astor in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddlepaddle) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddlepaddle) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddlepaddle) (4.25.5)\n",
      "Requirement already satisfied: shapely in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (2.0.6)\n",
      "Requirement already satisfied: scikit-image in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (0.24.0)\n",
      "Requirement already satisfied: imgaug in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (0.4.0)\n",
      "Requirement already satisfied: pyclipper in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (1.3.0.post5)\n",
      "Requirement already satisfied: lmdb in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (1.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (4.66.5)\n",
      "Requirement already satisfied: rapidfuzz in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (3.10.0)\n",
      "Requirement already satisfied: opencv-python in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (4.6.0.66)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (4.10.0.84)\n",
      "Requirement already satisfied: cython in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (3.0.11)\n",
      "Requirement already satisfied: pyyaml in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (6.0.2)\n",
      "Requirement already satisfied: python-docx in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (1.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (4.12.3)\n",
      "Requirement already satisfied: fonttools>=4.24.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (4.54.1)\n",
      "Requirement already satisfied: fire>=0.3.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (0.6.0)\n",
      "Requirement already satisfied: requests in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleocr) (2.32.3)\n",
      "Requirement already satisfied: six in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fire>=0.3.0->paddleocr) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fire>=0.3.0->paddleocr) (2.4.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from beautifulsoup4->paddleocr) (2.6)\n",
      "Requirement already satisfied: anyio in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from httpx->paddlepaddle) (4.6.0)\n",
      "Requirement already satisfied: certifi in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from httpx->paddlepaddle) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from httpx->paddlepaddle) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from httpx->paddlepaddle) (3.10)\n",
      "Requirement already satisfied: sniffio in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from httpx->paddlepaddle) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n",
      "Requirement already satisfied: scipy in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from imgaug->paddleocr) (1.14.1)\n",
      "Requirement already satisfied: matplotlib in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from imgaug->paddleocr) (3.9.2)\n",
      "Requirement already satisfied: imageio in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from imgaug->paddleocr) (2.35.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-image->paddleocr) (3.3)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-image->paddleocr) (2024.9.20)\n",
      "Requirement already satisfied: packaging>=21 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-image->paddleocr) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-image->paddleocr) (0.4)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from python-docx->paddleocr) (5.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from python-docx->paddleocr) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->paddleocr) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->paddleocr) (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->paddleocr) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->paddleocr) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->paddleocr) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->paddleocr) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->paddleocr) (2.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install paddlepaddle paddleocr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4102270c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(11932) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shapely in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 1)) (2.0.6)\n",
      "Requirement already satisfied: scikit-image in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 2)) (0.24.0)\n",
      "Requirement already satisfied: imgaug in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: pyclipper in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 4)) (1.3.0.post5)\n",
      "Requirement already satisfied: lmdb in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 5)) (1.5.1)\n",
      "Requirement already satisfied: tqdm in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 6)) (4.66.5)\n",
      "Requirement already satisfied: numpy<2.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: rapidfuzz in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 8)) (3.10.0)\n",
      "Requirement already satisfied: opencv-python in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 9)) (4.6.0.66)\n",
      "Requirement already satisfied: opencv-contrib-python in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 10)) (4.10.0.84)\n",
      "Requirement already satisfied: cython in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 11)) (3.0.11)\n",
      "Requirement already satisfied: Pillow in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 12)) (10.4.0)\n",
      "Requirement already satisfied: pyyaml in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 13)) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 14)) (2.32.3)\n",
      "Requirement already satisfied: albumentations==1.4.10 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 15)) (1.4.10)\n",
      "Requirement already satisfied: albucore==0.0.13 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from -r PaddleOCR/requirements.txt (line 17)) (0.0.13)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from albumentations==1.4.10->-r PaddleOCR/requirements.txt (line 15)) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from albumentations==1.4.10->-r PaddleOCR/requirements.txt (line 15)) (4.12.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3.2 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from albumentations==1.4.10->-r PaddleOCR/requirements.txt (line 15)) (1.5.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from albumentations==1.4.10->-r PaddleOCR/requirements.txt (line 15)) (2.9.2)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from albumentations==1.4.10->-r PaddleOCR/requirements.txt (line 15)) (4.10.0.84)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from albucore==0.0.13->-r PaddleOCR/requirements.txt (line 17)) (2.0.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-image->-r PaddleOCR/requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: imageio>=2.33 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-image->-r PaddleOCR/requirements.txt (line 2)) (2.35.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-image->-r PaddleOCR/requirements.txt (line 2)) (2024.9.20)\n",
      "Requirement already satisfied: packaging>=21 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-image->-r PaddleOCR/requirements.txt (line 2)) (24.1)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-image->-r PaddleOCR/requirements.txt (line 2)) (0.4)\n",
      "Requirement already satisfied: six in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from imgaug->-r PaddleOCR/requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: matplotlib in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from imgaug->-r PaddleOCR/requirements.txt (line 3)) (3.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->-r PaddleOCR/requirements.txt (line 14)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->-r PaddleOCR/requirements.txt (line 14)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->-r PaddleOCR/requirements.txt (line 14)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->-r PaddleOCR/requirements.txt (line 14)) (2024.8.30)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from pydantic>=2.7.0->albumentations==1.4.10->-r PaddleOCR/requirements.txt (line 15)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from pydantic>=2.7.0->albumentations==1.4.10->-r PaddleOCR/requirements.txt (line 15)) (2.23.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->-r PaddleOCR/requirements.txt (line 15)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->-r PaddleOCR/requirements.txt (line 15)) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->-r PaddleOCR/requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->-r PaddleOCR/requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->-r PaddleOCR/requirements.txt (line 3)) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->-r PaddleOCR/requirements.txt (line 3)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->-r PaddleOCR/requirements.txt (line 3)) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib->imgaug->-r PaddleOCR/requirements.txt (line 3)) (2.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r PaddleOCR/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2d7b2e9-63a7-48a8-b7be-dd50159ae733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%git` not found.\n"
     ]
    }
   ],
   "source": [
    "%git clone https://github.com/PaddlePaddle/PaddleOCR.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2567adc0-e0a1-4f64-b5a5-90b14ecfa490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: openpyxl in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (3.1.5)\n",
      "Requirement already satisfied: tablepyxl in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (0.6.1)\n",
      "Requirement already satisfied: fitz in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (0.0.1.dev2)\n",
      "Requirement already satisfied: PyMuPDF in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (1.24.10)\n",
      "Collecting paddleclas\n",
      "  Downloading paddleclas-2.5.2-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: et-xmlfile in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: premailer in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from tablepyxl) (3.10.0)\n",
      "Requirement already satisfied: requests in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from tablepyxl) (2.32.3)\n",
      "Requirement already satisfied: lxml in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from tablepyxl) (5.3.0)\n",
      "Requirement already satisfied: configobj in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fitz) (5.0.9)\n",
      "Requirement already satisfied: configparser in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fitz) (7.1.0)\n",
      "Requirement already satisfied: httplib2 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fitz) (0.22.0)\n",
      "Requirement already satisfied: nibabel in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fitz) (5.2.1)\n",
      "Requirement already satisfied: nipype in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fitz) (1.8.6)\n",
      "Requirement already satisfied: pandas in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fitz) (2.2.3)\n",
      "Requirement already satisfied: pyxnat in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fitz) (1.6.2)\n",
      "Requirement already satisfied: scipy in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from fitz) (1.14.1)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.10 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from PyMuPDF) (1.24.10)\n",
      "Collecting prettytable (from paddleclas)\n",
      "  Downloading prettytable-3.11.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting ujson (from paddleclas)\n",
      "  Using cached ujson-5.10.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.3 kB)\n",
      "Collecting opencv-python==4.6.0.66 (from paddleclas)\n",
      "  Downloading opencv_python-4.6.0.66-cp37-abi3-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tqdm in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleclas) (4.66.5)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleclas) (6.0.2)\n",
      "Collecting visualdl>=2.2.0 (from paddleclas)\n",
      "  Downloading visualdl-2.5.3-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.21.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from paddleclas) (1.5.2)\n",
      "Collecting gast==0.3.3 (from paddleclas)\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting faiss-cpu (from paddleclas)\n",
      "  Downloading faiss_cpu-1.8.0.post1-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.7 kB)\n",
      "Collecting easydict (from paddleclas)\n",
      "  Downloading easydict-1.13-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-learn>=0.21.0->paddleclas) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from scikit-learn>=0.21.0->paddleclas) (3.5.0)\n",
      "Collecting bce-python-sdk (from visualdl>=2.2.0->paddleclas)\n",
      "  Downloading bce_python_sdk-0.9.23-py3-none-any.whl.metadata (319 bytes)\n",
      "Collecting flask>=1.1.1 (from visualdl>=2.2.0->paddleclas)\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting Flask-Babel>=3.0.0 (from visualdl>=2.2.0->paddleclas)\n",
      "  Downloading flask_babel-4.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from visualdl>=2.2.0->paddleclas) (4.25.5)\n",
      "Collecting rarfile (from visualdl>=2.2.0->paddleclas)\n",
      "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: psutil in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from visualdl>=2.2.0->paddleclas) (5.9.0)\n",
      "Requirement already satisfied: click>=6.6.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (8.1.7)\n",
      "Requirement already satisfied: networkx>=2.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (3.3)\n",
      "Requirement already satisfied: prov>=1.5.2 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (2.0.1)\n",
      "Requirement already satisfied: pydot>=1.2.3 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (3.0.2)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (6.3.2)\n",
      "Requirement already satisfied: simplejson>=3.8.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (3.19.3)\n",
      "Requirement already satisfied: traits!=5.0,<6.4,>=4.6 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (6.3.2)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (3.16.1)\n",
      "Requirement already satisfied: etelemetry>=0.2.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (0.3.1)\n",
      "Requirement already satisfied: looseversion in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from nipype->fitz) (1.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from pandas->fitz) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from pandas->fitz) (2024.2)\n",
      "Requirement already satisfied: cssselect in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from premailer->tablepyxl) (1.2.0)\n",
      "Requirement already satisfied: cssutils in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from premailer->tablepyxl) (2.11.1)\n",
      "Requirement already satisfied: cachetools in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from premailer->tablepyxl) (5.5.0)\n",
      "Requirement already satisfied: wcwidth in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from prettytable->paddleclas) (0.2.13)\n",
      "Requirement already satisfied: pathlib>=1.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->tablepyxl) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->tablepyxl) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->tablepyxl) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from requests->tablepyxl) (2024.8.30)\n",
      "Requirement already satisfied: ci-info>=0.2 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from etelemetry>=0.2.0->nipype->fitz) (0.3.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from flask>=1.1.1->visualdl>=2.2.0->paddleclas) (3.0.4)\n",
      "Collecting Jinja2>=3.1.2 (from flask>=1.1.1->visualdl>=2.2.0->paddleclas)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting itsdangerous>=2.1.2 (from flask>=1.1.1->visualdl>=2.2.0->paddleclas)\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting blinker>=1.6.2 (from flask>=1.1.1->visualdl>=2.2.0->paddleclas)\n",
      "  Using cached blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting Babel>=2.12 (from Flask-Babel>=3.0.0->visualdl>=2.2.0->paddleclas)\n",
      "  Using cached babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from rdflib>=5.0.0->nipype->fitz) (0.6.1)\n",
      "Collecting pycryptodome>=3.8.0 (from bce-python-sdk->visualdl>=2.2.0->paddleclas)\n",
      "  Using cached pycryptodome-3.20.0-cp35-abi3-macosx_10_9_universal2.whl.metadata (3.4 kB)\n",
      "Collecting future>=0.6.0 (from bce-python-sdk->visualdl>=2.2.0->paddleclas)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: more-itertools in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from cssutils->premailer->tablepyxl) (10.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (from Jinja2>=3.1.2->flask>=1.1.1->visualdl>=2.2.0->paddleclas) (2.1.5)\n",
      "Downloading paddleclas-2.5.2-py3-none-any.whl (356 kB)\n",
      "Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Downloading opencv_python-4.6.0.66-cp37-abi3-macosx_11_0_arm64.whl (30.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.0/30.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m0m\n",
      "\u001b[?25hDownloading visualdl-2.5.3-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading easydict-1.13-py3-none-any.whl (6.8 kB)\n",
      "Downloading faiss_cpu-1.8.0.post1-cp311-cp311-macosx_11_0_arm64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading prettytable-3.11.0-py3-none-any.whl (28 kB)\n",
      "Using cached ujson-5.10.0-cp311-cp311-macosx_11_0_arm64.whl (51 kB)\n",
      "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "Downloading flask_babel-4.0.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading bce_python_sdk-0.9.23-py3-none-any.whl (336 kB)\n",
      "Downloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
      "Using cached babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
      "Using cached blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached pycryptodome-3.20.0-cp35-abi3-macosx_10_9_universal2.whl (2.4 MB)\n",
      "Installing collected packages: easydict, ujson, rarfile, pycryptodome, prettytable, opencv-python, Jinja2, itsdangerous, gast, future, faiss-cpu, blinker, Babel, flask, bce-python-sdk, Flask-Babel, visualdl, paddleclas\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.10.0.84\n",
      "    Uninstalling opencv-python-4.10.0.84:\n",
      "      Successfully uninstalled opencv-python-4.10.0.84\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.6.0\n",
      "    Uninstalling gast-0.6.0:\n",
      "      Successfully uninstalled gast-0.6.0\n",
      "Successfully installed Babel-2.16.0 Flask-Babel-4.0.0 Jinja2-3.1.4 bce-python-sdk-0.9.23 blinker-1.8.2 easydict-1.13 faiss-cpu-1.8.0.post1 flask-3.0.3 future-1.0.0 gast-0.3.3 itsdangerous-2.2.0 opencv-python-4.6.0.66 paddleclas-2.5.2 prettytable-3.11.0 pycryptodome-3.20.0 rarfile-4.2 ujson-5.10.0 visualdl-2.5.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib openpyxl tablepyxl fitz PyMuPDF paddleclas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2657d30-c984-4cc3-a36f-7268bc582bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3882903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymupdf\n",
    "pdf = pymupdf.open('UnrealText.pdf')\n",
    "pdf.page_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25795e88",
   "metadata": {},
   "source": [
    "#### Save individual pages as `.png` files in `pdf_images` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060f22e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved page 1 as pdf_images/page_1.png\n",
      "Saved page 2 as pdf_images/page_2.png\n",
      "Saved page 3 as pdf_images/page_3.png\n",
      "Saved page 4 as pdf_images/page_4.png\n",
      "Saved page 5 as pdf_images/page_5.png\n",
      "Saved page 6 as pdf_images/page_6.png\n",
      "Saved page 7 as pdf_images/page_7.png\n",
      "Saved page 8 as pdf_images/page_8.png\n",
      "Saved page 9 as pdf_images/page_9.png\n",
      "Saved page 10 as pdf_images/page_10.png\n",
      "Saved page 11 as pdf_images/page_11.png\n",
      "Saved page 12 as pdf_images/page_12.png\n",
      "Saved page 13 as pdf_images/page_13.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_folder = \"pdf_images\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for index, page in enumerate(pdf, start=1):\n",
    "    pix = page.get_pixmap()  \n",
    "    img_out_path = os.path.join(output_folder, f\"page_{index}.png\")\n",
    "    pix.save(img_out_path)\n",
    "\n",
    "    print(f\"Saved page {index} as {img_out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31bbbbf",
   "metadata": {},
   "source": [
    "#### Save individual extracted text in each pages as `.txt` files in `pdf_texts` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e5c3168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text from page 1 and saved as 'pdf_texts/page_1_text.txt'\n",
      "Extracted text from page 2 and saved as 'pdf_texts/page_2_text.txt'\n",
      "Extracted text from page 3 and saved as 'pdf_texts/page_3_text.txt'\n",
      "Extracted text from page 4 and saved as 'pdf_texts/page_4_text.txt'\n",
      "Extracted text from page 5 and saved as 'pdf_texts/page_5_text.txt'\n",
      "Extracted text from page 6 and saved as 'pdf_texts/page_6_text.txt'\n",
      "Extracted text from page 7 and saved as 'pdf_texts/page_7_text.txt'\n",
      "Extracted text from page 8 and saved as 'pdf_texts/page_8_text.txt'\n",
      "Extracted text from page 9 and saved as 'pdf_texts/page_9_text.txt'\n",
      "Extracted text from page 10 and saved as 'pdf_texts/page_10_text.txt'\n",
      "Extracted text from page 11 and saved as 'pdf_texts/page_11_text.txt'\n",
      "Extracted text from page 12 and saved as 'pdf_texts/page_12_text.txt'\n",
      "Extracted text from page 13 and saved as 'pdf_texts/page_13_text.txt'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_text_folder = \"pdf_texts\"\n",
    "if not os.path.exists(output_text_folder):\n",
    "    os.makedirs(output_text_folder)\n",
    "\n",
    "for page_num, page in enumerate(pdf, start=1):\n",
    "    page_text = page.get_text().encode(\"utf8\") \n",
    "\n",
    "    output_file_path = os.path.join(output_text_folder, f\"page_{page_num}_text.txt\")\n",
    "    with open(output_file_path, \"wb\") as out_file:\n",
    "        out_file.write(page_text)\n",
    "\n",
    "    print(f\"Extracted text from page {page_num} and saved as '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbda573",
   "metadata": {},
   "source": [
    "#### OCR processing and annotation of all `.png` images in the `pdf_images` folder, saving annotated outputs to `output/annotations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852fe3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/10/01 22:12:28] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/dhushanthankumararatnam/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/dhushanthankumararatnam/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/Users/dhushanthankumararatnam/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/10/01 22:12:29] ppocr DEBUG: dt_boxes num : 104, elapsed : 0.27304506301879883\n",
      "[2024/10/01 22:12:30] ppocr DEBUG: cls num  : 104, elapsed : 0.5405347347259521\n",
      "[2024/10/01 22:12:56] ppocr DEBUG: rec_res num  : 104, elapsed : 26.256542921066284\n",
      "[[[49.0, 73.0], [285.0, 73.0], [285.0, 83.0], [49.0, 83.0]], ('tion follow the same paradigm. First, they analyze back', 0.9887501001358032)]\n",
      "[[[307.0, 74.0], [544.0, 74.0], [544.0, 84.0], [307.0, 84.0]], ('to carry out comprehensive evaluations, and tend to over', 0.9968594312667847)]\n",
      "[[[48.0, 85.0], [289.0, 85.0], [289.0, 98.0], [48.0, 98.0]], ('ground images, e.g. by performing semantic segmentation', 0.986565887928009)]\n",
      "[[[306.0, 85.0], [546.0, 85.0], [546.0, 98.0], [306.0, 98.0]], ('estimate the progress of scene text recognition algorithms.', 0.9990171194076538)]\n",
      "[[[48.0, 97.0], [288.0, 97.0], [288.0, 110.0], [48.0, 110.0]], ('and depth estimation using off-the-shelf models. Then, po-', 0.9927543997764587)]\n",
      "[[[306.0, 96.0], [545.0, 97.0], [545.0, 110.0], [306.0, 109.0]], ('To address this issue, we re-annotate these datasets to in-', 0.9969362616539001)]\n",
      "[[[48.0, 109.0], [288.0, 109.0], [288.0, 122.0], [48.0, 122.0]], ('tential locations for text embedding are extracted from the', 0.9722949266433716)]\n",
      "[[[305.0, 108.0], [547.0, 110.0], [547.0, 123.0], [305.0, 121.0]], ('clude both upper-case and lower-case characters, digits,', 0.9976215362548828)]\n",
      "[[[49.0, 123.0], [287.0, 123.0], [287.0, 133.0], [49.0, 133.0]], ('segmented regions. Finally, text images (foregrounds) are', 0.9944707751274109)]\n",
      "[[[305.0, 121.0], [547.0, 122.0], [547.0, 135.0], [305.0, 134.0]], ('punctuation marks, and spaces if there are any. We urge', 0.9811460971832275)]\n",
      "[[[48.0, 134.0], [288.0, 134.0], [288.0, 147.0], [48.0, 147.0]], ('blended into the background images, with perceptive trans-', 0.9972341656684875)]\n",
      "[[[306.0, 134.0], [547.0, 134.0], [547.0, 147.0], [306.0, 147.0]], ('researchers to use the new annotations and evaluate in such', 0.9998359680175781)]\n",
      "[[[48.0, 146.0], [289.0, 146.0], [289.0, 158.0], [48.0, 158.0]], ('formation inferred from estimated depth. However, the', 0.9829224348068237)]\n",
      "[[[308.0, 147.0], [545.0, 147.0], [545.0, 156.0], [308.0, 156.0]], ('a full-symbol mode for better understanding of the advan-', 0.9974234104156494)]\n",
      "[[[48.0, 157.0], [289.0, 157.0], [289.0, 170.0], [48.0, 170.0]], ('analysis of background images with off-the-shelf models', 0.9927530288696289)]\n",
      "[[[306.0, 157.0], [500.0, 157.0], [500.0, 170.0], [306.0, 170.0]], ('tages and disadvantages of different algorithms.', 0.9943819642066956)]\n",
      "[[[48.0, 169.0], [288.0, 169.0], [288.0, 182.0], [48.0, 182.0]], ('may be rough and imprecise. The errors further propagate', 0.9971097111701965)]\n",
      "[[[49.0, 182.0], [287.0, 182.0], [287.0, 192.0], [49.0, 192.0]], ('to text proposal modules and result in text being embedded', 0.9928777813911438)]\n",
      "[[[308.0, 182.0], [394.0, 182.0], [394.0, 195.0], [308.0, 195.0]], ('2. Related Work', 0.9598706960678101)]\n",
      "[[[48.0, 193.0], [288.0, 193.0], [288.0, 206.0], [48.0, 206.0]], ('onto unsuitable locations. Moreover, the text embedding', 0.9681375026702881)]\n",
      "[[[49.0, 206.0], [287.0, 206.0], [287.0, 216.0], [49.0, 216.0]], ('process is ignorant of the overall image conditions such as', 0.9937768578529358)]\n",
      "[[[307.0, 201.0], [411.0, 203.0], [411.0, 216.0], [307.0, 214.0]], ('2.1. Synthetic Images', 0.9972440600395203)]\n",
      "[[[49.0, 218.0], [286.0, 218.0], [286.0, 228.0], [49.0, 228.0]], ('illumination and occlusions of the scene. These two factors', 0.9936158061027527)]\n",
      "[[[47.0, 228.0], [289.0, 229.0], [289.0, 242.0], [47.0, 241.0]], ('make text instances outstanding from backgrounds, leading', 0.9998955130577087)]\n",
      "[[[318.0, 220.0], [545.0, 222.0], [544.0, 235.0], [318.0, 233.0]], ('The synthesis of photo-realistic datasets has been a pop-', 0.9955611228942871)]\n",
      "[[[47.0, 241.0], [225.0, 241.0], [225.0, 253.0], [47.0, 253.0]], ('to a gap between synthetic and real images.', 0.9914747476577759)]\n",
      "[[[307.0, 234.0], [545.0, 234.0], [545.0, 247.0], [307.0, 247.0]], ('ular topic, since they provide detailed ground-truth annota-', 0.9985734224319458)]\n",
      "[[[59.0, 254.0], [287.0, 254.0], [287.0, 267.0], [59.0, 267.0]], ('In this paper, we propose a synthetic engine that syn-', 0.9976584911346436)]\n",
      "[[[306.0, 246.0], [545.0, 246.0], [545.0, 258.0], [306.0, 258.0]], ('tions at multiple granularity, and cost less than manual an-', 0.9843088388442993)]\n",
      "[[[48.0, 265.0], [287.0, 265.0], [287.0, 278.0], [48.0, 278.0]], ('thesizes scene text images from 3D virtual world. The', 0.9851980209350586)]\n",
      "[[[307.0, 257.0], [547.0, 257.0], [547.0, 270.0], [307.0, 270.0]], ('notations. In scene text detection and recognition, the use of', 0.9838951826095581)]\n",
      "[[[48.0, 278.0], [289.0, 278.0], [289.0, 291.0], [48.0, 291.0]], ('proposed engine is based on the famous Unreal Engine 4', 0.9977454543113708)]\n",
      "[[[306.0, 269.0], [547.0, 269.0], [547.0, 282.0], [306.0, 282.0]], ('synthetic datasets has become a standard practice. For scene', 0.9910086393356323)]\n",
      "[[[49.0, 291.0], [285.0, 291.0], [285.0, 301.0], [49.0, 301.0]], ('(UE4), and is therefore named as UnrealTexr. Specifically', 0.9808611273765564)]\n",
      "[[[307.0, 283.0], [544.0, 283.0], [544.0, 293.0], [307.0, 293.0]], ('text recognition, where images contain only one word, syn', 0.9967554211616516)]\n",
      "[[[48.0, 303.0], [288.0, 303.0], [288.0, 316.0], [48.0, 316.0]], ('text instances are regarded as planar polygon meshes with', 0.9914162158966064)]\n",
      "[[[307.0, 295.0], [545.0, 295.0], [545.0, 305.0], [307.0, 305.0]], ('thetic images are rendered through several steps [46, 10],', 0.9948192238807678)]\n",
      "[[[49.0, 315.0], [287.0, 315.0], [287.0, 325.0], [49.0, 325.0]], ('text foregrounds loaded as texture. These meshes are placed', 0.9998049139976501)]\n",
      "[[[308.0, 307.0], [545.0, 307.0], [545.0, 317.0], [308.0, 317.0]], ('including font rendering, coloring, homography transfor-', 0.999334990978241)]\n",
      "[[[48.0, 327.0], [289.0, 327.0], [289.0, 340.0], [48.0, 340.0]], ('in suitable positions in 3D world, and rendered together', 0.9879487156867981)]\n",
      "[[[307.0, 318.0], [547.0, 318.0], [547.0, 331.0], [307.0, 331.0]], ('mation, and background blending. Later, GANs [5] are', 0.9750092625617981)]\n",
      "[[[48.0, 339.0], [156.0, 339.0], [156.0, 351.0], [48.0, 351.0]], ('with the scene as a whole.', 0.994480550289154)]\n",
      "[[[308.0, 331.0], [546.0, 331.0], [546.0, 341.0], [308.0, 341.0]], ('incorporated to maintain style consistency for implanted', 0.988158106803894)]\n",
      "[[[306.0, 341.0], [546.0, 341.0], [546.0, 353.0], [306.0, 353.0]], ('text [51], but it is only for single-word images. As a re-', 0.9823866486549377)]\n",
      "[[[61.0, 351.0], [288.0, 352.0], [288.0, 365.0], [61.0, 364.0]], ('As shown in Fig. 1, the proposed synthesis engine, by', 0.9964091181755066)]\n",
      "[[[306.0, 353.0], [548.0, 353.0], [548.0, 366.0], [306.0, 366.0]], ('sult of these progresses, synthetic data alone are enough to', 0.992196798324585)]\n",
      "[[[48.0, 364.0], [287.0, 364.0], [287.0, 377.0], [48.0, 377.0]], ('its very nature, enjoys the following advantages over pre-', 0.9892510771751404)]\n",
      "[[[306.0, 366.0], [440.0, 366.0], [440.0, 376.0], [306.0, 376.0]], ('train state-of-the-art recognizers.', 0.9966814517974854)]\n",
      "[[[48.0, 376.0], [287.0, 377.0], [287.0, 388.0], [48.0, 387.0]], ('vious methods: (1) Text and scenes are rendered together,', 0.9963891506195068)]\n",
      "[[[318.0, 376.0], [548.0, 378.0], [548.0, 391.0], [318.0, 389.0]], ('To train scene text detectors, SynthText [6] proposes to', 0.9935629963874817)]\n",
      "[[[50.0, 389.0], [285.0, 389.0], [285.0, 399.0], [50.0, 399.0]], ('achieving realistic visual effects, e.g. illumination, occlu-', 0.9871864318847656)]\n",
      "[[[306.0, 390.0], [545.0, 390.0], [545.0, 403.0], [306.0, 403.0]], ('generate synthetic data by printing text on background im-', 0.9952461123466492)]\n",
      "[[[49.0, 401.0], [286.0, 401.0], [286.0, 411.0], [49.0, 411.0]], ('sion, and perspective transformation. (2) The method has', 0.9884233474731445)]\n",
      "[[[306.0, 402.0], [547.0, 402.0], [547.0, 415.0], [306.0, 415.0]], ('ages. It first analyzes images with off-the-shelf models, and', 0.9890110492706299)]\n",
      "[[[48.0, 412.0], [289.0, 412.0], [289.0, 425.0], [48.0, 425.0]], ('access to precise scene information, e.g. normal, depth, and', 0.9733564853668213)]\n",
      "[[[48.0, 423.0], [289.0, 424.0], [289.0, 437.0], [48.0, 436.0]], ('object meshes, and therefore can generate better text region', 0.9997419714927673)]\n",
      "[[[307.0, 414.0], [546.0, 414.0], [546.0, 427.0], [307.0, 427.0]], ('search suitable text regions on semantically consistent re-', 0.9923118948936462)]\n",
      "[[[307.0, 426.0], [548.0, 426.0], [548.0, 439.0], [307.0, 439.0]], ('gions. Text are implanted with perspective transformation', 0.99568110704422)]\n",
      "[[[48.0, 436.0], [279.0, 436.0], [279.0, 448.0], [48.0, 448.0]], ('proposals. These aspects are crucial in training detectors.', 0.9820246696472168)]\n",
      "[[[307.0, 438.0], [546.0, 438.0], [546.0, 450.0], [307.0, 450.0]], ('based on estimated depth. To maintain semantic coherency,', 0.9876899719238281)]\n",
      "[[[60.0, 449.0], [288.0, 449.0], [288.0, 462.0], [60.0, 462.0]], ('To further exploit the potential of UnrealText, we design', 0.9855657815933228)]\n",
      "[[[308.0, 449.0], [547.0, 449.0], [547.0, 462.0], [308.0, 462.0]], ('VISD [50] proposes to use semantic segmentation to filter', 0.9906533360481262)]\n",
      "[[[48.0, 461.0], [288.0, 461.0], [288.0, 474.0], [48.0, 474.0]], ('three key components: (1) A view finding algorithm that', 0.9710965156555176)]\n",
      "[[[306.0, 461.0], [548.0, 461.0], [548.0, 474.0], [306.0, 474.0]], ('out unreasonable surfaces such as human faces. They also', 0.9978726506233215)]\n",
      "[[[48.0, 473.0], [288.0, 473.0], [288.0, 486.0], [48.0, 486.0]], ('explores the virtual scenes and generates camera viewpoints', 0.9906877875328064)]\n",
      "[[[306.0, 473.0], [548.0, 473.0], [548.0, 486.0], [306.0, 486.0]], ('adopt an adaptive coloring scheme to fit the text into the', 0.9890819191932678)]\n",
      "[[[48.0, 485.0], [288.0, 485.0], [288.0, 498.0], [48.0, 498.0]], ('to obtain more diverse and natural backgrounds. (2) An en-', 0.9865487813949585)]\n",
      "[[[307.0, 485.0], [546.0, 485.0], [546.0, 498.0], [307.0, 498.0]], ('artistic style of backgrounds. However, without consider-', 0.9745141863822937)]\n",
      "[[[48.0, 497.0], [289.0, 497.0], [289.0, 510.0], [48.0, 510.0]], ('vironment randomization module that changes the lighting', 0.9983149766921997)]\n",
      "[[[307.0, 497.0], [547.0, 497.0], [547.0, 510.0], [307.0, 510.0]], ('ing the scene as a whole, these methods fail to render text', 0.99032062292099)]\n",
      "[[[49.0, 511.0], [288.0, 511.0], [288.0, 521.0], [49.0, 521.0]], ('conditions regularly, to simulate real-world variations. (3)', 0.9941189289093018)]\n",
      "[[[308.0, 511.0], [546.0, 511.0], [546.0, 521.0], [308.0, 521.0]], ('instances in a photo-realistic way, and text instances are too', 0.9995906352996826)]\n",
      "[[[49.0, 523.0], [286.0, 523.0], [286.0, 533.0], [49.0, 533.0]], ('A mesh-based text region generation method that finds suit-', 0.9896655082702637)]\n",
      "[[[307.0, 523.0], [545.0, 523.0], [545.0, 533.0], [307.0, 533.0]], ('outstanding from backgrounds. So far, the training of de-', 0.9871309995651245)]\n",
      "[[[48.0, 534.0], [247.0, 534.0], [247.0, 546.0], [48.0, 546.0]], ('able positions for text by probing the 3D meshes.', 0.9974780082702637)]\n",
      "[[[307.0, 533.0], [472.0, 534.0], [472.0, 547.0], [307.0, 545.0]], ('tectors still relies heavily on real images.', 0.9992765784263611)]\n",
      "[[[60.0, 547.0], [287.0, 547.0], [287.0, 560.0], [60.0, 560.0]], ('The contributions of this paper are summarized as fol-', 0.9904221892356873)]\n",
      "[[[319.0, 546.0], [547.0, 546.0], [547.0, 559.0], [319.0, 559.0]], ('Although GANs and other learning-based methods have', 0.9998620748519897)]\n",
      "[[[48.0, 558.0], [286.0, 558.0], [286.0, 571.0], [48.0, 571.0]], ('lows: (1) We propose a brand-new scene text image syn-', 0.9821262955665588)]\n",
      "[[[307.0, 558.0], [546.0, 558.0], [546.0, 571.0], [307.0, 571.0]], ('also shown great potential in generating realistic im-', 0.9930053949356079)]\n",
      "[[[47.0, 569.0], [288.0, 570.0], [288.0, 583.0], [47.0, 582.0]], ('thesis engine that renders images from 3D world, which is', 0.9964474439620972)]\n",
      "[[[306.0, 570.0], [547.0, 569.0], [547.0, 582.0], [306.0, 583.0]], ('ages [48, 17, 12], the generation of scene text images still', 0.9896250367164612)]\n",
      "[[[48.0, 581.0], [288.0, 582.0], [288.0, 595.0], [48.0, 594.0]], ('entirely different from previous approaches that embed text', 0.9916262626647949)]\n",
      "[[[308.0, 584.0], [545.0, 584.0], [545.0, 594.0], [308.0, 594.0]], ('require a large amount of manually labeled data [51]. Fur-', 0.9779133796691895)]\n",
      "[[[49.0, 596.0], [286.0, 596.0], [286.0, 606.0], [49.0, 606.0]], ('on 2D background images, termed as UnrealText. The pro-', 0.99676114320755)]\n",
      "[[[307.0, 596.0], [545.0, 596.0], [545.0, 606.0], [307.0, 606.0]], ('thermore, such data are sometimes not easy to collect, es-', 0.995479166507721)]\n",
      "[[[49.0, 608.0], [287.0, 608.0], [287.0, 618.0], [49.0, 618.0]], ('posed engine achieves realistic rendering effects and high', 0.9983076453208923)]\n",
      "[[[306.0, 606.0], [510.0, 607.0], [510.0, 620.0], [306.0, 619.0]], ('pecially for cases such as low resource languages.', 0.995796263217926)]\n",
      "[[[48.0, 619.0], [287.0, 619.0], [287.0, 632.0], [48.0, 632.0]], ('scalability. (2) With the proposed techniques, the synthe-', 0.9952722787857056)]\n",
      "[[[319.0, 619.0], [546.0, 619.0], [546.0, 632.0], [319.0, 632.0]], ('More recently, synthesizing images with 3D graph-', 0.9769386053085327)]\n",
      "[[[48.0, 631.0], [288.0, 631.0], [288.0, 643.0], [48.0, 643.0]], ('sis engine improves the performance of detectors and rec-', 0.9956889152526855)]\n",
      "[[[307.0, 631.0], [546.0, 631.0], [546.0, 644.0], [307.0, 644.0]], ('ics engine has become popular in several fields, in-', 0.9932442903518677)]\n",
      "[[[48.0, 643.0], [288.0, 643.0], [288.0, 655.0], [48.0, 655.0]], ('ognizers significantly. (3) We also generate a large scale', 0.9900192618370056)]\n",
      "[[[306.0, 643.0], [546.0, 643.0], [546.0, 655.0], [306.0, 655.0]], ('cluding human pose estimation [43], scene understand-', 0.9764880537986755)]\n",
      "[[[48.0, 654.0], [288.0, 654.0], [288.0, 667.0], [48.0, 667.0]], ('multilingual scene text dataset that will aid further research.', 0.9773510694503784)]\n",
      "[[[306.0, 654.0], [546.0, 654.0], [546.0, 667.0], [306.0, 667.0]], ('ing/segmentation [28, 24, 33, 35, 37], and object detec-', 0.9698434472084045)]\n",
      "[[[48.0, 666.0], [288.0, 666.0], [288.0, 679.0], [48.0, 679.0]], ('(4) Additionally, we notice that many of the popular scene', 0.9957040548324585)]\n",
      "[[[306.0, 666.0], [547.0, 666.0], [547.0, 679.0], [306.0, 679.0]], ('tion [29, 42, 8]. However, these methods either consider', 0.9840407967567444)]\n",
      "[[[49.0, 680.0], [286.0, 680.0], [286.0, 690.0], [49.0, 690.0]], ('text recognition datasets are only annotated in an incom-', 0.9900985360145569)]\n",
      "[[[308.0, 680.0], [546.0, 680.0], [546.0, 690.0], [308.0, 690.0]], ('simplistic cases, e.g. rendering 3D objects on top of static', 0.9895209670066833)]\n",
      "[[[48.0, 691.0], [288.0, 691.0], [288.0, 704.0], [48.0, 704.0]], ('plete way, providing only case-insensitive word annota-', 0.9815679788589478)]\n",
      "[[[307.0, 691.0], [547.0, 691.0], [547.0, 704.0], [307.0, 704.0]], ('background images [29, 43] and randomly arranging scenes', 0.9826115369796753)]\n",
      "[[[49.0, 704.0], [288.0, 704.0], [288.0, 714.0], [49.0, 714.0]], ('tions. With such limited annotations, researchers are unable', 0.9933801889419556)]\n",
      "[[[307.0, 703.0], [544.0, 703.0], [544.0, 713.0], [307.0, 713.0]], ('filled with objects [28, 24, 35, 8], or passively use off-the-', 0.9786514639854431)]\n",
      "[2024/10/01 22:12:57] ppocr DEBUG: dt_boxes num : 102, elapsed : 0.24706411361694336\n",
      "[2024/10/01 22:12:58] ppocr DEBUG: cls num  : 102, elapsed : 0.5330660343170166\n",
      "[2024/10/01 22:13:23] ppocr DEBUG: rec_res num  : 102, elapsed : 25.312451124191284\n",
      "[[[307.0, 74.0], [374.0, 74.0], [374.0, 85.0], [307.0, 85.0]], ('tems [18, 7, 30].', 0.943655788898468)]\n",
      "[[[47.0, 85.0], [288.0, 85.0], [288.0, 98.0], [47.0, 98.0]], ('trast to these researches, our proposed synthesis engine im-', 0.9906942844390869)]\n",
      "[[[48.0, 97.0], [289.0, 97.0], [289.0, 110.0], [48.0, 110.0]], ('plements active and regular interaction with 3D scenes, to', 0.9849222302436829)]\n",
      "[[[307.0, 99.0], [482.0, 99.0], [482.0, 112.0], [307.0, 112.0]], ('3. Scene Text in 3D Virtual World', 0.9984534382820129)]\n",
      "[[[48.0, 109.0], [242.0, 110.0], [242.0, 123.0], [48.0, 122.0]], ('generate realistic and diverse scene text images.', 0.9796230792999268)]\n",
      "[[[61.0, 124.0], [288.0, 124.0], [288.0, 134.0], [61.0, 134.0]], ('This paper is also a sequel to our previous attempt, the', 0.9995607733726501)]\n",
      "[[[307.0, 121.0], [372.0, 121.0], [372.0, 132.0], [307.0, 132.0]], ('3.1. Overview', 0.9871905446052551)]\n",
      "[[[48.0, 135.0], [288.0, 135.0], [288.0, 148.0], [48.0, 148.0]], ('SynthText3D[16]. SynthText3D closely follows the designs', 0.9883469939231873)]\n",
      "[[[49.0, 148.0], [286.0, 148.0], [286.0, 157.0], [49.0, 157.0]], ('of the SynthText method.While SynthText uses off-the-', 0.9814414381980896)]\n",
      "[[[319.0, 141.0], [546.0, 141.0], [546.0, 150.0], [319.0, 150.0]], ('In this section, we give a detailed introduction to our', 0.9996960163116455)]\n",
      "[[[48.0, 158.0], [289.0, 158.0], [289.0, 171.0], [48.0, 171.0]], ('shelf computer vision models to estimate segmentation and', 0.9934767484664917)]\n",
      "[[[306.0, 151.0], [546.0, 151.0], [546.0, 164.0], [306.0, 164.0]], ('scene text image synthesis engine, UnrealTexr, which is de-', 0.9876759052276611)]\n",
      "[[[48.0, 170.0], [289.0, 170.0], [289.0, 183.0], [48.0, 183.0]], ('depth maps for background images, SynthText3D uses the', 0.9962624907493591)]\n",
      "[[[306.0, 163.0], [546.0, 163.0], [546.0, 176.0], [306.0, 176.0]], ('veloped upon UE4 and the UnrealCV plugin [31]. The syn-', 0.9992852210998535)]\n",
      "[[[49.0, 183.0], [288.0, 183.0], [288.0, 193.0], [49.0, 193.0]], ('ground-truth segmentation and depth maps provided by the', 0.9872079491615295)]\n",
      "[[[306.0, 186.0], [545.0, 187.0], [545.0, 200.0], [306.0, 199.0]], ('efficient, taking about only 1-1.5 second to render and gen-', 0.9951770901679993)]\n",
      "[[[308.0, 177.0], [547.0, 177.0], [547.0, 187.0], [308.0, 187.0]], ('thesis engine: (1) produces photo-realistic images, (2) is', 0.9838947057723999)]\n",
      "[[[49.0, 195.0], [107.0, 195.0], [107.0, 205.0], [49.0, 205.0]], ('3D engines.', 0.8950667977333069)]\n",
      "[[[103.0, 195.0], [287.0, 195.0], [287.0, 205.0], [103.0, 205.0]], ('The rendering process of SynthText3D does', 0.9840256571769714)]\n",
      "[[[49.0, 207.0], [285.0, 207.0], [285.0, 217.0], [49.0, 217.0]], ('not involve interactions with the 3D worlds, such as the ob-', 0.9912578463554382)]\n",
      "[[[306.0, 210.0], [546.0, 211.0], [546.0, 225.0], [306.0, 224.0]], ('patible to off-the-shelf 3D scene models. As shown in Fig.', 0.9979285001754761)]\n",
      "[[[308.0, 201.0], [545.0, 201.0], [545.0, 211.0], [308.0, 211.0]], ('erate a new scene text image and, (3) is general and com-', 0.9888752698898315)]\n",
      "[[[49.0, 219.0], [287.0, 219.0], [287.0, 229.0], [49.0, 229.0]], ('ject meshes. As a result, SynthText3D is faced with at least', 0.9813462495803833)]\n",
      "[[[49.0, 231.0], [287.0, 231.0], [287.0, 241.0], [49.0, 241.0]], ('these two limitations: (1) the camera locations and rotations', 0.987067699432373)]\n",
      "[[[308.0, 224.0], [546.0, 224.0], [546.0, 237.0], [308.0, 237.0]], ('2, the pipeline mainly consists of a Viewfinder module (sec-', 0.9903624057769775)]\n",
      "[[[49.0, 243.0], [285.0, 243.0], [285.0, 252.0], [49.0, 252.0]], ('are labeled by human, limiting the scalability as well as di-', 0.9917811751365662)]\n",
      "[[[307.0, 236.0], [547.0, 236.0], [547.0, 248.0], [307.0, 248.0]], ('tion3.2), an Environment Randomization module (section', 0.9820457100868225)]\n",
      "[[[48.0, 252.0], [287.0, 252.0], [287.0, 265.0], [48.0, 265.0]], ('versity; (2) the generated text regions are limited to well', 0.9791703820228577)]\n",
      "[[[308.0, 247.0], [548.0, 247.0], [548.0, 259.0], [308.0, 259.0]], ('3.3), a Text Region Generation module (section3.4), and a', 0.9751336574554443)]\n",
      "[[[48.0, 265.0], [289.0, 265.0], [289.0, 278.0], [48.0, 278.0]], ('defined regions that the camera is facing upfront, resulting', 0.9953745603561401)]\n",
      "[[[307.0, 258.0], [458.0, 258.0], [458.0, 271.0], [307.0, 271.0]], ('Text Rendering module (section3.5).', 0.9623456597328186)]\n",
      "[[[48.0, 278.0], [170.0, 278.0], [170.0, 288.0], [48.0, 288.0]], ('in a unfavorable location bias.', 0.9984591007232666)]\n",
      "[[[320.0, 272.0], [546.0, 272.0], [546.0, 282.0], [320.0, 282.0]], ('Firstly, the viewfinder module explores around the 3D', 0.997068464756012)]\n",
      "[[[306.0, 282.0], [545.0, 282.0], [545.0, 295.0], [306.0, 295.0]], ('scene with the camera, generating camera viewpoints.', 0.9787198305130005)]\n",
      "[[[47.0, 299.0], [247.0, 300.0], [247.0, 313.0], [47.0, 312.0]], ('2.2. Scene Text Detection and Recognition', 0.9977481365203857)]\n",
      "[[[308.0, 295.0], [546.0, 295.0], [546.0, 308.0], [308.0, 308.0]], ('Then, the environment lighting is randomly adjusted. Next,', 0.9957922101020813)]\n",
      "[[[307.0, 309.0], [545.0, 309.0], [545.0, 319.0], [307.0, 319.0]], ('the text regions are proposed based on 2D scene informa-', 0.9883803129196167)]\n",
      "[[[60.0, 319.0], [289.0, 319.0], [289.0, 332.0], [60.0, 332.0]], ('Scene text detection and recognition, possibly as the', 0.9757241010665894)]\n",
      "[[[307.0, 320.0], [543.0, 320.0], [543.0, 330.0], [307.0, 330.0]], ('tion and refined with 3D mesh information in the graph', 0.9996389746665955)]\n",
      "[[[48.0, 331.0], [287.0, 331.0], [287.0, 344.0], [48.0, 344.0]], ('most human-centric computer vision task, has been a pop-', 0.9942871332168579)]\n",
      "[[[308.0, 333.0], [546.0, 333.0], [546.0, 343.0], [308.0, 343.0]], ('ics engine. After that, text foregrounds are generated with', 0.9842225909233093)]\n",
      "[[[48.0, 343.0], [288.0, 343.0], [288.0, 355.0], [48.0, 355.0]], ('ular research topic for many years [49, 21]. In scene text', 0.9911643266677856)]\n",
      "[[[307.0, 344.0], [545.0, 344.0], [545.0, 353.0], [307.0, 353.0]], ('randomly sampled fonts, colors, and text content, and are', 0.997624397277832)]\n",
      "[[[48.0, 354.0], [288.0, 354.0], [288.0, 367.0], [48.0, 367.0]], ('detection, there are mainly two branches of methodolo-', 0.9983699917793274)]\n",
      "[[[307.0, 355.0], [546.0, 355.0], [546.0, 368.0], [307.0, 368.0]], ('loaded as planar meshes. Finally, we retrieve the RGB im-', 0.9761372804641724)]\n",
      "[[[48.0, 366.0], [288.0, 366.0], [288.0, 379.0], [48.0, 379.0]], ('gies: Top-down methods that inherit the idea of region pro-', 0.9840619564056396)]\n",
      "[[[48.0, 378.0], [288.0, 378.0], [288.0, 391.0], [48.0, 391.0]], ('posal networks from general object detectors that detect text', 0.9934006333351135)]\n",
      "[[[308.0, 369.0], [546.0, 369.0], [546.0, 379.0], [308.0, 379.0]], ('age and corresponding text locations as well as text content', 0.9858893156051636)]\n",
      "[[[306.0, 379.0], [426.0, 379.0], [426.0, 392.0], [306.0, 392.0]], ('to make the synthetic dataset.', 0.9427219033241272)]\n",
      "[[[48.0, 390.0], [287.0, 390.0], [287.0, 403.0], [48.0, 403.0]], ('instances as rotated rectangles and polygons [19, 53, 11,', 0.9845855236053467)]\n",
      "[[[47.0, 401.0], [289.0, 402.0], [289.0, 415.0], [47.0, 414.0]], ('52, 47]; Bottom-up approaches that predict local segments', 0.9906784892082214)]\n",
      "[[[308.0, 402.0], [379.0, 402.0], [379.0, 413.0], [308.0, 413.0]], ('3.2. Viewfinder', 0.9970136284828186)]\n",
      "[[[49.0, 416.0], [287.0, 416.0], [287.0, 426.0], [49.0, 426.0]], ('and local geometric attributes, and compose them into in-', 0.9827620983123779)]\n",
      "[[[49.0, 428.0], [286.0, 428.0], [286.0, 438.0], [49.0, 438.0]], ('dividual text instances [38, 22, 2, 40]. Despite significant', 0.9720889925956726)]\n",
      "[[[319.0, 421.0], [545.0, 421.0], [545.0, 434.0], [319.0, 434.0]], ('The aim of the viewfinder module is to automatically de-', 0.9984754920005798)]\n",
      "[[[47.0, 438.0], [287.0, 439.0], [287.0, 451.0], [47.0, 450.0]], ('improvements on individual datasets, those most widely', 0.9917609095573425)]\n",
      "[[[307.0, 434.0], [548.0, 434.0], [548.0, 446.0], [307.0, 446.0]], ('termine a set of camera locations and rotations from the', 0.9894803166389465)]\n",
      "[[[48.0, 450.0], [287.0, 450.0], [287.0, 463.0], [48.0, 463.0]], ('used benchmark datasets are usually very small, with only', 0.9840114116668701)]\n",
      "[[[307.0, 446.0], [545.0, 446.0], [545.0, 458.0], [307.0, 458.0]], ('whole space of 3D scenes that are reasonable and non-', 0.988010048866272)]\n",
      "[[[48.0, 461.0], [287.0, 462.0], [287.0, 475.0], [48.0, 474.0]], ('around 500 to 1000 images in test sets, and are therefore', 0.9973254799842834)]\n",
      "[[[307.0, 457.0], [547.0, 457.0], [547.0, 470.0], [307.0, 470.0]], ('trivial, getting rid of unsuitable viewpoints such as from', 0.9817604422569275)]\n",
      "[[[47.0, 474.0], [287.0, 473.0], [287.0, 486.0], [47.0, 487.0]], ('prone to over-fitting. The generalization ability across dif-', 0.9782000780105591)]\n",
      "[[[306.0, 469.0], [498.0, 469.0], [498.0, 482.0], [306.0, 482.0]], ('inside object meshes (e.g. Fig. 3 bottom right)', 0.980791449546814)]\n",
      "[[[49.0, 488.0], [286.0, 488.0], [286.0, 498.0], [49.0, 498.0]], ('ferent domains remains an open question, and is not studied', 0.9993560314178467)]\n",
      "[[[317.0, 481.0], [546.0, 482.0], [546.0, 495.0], [317.0, 494.0]], ('Learning-based methods such as navigation and explo-', 0.9960814714431763)]\n",
      "[[[49.0, 500.0], [288.0, 500.0], [288.0, 510.0], [49.0, 510.0]], ('yet. The reason lies in the very limited real data and that.', 0.9620753526687622)]\n",
      "[[[49.0, 511.0], [285.0, 511.0], [285.0, 521.0], [49.0, 521.0]], ('synthetic data are not effective enough. Therefore, one im', 0.9858720302581787)]\n",
      "[[[307.0, 507.0], [544.0, 507.0], [544.0, 517.0], [307.0, 517.0]], ('not guaranteed to generalize to different 3D scenes. There-', 0.9746765494346619)]\n",
      "[[[48.0, 523.0], [289.0, 523.0], [289.0, 536.0], [48.0, 536.0]], ('portant motivation of our synthesis engine is to serve as a', 0.9850097298622131)]\n",
      "[[[307.0, 519.0], [543.0, 519.0], [543.0, 529.0], [307.0, 529.0]], ('fore, we turn to rule-based methods and design a plysically', 0.9787764549255371)]\n",
      "[[[48.0, 536.0], [256.0, 535.0], [256.0, 545.0], [48.0, 546.0]], ('stepping stone towards general scene text detection.', 0.9934327006340027)]\n",
      "[[[307.0, 531.0], [455.0, 531.0], [455.0, 544.0], [307.0, 544.0]], ('constrained 3D random walk (Fig.)', 0.9854539632797241)]\n",
      "[[[451.0, 531.0], [546.0, 532.0], [545.0, 543.0], [451.0, 541.0]], ('3 first row) equipped', 0.9830672740936279)]\n",
      "[[[59.0, 547.0], [287.0, 547.0], [287.0, 560.0], [59.0, 560.0]], ('Most scene text recognition models consist of CNN-', 0.9932839870452881)]\n",
      "[[[308.0, 544.0], [435.0, 544.0], [435.0, 553.0], [308.0, 553.0]], ('with auxiliary camera anchors.', 0.9794420599937439)]\n",
      "[[[48.0, 559.0], [288.0, 559.0], [288.0, 572.0], [48.0, 572.0]], ('based image feature extractors and attentional LSTM [9] or', 0.9997512698173523)]\n",
      "[[[48.0, 570.0], [288.0, 570.0], [288.0, 583.0], [48.0, 583.0]], ('transformer [44]-based encoder-decoder to predict the tex-', 0.9926849007606506)]\n",
      "[[[49.0, 584.0], [286.0, 584.0], [286.0, 594.0], [49.0, 594.0]], ('tual content [3, 39, 15, 23]. Since the encoder-decoder mod-', 0.9809551239013672)]\n",
      "[[[308.0, 575.0], [338.0, 575.0], [338.0, 585.0], [308.0, 585.0]], ('3.2.1', 0.8579661250114441)]\n",
      "[[[336.0, 575.0], [516.0, 575.0], [516.0, 585.0], [336.0, 585.0]], ('Physically-Constrained 3D Random Walk', 0.997710645198822)]\n",
      "[[[49.0, 596.0], [287.0, 596.0], [287.0, 606.0], [49.0, 606.0]], ('ule is a language model in essence, scene text recognizers', 0.9949893951416016)]\n",
      "[[[307.0, 594.0], [547.0, 594.0], [547.0, 607.0], [307.0, 607.0]], ('Starting from a valid location, the physically-constrained', 0.9993331432342529)]\n",
      "[[[49.0, 608.0], [287.0, 608.0], [287.0, 618.0], [49.0, 618.0]], ('have a high demand for training data with a large vocabu-', 0.9949836134910583)]\n",
      "[[[308.0, 608.0], [546.0, 608.0], [546.0, 618.0], [308.0, 618.0]], ('3D random walk aims to find the next valid and non-trivial', 0.9857598543167114)]\n",
      "[[[48.0, 619.0], [286.0, 619.0], [286.0, 629.0], [48.0, 629.0]], ('lary, which is extremely difficult for real-world data. Be-', 0.9897084832191467)]\n",
      "[[[307.0, 620.0], [546.0, 620.0], [546.0, 630.0], [307.0, 630.0]], ('location. In contrast to being valid, locations are invalid if', 0.9914186596870422)]\n",
      "[[[48.0, 631.0], [289.0, 631.0], [289.0, 644.0], [48.0, 644.0]], ('sides, scene text recognizers work on image crops that have', 0.9832964539527893)]\n",
      "[[[307.0, 631.0], [547.0, 631.0], [547.0, 643.0], [307.0, 643.0]], ('they are inside object meshes or far away from the scene', 0.9992046356201172)]\n",
      "[[[48.0, 643.0], [287.0, 643.0], [287.0, 655.0], [48.0, 655.0]], ('simple backgrounds, which are easy to synthesize. There-', 0.9982576966285706)]\n",
      "[[[307.0, 643.0], [547.0, 643.0], [547.0, 655.0], [307.0, 655.0]], ('boundary, for example. A non-trivial location should be not', 0.9961081743240356)]\n",
      "[[[48.0, 653.0], [287.0, 653.0], [287.0, 666.0], [48.0, 666.0]], ('fore, synthetic data are necessary for scene text recogniz-', 0.993930995464325)]\n",
      "[[[306.0, 654.0], [547.0, 654.0], [547.0, 667.0], [306.0, 667.0]], ('too close to the current location. Otherwise, the new view-', 0.9953641891479492)]\n",
      "[[[48.0, 666.0], [289.0, 666.0], [289.0, 679.0], [48.0, 679.0]], ('ers, and synthetic data alone are usually enough to achieve', 0.9888169169425964)]\n",
      "[[[306.0, 665.0], [546.0, 665.0], [546.0, 678.0], [306.0, 678.0]], ('point will be similar to the current one. The proposed 3D', 0.9903504252433777)]\n",
      "[[[49.0, 680.0], [286.0, 680.0], [286.0, 690.0], [49.0, 690.0]], ('state-of-the-art performance. Moreover, since the recogni-', 0.9918754696846008)]\n",
      "[[[307.0, 680.0], [546.0, 680.0], [546.0, 690.0], [307.0, 690.0]], ('random walk uses ray-casting [36], which is constrained by', 0.9929866790771484)]\n",
      "[[[49.0, 692.0], [288.0, 692.0], [288.0, 702.0], [49.0, 702.0]], ('tion modules require a large amount of data, synthetic data', 0.994658887386322)]\n",
      "[[[307.0, 691.0], [547.0, 691.0], [547.0, 704.0], [307.0, 704.0]], ('physically, to inspect the physical environment to determine', 0.9923105835914612)]\n",
      "[[[49.0, 704.0], [287.0, 704.0], [287.0, 714.0], [49.0, 714.0]], ('are also necessary in training end-to-end text spotting sys-', 0.9875971078872681)]\n",
      "[[[308.0, 704.0], [431.0, 704.0], [431.0, 714.0], [308.0, 714.0]], ('valid and non-trivial locations.', 0.9851924180984497)]\n",
      "[2024/10/01 22:13:24] ppocr DEBUG: dt_boxes num : 75, elapsed : 0.2652559280395508\n",
      "[2024/10/01 22:13:24] ppocr DEBUG: cls num  : 75, elapsed : 0.41158080101013184\n",
      "[2024/10/01 22:13:42] ppocr DEBUG: rec_res num  : 75, elapsed : 17.353016138076782\n",
      "[[[62.0, 106.0], [533.0, 106.0], [533.0, 119.0], [62.0, 119.0]], ('UnrealText: Synthesizing Realistic Scene Text Images from the Unreal World', 0.9804205894470215)]\n",
      "[[[155.0, 144.0], [241.0, 146.0], [240.0, 159.0], [155.0, 158.0]], ('Shangbang Long', 0.9862884879112244)]\n",
      "[[[357.0, 145.0], [409.0, 145.0], [409.0, 158.0], [357.0, 158.0]], ('Cong Yao', 0.9990290999412537)]\n",
      "[[[131.0, 159.0], [266.0, 159.0], [266.0, 172.0], [131.0, 172.0]], ('Carnegie Mellon University', 0.9966010451316833)]\n",
      "[[[301.0, 159.0], [463.0, 159.0], [463.0, 172.0], [301.0, 172.0]], ('Megvii (Face++) Technology Inc.', 0.994939923286438)]\n",
      "[[[146.0, 174.0], [251.0, 174.0], [251.0, 187.0], [146.0, 187.0]], ('shangbal@cs.cmu.edu', 0.9930393099784851)]\n",
      "[[[325.0, 174.0], [440.0, 174.0], [440.0, 187.0], [325.0, 187.0]], ('yaocong2010@gmai1.com', 0.9737717509269714)]\n",
      "[[[14.0, 204.0], [39.0, 204.0], [39.0, 323.0], [14.0, 323.0]], ('18 Aug 2020', 0.9983835220336914)]\n",
      "[[[414.0, 248.0], [431.0, 248.0], [431.0, 254.0], [414.0, 254.0]], ('RER', 0.6817124485969543)]\n",
      "[[[433.0, 268.0], [447.0, 268.0], [447.0, 276.0], [433.0, 276.0]], ('rat', 0.8794355392456055)]\n",
      "[[[417.0, 281.0], [449.0, 281.0], [449.0, 292.0], [417.0, 292.0]], ('re der', 0.9178689122200012)]\n",
      "[[[16.0, 320.0], [35.0, 320.0], [35.0, 359.0], [16.0, 359.0]], ('CV]', 0.964312732219696)]\n",
      "[[[95.0, 324.0], [163.0, 323.0], [163.0, 334.0], [95.0, 335.0]], ('Lighting & Shadows', 0.998687207698822)]\n",
      "[[[264.0, 325.0], [332.0, 325.0], [332.0, 335.0], [264.0, 335.0]], ('Suitable Text Region', 0.994937539100647)]\n",
      "[[[447.0, 325.0], [481.0, 325.0], [481.0, 335.0], [447.0, 335.0]], ('Occlsion', 0.9821085333824158)]\n",
      "[[[47.0, 335.0], [546.0, 337.0], [546.0, 349.0], [47.0, 347.0]], ('Figure 1: Demonstration of the proposed UnrealText synthesis engine, which achieves photo-realistic lighting conditions,', 0.996127724647522)]\n",
      "[[[19.0, 352.0], [36.0, 352.0], [36.0, 385.0], [19.0, 385.0]], ('[cs.', 0.9001697897911072)]\n",
      "[[[47.0, 347.0], [537.0, 348.0], [536.0, 361.0], [47.0, 360.0]], ('finds suitable text regions, and realizes natural occlusion (from left to right, zoomed-in views marked with green squares).', 0.9800519347190857)]\n",
      "[[[146.0, 369.0], [191.0, 369.0], [191.0, 380.0], [146.0, 380.0]], ('Abstract', 0.9960368871688843)]\n",
      "[[[307.0, 369.0], [546.0, 369.0], [546.0, 382.0], [307.0, 382.0]], ('els are data-thirsty, and it is expensive and sometimes dif-', 0.9775762557983398)]\n",
      "[[[15.0, 389.0], [36.0, 390.0], [34.0, 465.0], [13.0, 464.0]], ('10608v6', 0.9973912835121155)]\n",
      "[[[307.0, 381.0], [546.0, 381.0], [546.0, 394.0], [307.0, 394.0]], ('ficult, if not impossible, to collect enough data. More-', 0.989855170249939)]\n",
      "[[[59.0, 393.0], [288.0, 393.0], [288.0, 406.0], [59.0, 406.0]], ('Synthetic data has been a critical tool for training scene', 0.9896377325057983)]\n",
      "[[[306.0, 395.0], [546.0, 395.0], [546.0, 405.0], [306.0, 405.0]], ('over, the various applications, from traffic sign reading in', 0.9977836608886719)]\n",
      "[[[49.0, 407.0], [286.0, 407.0], [286.0, 417.0], [49.0, 417.0]], ('text detection and recognition models. On the one hand,', 0.9821077585220337)]\n",
      "[[[307.0, 406.0], [546.0, 406.0], [546.0, 419.0], [307.0, 419.0]], ('autonomous vehicles to instant translation, require a large', 0.9920082688331604)]\n",
      "[[[49.0, 419.0], [285.0, 419.0], [285.0, 429.0], [49.0, 429.0]], ('synthetic word images have proven to be a successful sub', 0.9886087775230408)]\n",
      "[[[308.0, 419.0], [545.0, 419.0], [545.0, 429.0], [308.0, 429.0]], ('amount of data specifically for each domain, further es-', 0.9879938364028931)]\n",
      "[[[48.0, 430.0], [288.0, 430.0], [288.0, 443.0], [48.0, 443.0]], ('stitute for real images in training scene text recognizers. On', 0.9816665649414062)]\n",
      "[[[307.0, 431.0], [544.0, 431.0], [544.0, 441.0], [307.0, 441.0]], ('calating this issue. Therefore, synthetic data and synthe-', 0.9821346402168274)]\n",
      "[[[48.0, 442.0], [288.0, 442.0], [288.0, 454.0], [48.0, 454.0]], ('the other hand, however, scene text detectors still heavily', 0.9950767159461975)]\n",
      "[[[307.0, 442.0], [546.0, 442.0], [546.0, 454.0], [307.0, 454.0]], ('sis algorithms are important for scene text tasks. Further-', 0.9949280619621277)]\n",
      "[[[48.0, 453.0], [288.0, 453.0], [288.0, 466.0], [48.0, 466.0]], ('rely on a large amount of manually annotated real-world', 0.9883547425270081)]\n",
      "[[[306.0, 453.0], [547.0, 453.0], [547.0, 466.0], [306.0, 466.0]], ('more, synthetic data can provide detailed annotations, such', 0.9993138313293457)]\n",
      "[[[16.0, 464.0], [34.0, 464.0], [34.0, 514.0], [16.0, 514.0]], ('2003.', 0.9917973279953003)]\n",
      "[[[48.0, 465.0], [288.0, 465.0], [288.0, 478.0], [48.0, 478.0]], ('images, which are expensive. In this paper, we introduce', 0.9823217391967773)]\n",
      "[[[307.0, 465.0], [548.0, 465.0], [548.0, 478.0], [307.0, 478.0]], ('as character-level or even pixel-level ground truths that are', 0.9952062368392944)]\n",
      "[[[48.0, 477.0], [286.0, 477.0], [286.0, 490.0], [48.0, 490.0]], ('UnrealText, an efficient image synthesis method that ren-', 0.9773041009902954)]\n",
      "[[[307.0, 477.0], [456.0, 477.0], [456.0, 490.0], [307.0, 490.0]], ('rare for real images due to high cost.', 0.9764629006385803)]\n",
      "[[[48.0, 489.0], [288.0, 489.0], [288.0, 502.0], [48.0, 502.0]], ('ders realistic images via a 3D graphics engine.3D syn-', 0.9892629981040955)]\n",
      "[[[48.0, 501.0], [289.0, 501.0], [289.0, 514.0], [48.0, 514.0]], ('thetic engine provides realistic appearance by rendering', 0.9852207899093628)]\n",
      "[[[319.0, 492.0], [546.0, 492.0], [546.0, 505.0], [319.0, 505.0]], ('Currently, there exist several synthesis algorithms [46,', 0.987136721611023)]\n",
      "[[[49.0, 515.0], [286.0, 515.0], [286.0, 525.0], [49.0, 525.0]], ('scene and text as a whole, and allows for better text re-', 0.9861899018287659)]\n",
      "[[[309.0, 506.0], [546.0, 506.0], [546.0, 516.0], [309.0, 516.0]], ('10, 6, 50] that have proven beneficial. Especially, in scene', 0.9887561202049255)]\n",
      "[[[48.0, 526.0], [286.0, 525.0], [286.0, 538.0], [48.0, 539.0]], ('gion proposals with access to precise scene information,', 0.9874077439308167)]\n",
      "[[[307.0, 517.0], [547.0, 517.0], [547.0, 530.0], [307.0, 530.0]], ('text recognition, training on synthetic data [10, 6] alone', 0.9982399940490723)]\n",
      "[[[48.0, 538.0], [287.0, 537.0], [287.0, 549.0], [48.0, 550.0]], ('e.g. normal and even object meshes. The comprehensive', 0.9720956087112427)]\n",
      "[[[306.0, 529.0], [546.0, 529.0], [546.0, 542.0], [306.0, 542.0]], ('has become a widely accepted standard practice. Some re-', 0.9943065643310547)]\n",
      "[[[49.0, 550.0], [286.0, 550.0], [286.0, 560.0], [49.0, 560.0]], ('experiments verify its effectiveness on both scene text de-', 0.9852728247642517)]\n",
      "[[[307.0, 541.0], [547.0, 541.0], [547.0, 553.0], [307.0, 553.0]], ('searchers that attempt training on both synthetic and real', 0.9937447309494019)]\n",
      "[[[48.0, 561.0], [288.0, 561.0], [288.0, 574.0], [48.0, 574.0]], ('tection and recognition. We also generate a multilingual', 0.9765005111694336)]\n",
      "[[[306.0, 552.0], [547.0, 552.0], [547.0, 565.0], [306.0, 565.0]], ('data only report marginal improvements [15, 20] on most', 0.9981991648674011)]\n",
      "[[[306.0, 564.0], [547.0, 564.0], [547.0, 577.0], [306.0, 577.0]], ('datasets. Mixing synthetic and real data is only improving', 0.9858686923980713)]\n",
      "[[[50.0, 575.0], [286.0, 575.0], [286.0, 585.0], [50.0, 585.0]], ('version for future research into multilingual scene text de-', 0.989069402217865)]\n",
      "[[[307.0, 576.0], [547.0, 576.0], [547.0, 589.0], [307.0, 589.0]], ('performance on a few difficult cases that are not yet well', 0.9976546764373779)]\n",
      "[[[49.0, 586.0], [287.0, 586.0], [287.0, 596.0], [49.0, 596.0]], ('tection and recognition. Additionally, we re-annotate scene', 0.9820675253868103)]\n",
      "[[[306.0, 588.0], [546.0, 588.0], [546.0, 601.0], [306.0, 601.0]], ('covered by existing synthetic datasets, such as seriously', 0.9805473685264587)]\n",
      "[[[50.0, 598.0], [286.0, 598.0], [286.0, 608.0], [50.0, 608.0]], ('text recognition datasets in a case-sensitive way and in-', 0.9646046757698059)]\n",
      "[[[305.0, 599.0], [546.0, 599.0], [546.0, 612.0], [305.0, 612.0]], ('blurred or curved text. This is reasonable, since cropped', 0.9863039255142212)]\n",
      "[[[49.0, 610.0], [287.0, 610.0], [287.0, 620.0], [49.0, 620.0]], ('clude punctuation marks for more comprehensive evalua-', 0.9936432242393494)]\n",
      "[[[49.0, 621.0], [286.0, 621.0], [286.0, 631.0], [49.0, 631.0]], ('tions. The code and the generated datasets are released at:', 0.9848207235336304)]\n",
      "[[[308.0, 614.0], [545.0, 614.0], [545.0, 624.0], [308.0, 624.0]], ('text images have much simpler background, and synthetic', 0.9893922209739685)]\n",
      "[[[306.0, 624.0], [545.0, 624.0], [545.0, 637.0], [306.0, 637.0]], ('data enjoys advantages in larger vocabulary size and diver-', 0.9835794568061829)]\n",
      "[[[49.0, 634.0], [200.0, 634.0], [200.0, 644.0], [49.0, 644.0]], ('Rnps:Jijyouhou.github.io/UnreaSTexu.', 0.8457350730895996)]\n",
      "[[[308.0, 637.0], [548.0, 637.0], [548.0, 649.0], [308.0, 649.0]], ('sity of backgrounds, fonts, and lighting conditions, as well', 0.9870597720146179)]\n",
      "[[[50.0, 656.0], [128.0, 656.0], [128.0, 668.0], [50.0, 668.0]], ('1. Introduction', 0.9752236008644104)]\n",
      "[[[307.0, 647.0], [474.0, 648.0], [474.0, 661.0], [307.0, 660.0]], ('as thousands of times more data samples.', 0.9832832217216492)]\n",
      "[[[319.0, 663.0], [547.0, 663.0], [547.0, 676.0], [319.0, 676.0]], ('On the contrary, however, scene text detection is still', 0.9827271103858948)]\n",
      "[[[59.0, 673.0], [288.0, 674.0], [288.0, 688.0], [59.0, 687.0]], ('With the resurgence of neural networks, the past few', 0.9944196343421936)]\n",
      "[[[306.0, 675.0], [547.0, 675.0], [547.0, 688.0], [306.0, 688.0]], ('heavily dependent on real-world data. Synthetic data [6, 50]', 0.9860829710960388)]\n",
      "[[[47.0, 687.0], [289.0, 687.0], [289.0, 700.0], [47.0, 700.0]], ('years have witnessed significant progress in the field of', 0.9901286959648132)]\n",
      "[[[306.0, 687.0], [546.0, 687.0], [546.0, 700.0], [306.0, 700.0]], ('plays a less significant role, and only brings marginal im-', 0.9989439249038696)]\n",
      "[[[49.0, 701.0], [287.0, 701.0], [287.0, 711.0], [49.0, 711.0]], ('scene text detection and recognition. However, these mod-', 0.9847614765167236)]\n",
      "[[[307.0, 701.0], [361.0, 701.0], [361.0, 711.0], [307.0, 711.0]], ('provements.', 0.9992647767066956)]\n",
      "[[[360.0, 698.0], [545.0, 698.0], [545.0, 711.0], [360.0, 711.0]], ('Existing synthesizers for scene text detec-', 0.9957284331321716)]\n",
      "[2024/10/01 22:13:43] ppocr DEBUG: dt_boxes num : 62, elapsed : 0.30420923233032227\n",
      "[2024/10/01 22:13:43] ppocr DEBUG: cls num  : 62, elapsed : 0.37078380584716797\n",
      "[2024/10/01 22:14:01] ppocr DEBUG: rec_res num  : 62, elapsed : 17.48275589942932\n",
      "[[[176.0, 76.0], [203.0, 76.0], [203.0, 83.0], [176.0, 83.0]], ('Viewfindi', 0.9775442481040955)]\n",
      "[[[239.0, 73.0], [275.0, 73.0], [275.0, 80.0], [239.0, 80.0]], ('Errvironment', 0.8661117553710938)]\n",
      "[[[315.0, 76.0], [347.0, 76.0], [347.0, 83.0], [315.0, 83.0]], ('Text Regior', 0.9614933133125305)]\n",
      "[[[434.0, 238.0], [500.0, 240.0], [500.0, 249.0], [434.0, 246.0]], ('ectiors FdEull', 0.7306765913963318)]\n",
      "[[[49.0, 261.0], [545.0, 261.0], [545.0, 271.0], [49.0, 271.0]], ('Figure 2: The pipeline of the proposed synthesis method. The arrows indicate the order. For simplicity, we only show one text', 0.9921896457672119)]\n",
      "[[[48.0, 272.0], [547.0, 272.0], [547.0, 285.0], [48.0, 285.0]], ('region. From left to right: scene overview, diverse viewpoints, various lighting conditions (light color, intensity, shadows,', 0.9833638668060303)]\n",
      "[[[47.0, 283.0], [237.0, 285.0], [236.0, 298.0], [47.0, 296.0]], ('etc.), text region generation and text rendering.', 0.9900751709938049)]\n",
      "[[[59.0, 303.0], [288.0, 304.0], [288.0, 317.0], [59.0, 316.0]], ('In each step, we first randomly change the pitch and yaw', 0.9867748618125916)]\n",
      "[[[433.0, 303.0], [471.0, 303.0], [471.0, 310.0], [433.0, 310.0]], ('(3) Ray Castiny', 0.964119553565979)]\n",
      "[[[47.0, 315.0], [288.0, 317.0], [288.0, 330.0], [47.0, 328.0]], ('values of the camera rotation, making the camera pointing', 0.987524151802063)]\n",
      "[[[49.0, 329.0], [285.0, 329.0], [285.0, 339.0], [49.0, 339.0]], ('to a new direction. Then, we cast a ray from the camera lo-', 0.9881436228752136)]\n",
      "[[[48.0, 340.0], [288.0, 341.0], [288.0, 353.0], [48.0, 352.0]], ('cation towards the direction of the viewpoint. The ray stops', 0.9931666254997253)]\n",
      "[[[48.0, 351.0], [288.0, 352.0], [288.0, 365.0], [48.0, 364.0]], ('when it hits any object meshes or reaches a fixed maximum', 0.9946327805519104)]\n",
      "[[[49.0, 364.0], [287.0, 364.0], [287.0, 374.0], [49.0, 374.0]], ('length. By design, the path from the current location to the', 0.9908586144447327)]\n",
      "[[[48.0, 376.0], [288.0, 375.0], [288.0, 386.0], [48.0, 387.0]], ('stopping position is free of any barrier, i.e. not inside of.', 0.9802298545837402)]\n",
      "[[[49.0, 389.0], [287.0, 389.0], [287.0, 399.0], [49.0, 399.0]], ('any object meshes. Therefore, points along this ray path are', 0.994670033454895)]\n",
      "[[[48.0, 399.0], [288.0, 399.0], [288.0, 412.0], [48.0, 412.0]], ('all valid. Finally, we randomly sample one point between', 0.9872813820838928)]\n",
      "[[[49.0, 413.0], [287.0, 413.0], [287.0, 423.0], [49.0, 423.0]], ('the -th and -th of this path, and set it as the new location', 0.9913990497589111)]\n",
      "[[[48.0, 424.0], [289.0, 424.0], [289.0, 437.0], [48.0, 437.0]], ('of the camera, which is non-trivial. The proposed random', 0.9846777319908142)]\n",
      "[[[47.0, 435.0], [274.0, 436.0], [274.0, 448.0], [47.0, 447.0]], ('walk algorithm can generate diverse camera viewpoints.', 0.9971867799758911)]\n",
      "[[[307.0, 426.0], [548.0, 426.0], [548.0, 439.0], [307.0, 439.0]], ('Figure 3:  In the first row (1)-(4), we illustrate the', 0.9742329716682434)]\n",
      "[[[307.0, 440.0], [544.0, 440.0], [544.0, 449.0], [307.0, 449.0]], ('physically-constrained 3D random walk.For better visu', 0.9925435185432434)]\n",
      "[[[307.0, 450.0], [547.0, 450.0], [547.0, 463.0], [307.0, 463.0]], ('alization, we use a camera object to represent the viewpoint', 0.9991315603256226)]\n",
      "[[[49.0, 462.0], [196.0, 463.0], [196.0, 476.0], [49.0, 475.0]], ('3.2.2Auxiliary Camera Anchors', 0.9763965010643005)]\n",
      "[[[307.0, 461.0], [430.0, 461.0], [430.0, 474.0], [307.0, 474.0]], ('(marked with green boxes and', 0.946593701839447)]\n",
      "[[[427.0, 461.0], [546.0, 461.0], [546.0, 474.0], [427.0, 474.0]], ('d arrows). In the second row,', 0.9633433222770691)]\n",
      "[[[305.0, 473.0], [546.0, 472.0], [546.0, 485.0], [305.0, 486.0]], ('we compare viewpoints from the proposed method with ran-', 0.9962693452835083)]\n",
      "[[[48.0, 483.0], [287.0, 483.0], [287.0, 496.0], [48.0, 496.0]], ('The proposed random walk algorithm, however, is ineffi-', 0.9816834330558777)]\n",
      "[[[306.0, 484.0], [419.0, 485.0], [419.0, 498.0], [306.0, 497.0]], ('domly sampled viewpoints.', 0.9882146716117859)]\n",
      "[[[49.0, 497.0], [286.0, 497.0], [286.0, 507.0], [49.0, 507.0]], ('cient in terms of exploration. Therefore, we manually select', 0.9956035017967224)]\n",
      "[[[49.0, 509.0], [287.0, 509.0], [287.0, 519.0], [49.0, 519.0]], ('a set of N camera anchors across the 3D scenes as start-', 0.986587405204773)]\n",
      "[[[48.0, 521.0], [288.0, 521.0], [288.0, 531.0], [48.0, 531.0]], ('ing points. After every T steps, we reset the location of', 0.9910291433334351)]\n",
      "[[[308.0, 521.0], [544.0, 521.0], [544.0, 531.0], [308.0, 531.0]], ('diversity of the generated images and results in stronger de-', 0.9931017756462097)]\n",
      "[[[48.0, 532.0], [289.0, 532.0], [289.0, 544.0], [48.0, 544.0]], ('the camera to a randomly sampled camera anchor. We set', 0.9945884943008423)]\n",
      "[[[307.0, 532.0], [547.0, 532.0], [547.0, 544.0], [307.0, 544.0]], ('tector performance. The proposed randomization can also', 0.9860780835151672)]\n",
      "[[[47.0, 542.0], [288.0, 544.0], [288.0, 556.0], [47.0, 555.0]], ('N = 150-200 and T = 100. Note that the selection of cam-', 0.9544042944908142)]\n",
      "[[[306.0, 544.0], [481.0, 544.0], [481.0, 556.0], [306.0, 556.0]], ('benefit sim-to-real domain adaptation [41].', 0.9665300846099854)]\n",
      "[[[48.0, 555.0], [288.0, 555.0], [288.0, 568.0], [48.0, 568.0]], ('era anchors requires only little carefulness. We only need to', 0.9890745282173157)]\n",
      "[[[48.0, 567.0], [288.0, 567.0], [288.0, 580.0], [48.0, 580.0]], ('ensure coverage over the space. It takes around 20 to 30 sec-', 0.9975818395614624)]\n",
      "[[[308.0, 563.0], [441.0, 563.0], [441.0, 576.0], [308.0, 576.0]], ('3.4. Text Region Generation', 0.9946741461753845)]\n",
      "[[[48.0, 579.0], [288.0, 579.0], [288.0, 592.0], [48.0, 592.0]], ('onds for each scene, which is trivial and not a bottleneck of', 0.9809058904647827)]\n",
      "[[[317.0, 581.0], [547.0, 582.0], [547.0, 595.0], [317.0, 594.0]], ('In real-world, text instances are usually embedded on', 0.9956132173538208)]\n",
      "[[[48.0, 591.0], [289.0, 591.0], [289.0, 604.0], [48.0, 604.0]], ('scalability. The manual but efficient selection of camera is', 0.9940233826637268)]\n",
      "[[[49.0, 605.0], [288.0, 605.0], [288.0, 615.0], [49.0, 615.0]], ('compatible with the proposed random walk algorithm that', 0.9903370141983032)]\n",
      "[[[308.0, 596.0], [546.0, 596.0], [546.0, 606.0], [308.0, 606.0]], ('well-defined surfaces, e.g. traffic signs, to maintain good', 0.9923179149627686)]\n",
      "[[[48.0, 616.0], [167.0, 615.0], [167.0, 628.0], [48.0, 629.0]], ('generates diverse viewpoints.', 0.9701984524726868)]\n",
      "[[[308.0, 608.0], [545.0, 608.0], [545.0, 618.0], [308.0, 618.0]], ('legibility. Previous works find suitable regions by using es-', 0.9907707571983337)]\n",
      "[[[306.0, 618.0], [546.0, 619.0], [546.0, 632.0], [306.0, 631.0]], ('timated scene information, such as gPb-UCM [1] in Syn-', 0.9895275831222534)]\n",
      "[[[48.0, 636.0], [206.0, 636.0], [206.0, 648.0], [48.0, 648.0]], ('3.3. Environment Randomization', 0.975346028804779)]\n",
      "[[[308.0, 643.0], [546.0, 643.0], [546.0, 653.0], [308.0, 653.0]], ('tion. However, these methods are imprecise and often fail', 0.9940847158432007)]\n",
      "[[[60.0, 654.0], [287.0, 654.0], [287.0, 667.0], [60.0, 667.0]], ('To produce real-world variations such as lighting condi-', 0.9916972517967224)]\n",
      "[[[306.0, 654.0], [548.0, 654.0], [548.0, 667.0], [306.0, 667.0]], ('to find appropriate regions. Therefore, we propose to find', 0.999692440032959)]\n",
      "[[[48.0, 666.0], [289.0, 666.0], [289.0, 679.0], [48.0, 679.0]], ('tions, we randomly change the intensity, color, and direction', 0.9763279557228088)]\n",
      "[[[306.0, 666.0], [546.0, 666.0], [546.0, 679.0], [306.0, 679.0]], ('text regions by probing around object meshes in 3D world.', 0.9868764877319336)]\n",
      "[[[49.0, 680.0], [287.0, 680.0], [287.0, 690.0], [49.0, 690.0]], ('of all light sources in the scene. In addition to illuminations,', 0.9917776584625244)]\n",
      "[[[306.0, 677.0], [547.0, 678.0], [547.0, 691.0], [306.0, 690.0]], ('Since inspecting all object meshes is time-consuming, we', 0.994269073009491)]\n",
      "[[[49.0, 692.0], [287.0, 692.0], [287.0, 702.0], [49.0, 702.0]], ('we also add fog conditions and randomly adjust its inten-', 0.9845709800720215)]\n",
      "[[[307.0, 691.0], [548.0, 691.0], [548.0, 704.0], [307.0, 704.0]], ('propose a 2-staged pipeline: (1) We retrieve ground truth', 0.9934518337249756)]\n",
      "[[[48.0, 703.0], [287.0, 703.0], [287.0, 713.0], [48.0, 713.0]], ('sity. The environment randomization proves to increase the', 0.9912081360816956)]\n",
      "[[[306.0, 703.0], [546.0, 703.0], [546.0, 716.0], [306.0, 716.0]], ('surface normal map to generate initial text region propos-', 0.991488516330719)]\n",
      "[2024/10/01 22:14:02] ppocr DEBUG: dt_boxes num : 98, elapsed : 0.40686798095703125\n",
      "[2024/10/01 22:14:02] ppocr DEBUG: cls num  : 98, elapsed : 0.7353968620300293\n",
      "[2024/10/01 22:14:27] ppocr DEBUG: rec_res num  : 98, elapsed : 24.46180486679077\n",
      "[[[48.0, 73.0], [288.0, 73.0], [288.0, 86.0], [48.0, 86.0]], ('als; (2) Initial proposals are then projected to and refined', 0.9944945573806763)]\n",
      "[[[48.0, 85.0], [289.0, 85.0], [289.0, 98.0], [48.0, 98.0]], ('in the 3D world using object meshes. Finally, we sample', 0.9876179099082947)]\n",
      "[[[48.0, 97.0], [288.0, 97.0], [288.0, 110.0], [48.0, 110.0]], ('a subset from the refined proposals to render. To avoid oc-', 0.9924224615097046)]\n",
      "[[[48.0, 110.0], [289.0, 110.0], [289.0, 123.0], [48.0, 123.0]], ('clusion among proposals, we project them back to screen', 0.995241105556488)]\n",
      "[[[47.0, 122.0], [289.0, 121.0], [289.0, 134.0], [47.0, 135.0]], ('space, and discard regions that overlap with each other one', 0.9931566119194031)]\n",
      "[[[413.0, 123.0], [445.0, 123.0], [445.0, 133.0], [413.0, 133.0]], ('Front View', 0.9984644055366516)]\n",
      "[[[461.0, 124.0], [496.0, 124.0], [496.0, 131.0], [461.0, 131.0]], ('(3) Create O', 0.9480482935905457)]\n",
      "[[[508.0, 123.0], [543.0, 123.0], [543.0, 133.0], [508.0, 133.0]], ('gonal Square', 0.9881457686424255)]\n",
      "[[[49.0, 135.0], [269.0, 135.0], [269.0, 145.0], [49.0, 145.0]], ('by one in a shuffled order until occlusion is eliminated..', 0.9721236824989319)]\n",
      "[[[49.0, 160.0], [81.0, 160.0], [81.0, 170.0], [49.0, 170.0]], ('3.4.1', 0.931932270526886)]\n",
      "[[[75.0, 157.0], [236.0, 159.0], [235.0, 172.0], [74.0, 170.0]], ('Initial Proposals from Normal Maps', 0.9992381930351257)]\n",
      "[[[48.0, 178.0], [289.0, 178.0], [289.0, 191.0], [48.0, 191.0]], ('In computer graphics, normal values are unit vectors that', 0.9943104386329651)]\n",
      "[[[49.0, 191.0], [288.0, 191.0], [288.0, 201.0], [49.0, 201.0]], ('are perpendicular to a surface. Therefore, when projected', 0.9890049695968628)]\n",
      "[[[318.0, 184.0], [364.0, 184.0], [364.0, 191.0], [318.0, 191.0]], ('(4) Original Vier', 0.9531917572021484)]\n",
      "[[[48.0, 203.0], [287.0, 203.0], [287.0, 213.0], [48.0, 213.0]], ('to 2D screen space, a region with similar normal values', 0.9956006407737732)]\n",
      "[[[306.0, 198.0], [545.0, 198.0], [545.0, 211.0], [306.0, 211.0]], ('Figure 4: Illustration of the refinement of initial proposals.', 0.9860216975212097)]\n",
      "[[[49.0, 215.0], [288.0, 215.0], [288.0, 225.0], [49.0, 225.0]], ('tends to be a well-defined region to embed text on. We find', 0.9883042573928833)]\n",
      "[[[306.0, 209.0], [547.0, 210.0], [547.0, 223.0], [306.0, 222.0]], ('We draw green bounding boxes to represent proposals in 2D', 0.9959113001823425)]\n",
      "[[[306.0, 223.0], [547.0, 223.0], [547.0, 236.0], [306.0, 236.0]], ('screen space, and use planar meshes to represent proposals', 0.996661365032196)]\n",
      "[[[48.0, 238.0], [289.0, 238.0], [289.0, 250.0], [48.0, 250.0]], ('pixels across the surface normal map, and retrieve those', 0.9883793592453003)]\n",
      "[[[306.0, 235.0], [546.0, 235.0], [546.0, 248.0], [306.0, 248.0]], ('in 3D space. (1) Initial proposals are made in 2D space.', 0.984527587890625)]\n",
      "[[[49.0, 250.0], [287.0, 250.0], [287.0, 260.0], [49.0, 260.0]], ('with smoorh surface normal: the minimum cosine similar-', 0.9825463891029358)]\n",
      "[[[307.0, 247.0], [547.0, 247.0], [547.0, 259.0], [307.0, 259.0]], ('(2) When we project them into 3D world and inspect them', 0.9951202273368835)]\n",
      "[[[47.0, 260.0], [288.0, 260.0], [288.0, 273.0], [47.0, 273.0]], ('ity value between any two pixels is larger than a threshold', 0.9797255992889404)]\n",
      "[[[306.0, 258.0], [548.0, 258.0], [548.0, 271.0], [306.0, 271.0]], ('from the front view, they are in distorted forms. (3) Based', 0.9991689324378967)]\n",
      "[[[49.0, 276.0], [61.0, 276.0], [61.0, 283.0], [49.0, 283.0]], ('I.', 0.9017543792724609)]\n",
      "[[[59.0, 273.0], [289.0, 273.0], [289.0, 286.0], [59.0, 286.0]], ('We set t to 0.95, which proves to produce reasonable', 0.9938322305679321)]\n",
      "[[[306.0, 270.0], [548.0, 270.0], [548.0, 283.0], [306.0, 283.0]], ('on the sizes of the distorted proposals and the positions of', 0.9959955811500549)]\n",
      "[[[46.0, 284.0], [288.0, 285.0], [288.0, 298.0], [46.0, 297.0]], ('results. We randomly sample at most 10 non-overlapping', 0.9928316473960876)]\n",
      "[[[307.0, 284.0], [547.0, 284.0], [547.0, 294.0], [307.0, 294.0]], ('the center points, we re-initialize orthogonal squares on the', 0.9947490096092224)]\n",
      "[[[47.0, 296.0], [288.0, 298.0], [288.0, 311.0], [47.0, 309.0]], ('valid image regions to make the initial proposals. Making', 0.9785525798797607)]\n",
      "[[[306.0, 295.0], [545.0, 295.0], [545.0, 305.0], [306.0, 305.0]], ('same surfaces with horizontal sides orthogonal to the grav-', 0.985126256942749)]\n",
      "[[[49.0, 311.0], [286.0, 311.0], [286.0, 321.0], [49.0, 321.0]], ('od aan nn nnn nnnnnnnn nn an adnnn nnnnnn mnan arnandnad', 0.5697585940361023)]\n",
      "[[[307.0, 307.0], [546.0, 307.0], [546.0, 320.0], [307.0, 320.0]], ('ity direction. (5) Then we expand the squares. (6) Finally,', 0.9980679154396057)]\n",
      "[[[49.0, 323.0], [156.0, 323.0], [156.0, 333.0], [49.0, 333.0]], ('tential and visible regions..', 0.9707010388374329)]\n",
      "[[[306.0, 318.0], [546.0, 318.0], [546.0, 331.0], [306.0, 331.0]], ('we obtain text regions in 2D screen space with natural per-', 0.9901125431060791)]\n",
      "[[[307.0, 331.0], [385.0, 332.0], [384.0, 343.0], [307.0, 342.0]], ('spective distortion.', 0.9819797277450562)]\n",
      "[[[49.0, 348.0], [79.0, 348.0], [79.0, 358.0], [49.0, 358.0]], ('3.4.2', 0.9586108326911926)]\n",
      "[[[75.0, 346.0], [220.0, 346.0], [220.0, 359.0], [75.0, 359.0]], ('Refining Proposals in 3D Worlds', 0.9992250800132751)]\n",
      "[[[306.0, 354.0], [547.0, 354.0], [547.0, 367.0], [306.0, 367.0]], ('does. We also use the same text corpus, Newsgroup20. The', 0.9970539212226868)]\n",
      "[[[49.0, 367.0], [126.0, 367.0], [126.0, 377.0], [49.0, 377.0]], ('As shown in Fig.', 0.9994451403617859)]\n",
      "[[[128.0, 366.0], [288.0, 366.0], [288.0, 379.0], [128.0, 379.0]], ('4, rectangular initial proposals in 2D', 0.9763500094413757)]\n",
      "[[[307.0, 366.0], [548.0, 366.0], [548.0, 379.0], [307.0, 379.0]], ('generated text images have zero alpha values on non-stroke', 0.9967700242996216)]\n",
      "[[[48.0, 378.0], [289.0, 378.0], [289.0, 391.0], [48.0, 391.0]], ('screen space will be distorted when projected into 3D', 0.9923900365829468)]\n",
      "[[[307.0, 379.0], [433.0, 379.0], [433.0, 389.0], [307.0, 389.0]], ('pixels, and non zero for others..', 0.9832482933998108)]\n",
      "[[[48.0, 390.0], [289.0, 390.0], [289.0, 403.0], [48.0, 403.0]], ('world. Thus, we need to first rectify the proposals in 3D', 0.992337703704834)]\n",
      "[[[307.0, 389.0], [546.0, 389.0], [546.0, 402.0], [307.0, 402.0]], ('Rendering Text in 3D World: We first perform triangula-', 0.9900409579277039)]\n",
      "[[[48.0, 402.0], [289.0, 402.0], [289.0, 415.0], [48.0, 415.0]], ('world. We project the center point of the initial proposals', 0.9872714281082153)]\n",
      "[[[306.0, 402.0], [547.0, 402.0], [547.0, 415.0], [306.0, 415.0]], ('tion for the refined proposals to generate planar triangular', 0.9861112236976624)]\n",
      "[[[47.0, 415.0], [289.0, 415.0], [289.0, 428.0], [47.0, 428.0]], ('into 3D space, and re-initialize orthogonal squares on the', 0.9948622584342957)]\n",
      "[[[307.0, 414.0], [546.0, 414.0], [546.0, 427.0], [307.0, 427.0]], ('meshes that are closely attached to the underlying surface.', 0.9988042116165161)]\n",
      "[[[49.0, 428.0], [288.0, 428.0], [288.0, 438.0], [49.0, 438.0]], ('corresponding mesh surfaces around the center points: the', 0.9980829954147339)]\n",
      "[[[308.0, 426.0], [548.0, 426.0], [548.0, 439.0], [308.0, 439.0]], ('Then we load the text images as texture onto the generated', 0.9885067343711853)]\n",
      "[[[48.0, 439.0], [289.0, 439.0], [289.0, 451.0], [48.0, 451.0]], ('horizontal sides are orthogonal to the gravity direction. The', 0.9971108436584473)]\n",
      "[[[307.0, 438.0], [547.0, 438.0], [547.0, 450.0], [307.0, 450.0]], ('meshes.We also randomly sample the texture attributes,', 0.9976394176483154)]\n",
      "[[[48.0, 449.0], [289.0, 449.0], [289.0, 462.0], [48.0, 462.0]], ('side lengths are set to the shortest sides of the quadrilaterals', 0.9922388792037964)]\n",
      "[[[307.0, 449.0], [510.0, 449.0], [510.0, 462.0], [307.0, 462.0]], ('such as the ratio of diffuse and specular reflection.', 0.99847412109375)]\n",
      "[[[48.0, 461.0], [287.0, 461.0], [287.0, 474.0], [48.0, 474.0]], ('created by projecting the four corners of initial proposals', 0.9975799322128296)]\n",
      "[[[47.0, 473.0], [288.0, 473.0], [288.0, 486.0], [47.0, 486.0]], ('into the 3D space. Then we enlarge the widths and heights', 0.9790819883346558)]\n",
      "[[[308.0, 467.0], [440.0, 467.0], [440.0, 480.0], [308.0, 480.0]], ('3.6. Implementation Details', 0.9979144334793091)]\n",
      "[[[48.0, 485.0], [288.0, 485.0], [288.0, 498.0], [48.0, 498.0]], ('along the horizontal and vertical sides alternatively. The', 0.9742209911346436)]\n",
      "[[[319.0, 486.0], [547.0, 486.0], [547.0, 499.0], [319.0, 499.0]], ('The proposed synthesis engine is implemented based on', 0.9947448372840881)]\n",
      "[[[48.0, 497.0], [287.0, 497.0], [287.0, 510.0], [48.0, 510.0]], ('expansion of one direction stops when the sides of that di-', 0.9987729787826538)]\n",
      "[[[306.0, 497.0], [546.0, 498.0], [546.0, 511.0], [306.0, 510.0]], ('UE4.22 and the UnrealCV plugin. On an ubuntu worksta-', 0.9988872408866882)]\n",
      "[[[49.0, 511.0], [288.0, 511.0], [288.0, 521.0], [49.0, 521.0]], ('rection get off the surface, hit other meshes, or reach the', 0.9925897717475891)]\n",
      "[[[307.0, 510.0], [547.0, 510.0], [547.0, 523.0], [307.0, 523.0]], ('tion with an 8-core Intel CPU, an NVIDIA GeForce RTX', 0.976409375667572)]\n",
      "[[[49.0, 522.0], [286.0, 522.0], [286.0, 532.0], [49.0, 532.0]], ('preset maximum expansion ratio. The proposed refining al-', 0.9881739020347595)]\n",
      "[[[307.0, 523.0], [548.0, 523.0], [548.0, 536.0], [307.0, 536.0]], ('2070 GPU, and 16G RAM, the synthesis speed is 0.7-1.5', 0.99411940574646)]\n",
      "[[[48.0, 534.0], [289.0, 534.0], [289.0, 546.0], [48.0, 546.0]], ('gorithm works in 3D world space, and is able to produce', 0.9986700415611267)]\n",
      "[[[48.0, 544.0], [272.0, 544.0], [272.0, 557.0], [48.0, 557.0]], ('natural homography transformation in 2D screen space.', 0.9964227676391602)]\n",
      "[[[306.0, 535.0], [546.0, 534.0], [546.0, 546.0], [306.0, 547.0]], ('seconds per image with a resolution of 1080  720, depend-', 0.9915326237678528)]\n",
      "[[[307.0, 546.0], [478.0, 546.0], [478.0, 559.0], [307.0, 559.0]], ('ing on the complexity of the scene model.', 0.999626636505127)]\n",
      "[[[47.0, 562.0], [144.0, 563.0], [144.0, 577.0], [47.0, 576.0]], ('3.5. Text Rendering', 0.9717376232147217)]\n",
      "[[[318.0, 557.0], [545.0, 557.0], [545.0, 570.0], [318.0, 570.0]], ('We collect 30 scene models from the official UE4 mar-', 0.9906273484230042)]\n",
      "[[[306.0, 570.0], [547.0, 570.0], [547.0, 583.0], [306.0, 583.0]], ('ketplace. The engine is used to generate 600K scene text', 0.982787549495697)]\n",
      "[[[48.0, 582.0], [289.0, 582.0], [289.0, 595.0], [48.0, 595.0]], ('Generating Text Images: Given text regions as proposed', 0.9989711046218872)]\n",
      "[[[307.0, 583.0], [430.0, 583.0], [430.0, 593.0], [307.0, 593.0]], ('images with English words.', 0.9981873035430908)]\n",
      "[[[435.0, 582.0], [545.0, 583.0], [544.0, 594.0], [435.0, 593.0]], ('With the same configura-', 0.99102783203125)]\n",
      "[[[49.0, 596.0], [287.0, 596.0], [287.0, 606.0], [49.0, 606.0]], ('and refined in section 3.4, the text generation module sam-', 0.9837824106216431)]\n",
      "[[[307.0, 596.0], [546.0, 596.0], [546.0, 606.0], [307.0, 606.0]], ('tion, we also generate a multilingual version, making it the', 0.9986363649368286)]\n",
      "[[[49.0, 608.0], [288.0, 608.0], [288.0, 618.0], [49.0, 618.0]], ('ples text content and renders text images with certain fonts', 0.9991617798805237)]\n",
      "[[[307.0, 608.0], [461.0, 608.0], [461.0, 618.0], [307.0, 618.0]], ('largest multilingual scene text dataset.', 0.9964854121208191)]\n",
      "[[[48.0, 618.0], [288.0, 619.0], [288.0, 632.0], [48.0, 631.0]], ('and text colors. The numbers of lines and characters per', 0.992400586605072)]\n",
      "[[[48.0, 632.0], [288.0, 632.0], [288.0, 642.0], [48.0, 642.0]], ('line are determined by the font size and the size of refined', 0.9921799898147583)]\n",
      "[[[308.0, 629.0], [511.0, 629.0], [511.0, 642.0], [308.0, 642.0]], ('4. Experiments on Scene Text Detection', 0.9857898354530334)]\n",
      "[[[48.0, 643.0], [289.0, 643.0], [289.0, 655.0], [48.0, 655.0]], ('proposals in 2D space to make sure the characters are not', 0.9992048144340515)]\n",
      "[[[306.0, 645.0], [367.0, 648.0], [366.0, 662.0], [306.0, 659.0]], ('4.1. Settings', 0.995728075504303)]\n",
      "[[[48.0, 656.0], [287.0, 656.0], [287.0, 666.0], [48.0, 666.0]], ('too small and ensure legibility. For a fairer comparison, we', 0.9950841665267944)]\n",
      "[[[49.0, 668.0], [288.0, 668.0], [288.0, 678.0], [49.0, 678.0]], ('also use the same font set from Google Fonts  as SynthText', 0.9914097189903259)]\n",
      "[[[319.0, 666.0], [547.0, 666.0], [547.0, 679.0], [319.0, 679.0]], ('We first verify the effectiveness of the proposed engine', 0.993371307849884)]\n",
      "[[[60.0, 685.0], [287.0, 685.0], [287.0, 695.0], [60.0, 695.0]], (' when the distances from the rectangular proposals corners to the near-', 0.9795682430267334)]\n",
      "[[[307.0, 680.0], [545.0, 680.0], [545.0, 690.0], [307.0, 690.0]], ('by training detectors on the synthesized images and evaluat-', 0.9960861206054688)]\n",
      "[[[49.0, 695.0], [260.0, 695.0], [260.0, 705.0], [49.0, 705.0]], ('est point on the underlying surface mesh excred certain threshold', 0.9898741245269775)]\n",
      "[[[307.0, 691.0], [546.0, 691.0], [546.0, 704.0], [307.0, 704.0]], ('ing them on real image datasets. We use a previous yet time-', 0.997974157333374)]\n",
      "[[[60.0, 705.0], [183.0, 705.0], [183.0, 715.0], [60.0, 715.0]], ('2https://fonts.google.com/', 0.947959840297699)]\n",
      "[[[307.0, 704.0], [547.0, 704.0], [547.0, 714.0], [307.0, 714.0]], ('tested state-of-the-art model, EAST [53], which is fast and', 0.9962226748466492)]\n",
      "[2024/10/01 22:14:28] ppocr DEBUG: dt_boxes num : 182, elapsed : 0.281998872756958\n",
      "[2024/10/01 22:14:29] ppocr DEBUG: cls num  : 182, elapsed : 1.0797269344329834\n",
      "[2024/10/01 22:15:00] ppocr DEBUG: rec_res num  : 182, elapsed : 31.299883127212524\n",
      "[[[111.0, 70.0], [225.0, 70.0], [225.0, 83.0], [111.0, 83.0]], ('Evaluation on ICDAR 2015', 0.984616219997406)]\n",
      "[[[306.0, 73.0], [544.0, 73.0], [544.0, 83.0], [306.0, 83.0]], ('ule and the environment randomization module in increas-', 0.9907665252685547)]\n",
      "[[[96.0, 84.0], [154.0, 84.0], [154.0, 95.0], [96.0, 95.0]], ('Training Data', 0.999940037727356)]\n",
      "[[[203.0, 85.0], [211.0, 85.0], [211.0, 95.0], [203.0, 95.0]], ('P', 0.9963750243186951)]\n",
      "[[[260.0, 86.0], [275.0, 86.0], [275.0, 94.0], [260.0, 94.0]], ('F1', 0.9907872080802917)]\n",
      "[[[306.0, 85.0], [458.0, 85.0], [458.0, 98.0], [306.0, 98.0]], ('ing the diversity of synthetic images.', 0.9887080788612366)]\n",
      "[[[112.0, 96.0], [136.0, 96.0], [136.0, 108.0], [112.0, 108.0]], ('IC15', 0.9998765587806702)]\n",
      "[[[196.0, 96.0], [220.0, 98.0], [218.0, 110.0], [195.0, 108.0]], ('84.6', 0.9973722100257874)]\n",
      "[[[226.0, 97.0], [248.0, 97.0], [248.0, 108.0], [226.0, 108.0]], ('78.5', 0.9951024055480957)]\n",
      "[[[255.0, 97.0], [278.0, 97.0], [278.0, 108.0], [255.0, 108.0]], ('81.4', 0.9950650334358215)]\n",
      "[[[76.0, 108.0], [171.0, 108.0], [171.0, 121.0], [76.0, 121.0]], ('IC15 + SynthText 10K', 0.9993646740913391)]\n",
      "[[[197.0, 108.0], [218.0, 108.0], [218.0, 120.0], [197.0, 120.0]], ('85.6', 0.9994661808013916)]\n",
      "[[[256.0, 108.0], [277.0, 108.0], [277.0, 120.0], [256.0, 120.0]], ('82.4', 0.9996365308761597)]\n",
      "[[[308.0, 99.0], [547.0, 99.0], [547.0, 112.0], [308.0, 112.0]], ('Ablating Viewfinder Module We derive two baselines', 0.9852102398872375)]\n",
      "[[[226.0, 109.0], [247.0, 109.0], [247.0, 120.0], [226.0, 120.0]], ('79.5', 0.9994555711746216)]\n",
      "[[[86.0, 121.0], [163.0, 121.0], [163.0, 133.0], [86.0, 133.0]], ('IC15 + VISD 10K', 0.9713543057441711)]\n",
      "[[[197.0, 121.0], [218.0, 121.0], [218.0, 133.0], [197.0, 133.0]], ('86.3', 0.9996064305305481)]\n",
      "[[[226.0, 121.0], [248.0, 121.0], [248.0, 132.0], [226.0, 132.0]], ('80.0', 0.9981128573417664)]\n",
      "[[[255.0, 119.0], [278.0, 119.0], [278.0, 134.0], [255.0, 134.0]], ('83.1', 0.9974121451377869)]\n",
      "[[[307.0, 113.0], [546.0, 113.0], [546.0, 123.0], [307.0, 123.0]], ('from the proposed viewfinder module: (1) Random View-', 0.9948164224624634)]\n",
      "[[[71.0, 132.0], [176.0, 132.0], [176.0, 145.0], [71.0, 145.0]], ('IC15 + SynthText3D 10K', 0.9687173962593079)]\n",
      "[[[306.0, 123.0], [545.0, 124.0], [545.0, 137.0], [306.0, 136.0]], ('point + Manual Anchor that randomly samples camera lo-', 0.9966410398483276)]\n",
      "[[[197.0, 133.0], [219.0, 133.0], [219.0, 145.0], [197.0, 145.0]], ('86.6', 0.9990580081939697)]\n",
      "[[[226.0, 133.0], [247.0, 133.0], [247.0, 145.0], [226.0, 145.0]], ('80.4', 0.999431848526001)]\n",
      "[[[256.0, 133.0], [277.0, 133.0], [277.0, 144.0], [256.0, 144.0]], ('83.4', 0.9994043707847595)]\n",
      "[[[74.0, 144.0], [173.0, 144.0], [173.0, 156.0], [74.0, 156.0]], ('IC15 + UnrealText 10K', 0.9972773790359497)]\n",
      "[[[196.0, 145.0], [219.0, 145.0], [219.0, 155.0], [196.0, 155.0]], ('86.9', 0.9935118556022644)]\n",
      "[[[226.0, 145.0], [248.0, 145.0], [248.0, 155.0], [226.0, 155.0]], ('81.0', 0.9992513060569763)]\n",
      "[[[256.0, 145.0], [279.0, 145.0], [279.0, 155.0], [256.0, 155.0]], ('88', 0.8938997983932495)]\n",
      "[[[306.0, 136.0], [548.0, 136.0], [548.0, 148.0], [306.0, 148.0]], ('cations and rotations from the norm-ball spaces centered', 0.9858469367027283)]\n",
      "[[[72.0, 157.0], [173.0, 157.0], [173.0, 167.0], [72.0, 167.0]], ('IC15 + UnrealText 600K', 0.9946829080581665)]\n",
      "[[[224.0, 157.0], [280.0, 157.0], [280.0, 167.0], [224.0, 167.0]], ('80.8 84.5', 0.9102046489715576)]\n",
      "[[[307.0, 148.0], [548.0, 148.0], [548.0, 160.0], [307.0, 160.0]], ('around auxiliary camera anchors. (2) Random Viewpoint', 0.983757734298706)]\n",
      "[[[196.0, 158.0], [218.0, 158.0], [218.0, 166.0], [196.0, 166.0]], ('88.5', 0.9979780912399292)]\n",
      "[[[111.0, 168.0], [224.0, 168.0], [224.0, 181.0], [111.0, 181.0]], ('Evaluation on ICDAR 2013', 0.9893690943717957)]\n",
      "[[[307.0, 160.0], [546.0, 160.0], [546.0, 170.0], [307.0, 170.0]], ('Onfy that randomly samples camera locations and rotations', 0.9874160885810852)]\n",
      "[[[308.0, 172.0], [545.0, 172.0], [545.0, 182.0], [308.0, 182.0]], ('from the whole scene space, without checking their qual-', 0.9943508505821228)]\n",
      "[[[96.0, 182.0], [153.0, 182.0], [153.0, 193.0], [96.0, 193.0]], ('Training Data', 0.9999480843544006)]\n",
      "[[[204.0, 184.0], [209.0, 184.0], [209.0, 190.0], [204.0, 190.0]], ('F', 0.7862427830696106)]\n",
      "[[[234.0, 184.0], [241.0, 184.0], [241.0, 191.0], [234.0, 191.0]], ('R', 0.9833353757858276)]\n",
      "[[[260.0, 183.0], [276.0, 183.0], [276.0, 191.0], [260.0, 191.0]], ('F1', 0.9877926707267761)]\n",
      "[[[307.0, 184.0], [547.0, 184.0], [547.0, 194.0], [307.0, 194.0]], ('ity. For experiments, we fix the number of scenes to 10 to', 0.9966228604316711)]\n",
      "[[[113.0, 194.0], [137.0, 194.0], [137.0, 205.0], [113.0, 205.0]], ('IC13', 0.9683946371078491)]\n",
      "[[[199.0, 196.0], [216.0, 196.0], [216.0, 204.0], [199.0, 204.0]], ('82.6', 0.9999027252197266)]\n",
      "[[[226.0, 194.0], [248.0, 194.0], [248.0, 205.0], [226.0, 205.0]], ('70.0', 0.9933606386184692)]\n",
      "[[[257.0, 194.0], [279.0, 194.0], [279.0, 205.0], [257.0, 205.0]], ('75.8', 0.9871882200241089)]\n",
      "[[[76.0, 205.0], [171.0, 205.0], [171.0, 218.0], [76.0, 218.0]], ('IC13 + SynthText 10K', 0.9973499178886414)]\n",
      "[[[307.0, 196.0], [547.0, 196.0], [547.0, 206.0], [307.0, 206.0]], ('control scene diversity and generate different numbers of', 0.9915494322776794)]\n",
      "[[[198.0, 206.0], [217.0, 206.0], [217.0, 217.0], [198.0, 217.0]], ('85.3', 0.9997423887252808)]\n",
      "[[[257.0, 206.0], [277.0, 206.0], [277.0, 217.0], [257.0, 217.0]], ('78.3', 0.9996153712272644)]\n",
      "[[[257.0, 215.0], [278.0, 218.0], [277.0, 230.0], [255.0, 227.0]], ('79.0', 0.997694730758667)]\n",
      "[[[306.0, 207.0], [547.0, 207.0], [547.0, 220.0], [306.0, 220.0]], ('images, and compare their performance curve. By fixing', 0.9871098399162292)]\n",
      "[[[87.0, 218.0], [161.0, 218.0], [161.0, 229.0], [87.0, 229.0]], ('IC13 + VISD 10K', 0.9800504446029663)]\n",
      "[[[198.0, 218.0], [217.0, 218.0], [217.0, 229.0], [198.0, 229.0]], ('85.9', 0.9998356699943542)]\n",
      "[[[226.0, 218.0], [247.0, 218.0], [247.0, 229.0], [226.0, 229.0]], ('73.1', 0.9989407062530518)]\n",
      "[[[70.0, 229.0], [177.0, 229.0], [177.0, 242.0], [70.0, 242.0]], ('IC13 + SynthText3D 10K', 0.9826413989067078)]\n",
      "[[[307.0, 220.0], [545.0, 220.0], [545.0, 230.0], [307.0, 230.0]], ('the number of scenes, we compare how well different view', 0.9991570711135864)]\n",
      "[[[198.0, 230.0], [218.0, 230.0], [218.0, 242.0], [198.0, 242.0]], ('86.4', 0.9994565844535828)]\n",
      "[[[226.0, 230.0], [248.0, 230.0], [248.0, 241.0], [226.0, 241.0]], ('73.0', 0.9962420463562012)]\n",
      "[[[257.0, 230.0], [278.0, 230.0], [278.0, 242.0], [257.0, 242.0]], ('16', 0.9418854713439941)]\n",
      "[[[307.0, 231.0], [466.0, 231.0], [466.0, 244.0], [307.0, 244.0]], ('finding methods can exploit the scenes.', 0.9769843816757202)]\n",
      "[[[74.0, 241.0], [172.0, 241.0], [172.0, 253.0], [74.0, 253.0]], ('IC13 + UnrealText 10K', 0.9970898628234863)]\n",
      "[[[196.0, 242.0], [217.0, 242.0], [217.0, 252.0], [196.0, 252.0]], ('88.5', 0.996742844581604)]\n",
      "[[[226.0, 242.0], [248.0, 242.0], [248.0, 252.0], [226.0, 252.0]], ('74.7', 0.9994592666625977)]\n",
      "[[[255.0, 242.0], [279.0, 242.0], [279.0, 252.0], [255.0, 252.0]], ('81.0', 0.99051833152771)]\n",
      "[[[71.0, 252.0], [173.0, 253.0], [173.0, 264.0], [70.0, 263.0]], ('IC13 + UnrealText 600K', 0.9774402379989624)]\n",
      "[[[197.0, 253.0], [219.0, 253.0], [219.0, 264.0], [197.0, 264.0]], ('92.3', 0.9933479428291321)]\n",
      "[[[225.0, 253.0], [280.0, 253.0], [280.0, 263.0], [225.0, 263.0]], ('73.481.8', 0.9431889057159424)]\n",
      "[[[309.0, 246.0], [547.0, 246.0], [547.0, 255.0], [309.0, 255.0]], ('Ablating Environment Randomization We remove the', 0.9843906760215759)]\n",
      "[[[116.0, 265.0], [220.0, 265.0], [220.0, 279.0], [116.0, 279.0]], ('Evaluation on MLT 2017', 0.9994779825210571)]\n",
      "[[[307.0, 256.0], [547.0, 256.0], [547.0, 269.0], [307.0, 269.0]], ('environment randomization module, and keep the scene', 0.985961377620697)]\n",
      "[[[96.0, 279.0], [153.0, 279.0], [153.0, 290.0], [96.0, 290.0]], ('Training Data', 0.9999480843544006)]\n",
      "[[[308.0, 270.0], [546.0, 270.0], [546.0, 280.0], [308.0, 280.0]], ('models unchanged during synthesis. For experiments, we', 0.9935559630393982)]\n",
      "[[[204.0, 281.0], [211.0, 281.0], [211.0, 288.0], [204.0, 288.0]], ('P', 0.9894486665725708)]\n",
      "[[[232.0, 280.0], [243.0, 280.0], [243.0, 288.0], [232.0, 288.0]], ('R', 0.9968001842498779)]\n",
      "[[[259.0, 280.0], [278.0, 280.0], [278.0, 288.0], [259.0, 288.0]], ('F1', 0.9794521927833557)]\n",
      "[[[308.0, 282.0], [546.0, 282.0], [546.0, 292.0], [308.0, 292.0]], ('fix the total number of images to 10K and use different', 0.982257068157196)]\n",
      "[[[102.0, 292.0], [147.0, 292.0], [147.0, 303.0], [102.0, 303.0]], ('MLT 2017', 0.9995974898338318)]\n",
      "[[[198.0, 292.0], [220.0, 292.0], [220.0, 303.0], [198.0, 303.0]], ('72.9', 0.9765718579292297)]\n",
      "[[[225.0, 292.0], [248.0, 292.0], [248.0, 303.0], [225.0, 303.0]], ('67.4', 0.9991576671600342)]\n",
      "[[[254.0, 292.0], [278.0, 292.0], [278.0, 303.0], [254.0, 303.0]], ('10L', 0.636114239692688)]\n",
      "[[[64.0, 303.0], [183.0, 303.0], [183.0, 316.0], [64.0, 316.0]], ('MLT 2017 + SynthText 10K', 0.9912822842597961)]\n",
      "[[[256.0, 303.0], [278.0, 303.0], [278.0, 315.0], [256.0, 315.0]], ('70.3', 0.9933835864067078)]\n",
      "[[[308.0, 294.0], [546.0, 294.0], [546.0, 304.0], [308.0, 304.0]], ('number of scenes. In this way, we can compare the diversity', 0.9946109056472778)]\n",
      "[[[198.0, 304.0], [217.0, 304.0], [217.0, 315.0], [198.0, 315.0]], ('73.1', 0.9996105432510376)]\n",
      "[[[200.0, 313.0], [218.0, 315.0], [216.0, 325.0], [199.0, 322.0]], ('73.3', 0.9968511462211609)]\n",
      "[[[226.0, 304.0], [248.0, 304.0], [248.0, 315.0], [226.0, 315.0]], ('67.7', 0.9969264268875122)]\n",
      "[[[74.0, 314.0], [173.0, 314.0], [173.0, 327.0], [74.0, 327.0]], ('MLT 2017 + VISD 10K', 0.9910473227500916)]\n",
      "[[[256.0, 314.0], [277.0, 314.0], [277.0, 326.0], [256.0, 326.0]], ('70.5', 0.9985212087631226)]\n",
      "[[[307.0, 305.0], [486.0, 305.0], [486.0, 318.0], [307.0, 318.0]], ('of images generated with different methods.', 0.9961951375007629)]\n",
      "[[[226.0, 315.0], [248.0, 315.0], [248.0, 326.0], [226.0, 326.0]], ('67.9', 0.999479353427887)]\n",
      "[[[59.0, 326.0], [188.0, 326.0], [188.0, 339.0], [59.0, 339.0]], ('MLT 2017 + SynthText3D 10K', 0.9973612427711487)]\n",
      "[[[226.0, 327.0], [248.0, 327.0], [248.0, 339.0], [226.0, 339.0]], ('67.6', 0.9995366930961609)]\n",
      "[[[256.0, 327.0], [278.0, 327.0], [278.0, 339.0], [256.0, 339.0]], ('70.6', 0.9984827637672424)]\n",
      "[[[319.0, 318.0], [546.0, 318.0], [546.0, 331.0], [319.0, 331.0]], ('We train the EAST models with different numbers of im-', 0.9881523847579956)]\n",
      "[[[62.0, 338.0], [184.0, 338.0], [184.0, 350.0], [62.0, 350.0]], ('MLT 2017 + UnrealText 10K', 0.9979899525642395)]\n",
      "[[[200.0, 329.0], [217.0, 329.0], [217.0, 337.0], [200.0, 337.0]], ('73.8', 0.9996352195739746)]\n",
      "[[[226.0, 339.0], [248.0, 339.0], [248.0, 349.0], [226.0, 349.0]], ('68.7', 0.9968011379241943)]\n",
      "[[[255.0, 339.0], [279.0, 339.0], [279.0, 349.0], [255.0, 349.0]], ('71.6', 0.9937832951545715)]\n",
      "[[[306.0, 331.0], [548.0, 331.0], [548.0, 344.0], [306.0, 344.0]], ('ages or scenes, evaluate them on the 3 real datasets, and', 0.984032928943634)]\n",
      "[[[55.0, 350.0], [185.0, 350.0], [185.0, 363.0], [55.0, 363.0]], ('MLT 2017 + UnrealText 600K', 0.9751354455947876)]\n",
      "[[[199.0, 341.0], [217.0, 341.0], [217.0, 347.0], [199.0, 347.0]], ('74.6', 0.9754649996757507)]\n",
      "[[[305.0, 342.0], [546.0, 342.0], [546.0, 354.0], [305.0, 354.0]], ('compute the arithmetic mean of the F1-scores. As shown', 0.9927430748939514)]\n",
      "[[[227.0, 353.0], [247.0, 353.0], [247.0, 361.0], [227.0, 361.0]], ('67.4', 0.9995139837265015)]\n",
      "[[[256.0, 353.0], [279.0, 353.0], [279.0, 361.0], [256.0, 361.0]], ('74.1', 0.9971347451210022)]\n",
      "[[[305.0, 352.0], [545.0, 352.0], [545.0, 365.0], [305.0, 365.0]], ('in Fig. 5 (a), we observe that the proposed combination,', 0.9891507029533386)]\n",
      "[[[47.0, 367.0], [287.0, 368.0], [287.0, 382.0], [47.0, 381.0]], ('Table 2: Detection performances of EAST models pre-', 0.9800520539283752)]\n",
      "[[[305.0, 364.0], [546.0, 364.0], [546.0, 377.0], [305.0, 377.0]], ('i.e. Random Walk + Manual Anchor, achieves significantly', 0.9803673624992371)]\n",
      "[[[48.0, 381.0], [272.0, 381.0], [272.0, 394.0], [48.0, 394.0]], ('trained on synthetic and then finetuned on real datasets.', 0.9915968775749207)]\n",
      "[[[307.0, 379.0], [544.0, 379.0], [544.0, 389.0], [307.0, 389.0]], ('higher F1-scores consistently for different numbers of im-', 0.9863258600234985)]\n",
      "[[[308.0, 391.0], [546.0, 391.0], [546.0, 401.0], [308.0, 401.0]], ('ages. Especially, larger sizes of training sets result in greater', 0.9970780611038208)]\n",
      "[[[307.0, 402.0], [548.0, 402.0], [548.0, 415.0], [307.0, 415.0]], ('performance gaps. We also inspect the images generated', 0.9880344271659851)]\n",
      "[[[49.0, 416.0], [286.0, 416.0], [286.0, 426.0], [49.0, 426.0]], ('for 1x schedule long. All the hyperparameters are set to', 0.9824627637863159)]\n",
      "[[[307.0, 413.0], [548.0, 413.0], [548.0, 427.0], [307.0, 427.0]], ('with these methods respectively. When starting from the', 0.9959844350814819)]\n",
      "[[[49.0, 428.0], [287.0, 428.0], [287.0, 438.0], [49.0, 438.0]], ('default values. The results are summarized in Tab. 3. We', 0.9930031895637512)]\n",
      "[[[306.0, 426.0], [547.0, 426.0], [547.0, 439.0], [306.0, 439.0]], ('same anchor point, the proposed random walk can gener-', 0.9911022186279297)]\n",
      "[[[47.0, 438.0], [288.0, 439.0], [288.0, 451.0], [47.0, 450.0]], ('notice that the two synthetic datasets with natural images', 0.9913184642791748)]\n",
      "[[[307.0, 438.0], [547.0, 438.0], [547.0, 450.0], [307.0, 450.0]], ('ate more diverse viewpoints and can traverse much larger', 0.9978505373001099)]\n",
      "[[[48.0, 450.0], [288.0, 450.0], [288.0, 463.0], [48.0, 463.0]], ('as backgrounds, i.e. SynthText and VISD, result in similar', 0.9926098585128784)]\n",
      "[[[307.0, 450.0], [545.0, 450.0], [545.0, 460.0], [307.0, 460.0]], ('area.In contrast, the Random Viewpoint + Manual An-', 0.9856634736061096)]\n",
      "[[[49.0, 462.0], [287.0, 462.0], [287.0, 475.0], [49.0, 475.0]], ('performances. SynthText3D and our UnrealText are signif-', 0.9867120981216431)]\n",
      "[[[306.0, 461.0], [547.0, 461.0], [547.0, 474.0], [306.0, 474.0]], ('chor method degenerates either into random rotation only', 0.9917121529579163)]\n",
      "[[[48.0, 474.0], [288.0, 474.0], [288.0, 487.0], [48.0, 487.0]], ('icantly better than them. UnrealText is further a significant', 0.9901531338691711)]\n",
      "[[[306.0, 473.0], [547.0, 473.0], [547.0, 486.0], [306.0, 486.0]], ('when we set a small norm ball size for random location, or', 0.9919236898422241)]\n",
      "[[[48.0, 486.0], [286.0, 486.0], [286.0, 499.0], [48.0, 499.0]], ('improvement over SynthText3D. When we combine Unre-', 0.9990081787109375)]\n",
      "[[[307.0, 485.0], [547.0, 485.0], [547.0, 498.0], [307.0, 498.0]], ('into Random Viewpoint Only when we set a large norm ball', 0.9903342127799988)]\n",
      "[[[49.0, 500.0], [288.0, 500.0], [288.0, 510.0], [49.0, 510.0]], ('alText and SynthText, the two highly scalable engines, the', 0.9854971766471863)]\n",
      "[[[306.0, 497.0], [548.0, 497.0], [548.0, 510.0], [306.0, 510.0]], ('size. As a result, the Random Viewpoint + Manual Anchor', 0.9967228174209595)]\n",
      "[[[49.0, 511.0], [166.0, 511.0], [166.0, 521.0], [49.0, 521.0]], ('performances are even better.', 0.988007128238678)]\n",
      "[[[308.0, 511.0], [547.0, 511.0], [547.0, 521.0], [308.0, 521.0]], ('method requires careful manual selection of anchors, and', 0.987719714641571)]\n",
      "[[[308.0, 523.0], [546.0, 523.0], [546.0, 533.0], [308.0, 533.0]], ('we also need to manually tune the norm ball sizes for dif-', 0.981791079044342)]\n",
      "[[[86.0, 533.0], [132.0, 533.0], [132.0, 544.0], [86.0, 544.0]], ('Training Data', 0.9994964003562927)]\n",
      "[[[171.0, 534.0], [190.0, 534.0], [190.0, 542.0], [171.0, 542.0]], ('IC15', 0.9998816251754761)]\n",
      "[[[211.0, 534.0], [232.0, 534.0], [232.0, 542.0], [211.0, 542.0]], ('IC13', 0.8818467855453491)]\n",
      "[[[236.0, 533.0], [284.0, 533.0], [284.0, 543.0], [236.0, 543.0]], ('MLT 2017', 0.9436668157577515)]\n",
      "[[[83.0, 543.0], [133.0, 543.0], [133.0, 552.0], [83.0, 552.0]], ('SynthText I0K', 0.9492244124412537)]\n",
      "[[[163.0, 543.0], [202.0, 543.0], [202.0, 552.0], [163.0, 552.0]], ('13.6/13.4', 0.9919178485870361)]\n",
      "[[[243.0, 543.0], [280.0, 543.0], [280.0, 552.0], [243.0, 552.0]], ('S1/961', 0.7447709441184998)]\n",
      "[[[307.0, 534.0], [544.0, 534.0], [544.0, 544.0], [307.0, 544.0]], ('ferent scenes, which restricts the scalability of the synthe-', 0.9756301641464233)]\n",
      "[[[82.0, 551.0], [136.0, 551.0], [136.0, 562.0], [82.0, 562.0]], ('VISD 10K (full)', 0.9933633208274841)]\n",
      "[[[165.0, 551.0], [198.0, 551.0], [198.0, 562.0], [165.0, 562.0]], ('13.8/13.5', 0.9985249638557434)]\n",
      "[[[203.0, 551.0], [238.0, 551.0], [238.0, 562.0], [203.0, 562.0]], ('37.6/37.6', 0.9990627765655518)]\n",
      "[[[245.0, 551.0], [278.0, 551.0], [278.0, 562.0], [245.0, 562.0]], ('18.1/18.4', 0.9993861317634583)]\n",
      "[[[307.0, 545.0], [547.0, 545.0], [547.0, 558.0], [307.0, 558.0]], ('sis engine. Meanwhile, our proposed random walk based', 0.9860395193099976)]\n",
      "[[[68.0, 560.0], [148.0, 560.0], [148.0, 573.0], [68.0, 573.0]], ('SynthText3D 10K (full)', 0.9974914193153381)]\n",
      "[[[165.0, 561.0], [199.0, 561.0], [199.0, 572.0], [165.0, 572.0]], ('19.4/19.8', 0.9988003969192505)]\n",
      "[[[203.0, 561.0], [238.0, 561.0], [238.0, 572.0], [203.0, 572.0]], ('37.9/39.3', 0.9940401911735535)]\n",
      "[[[245.0, 561.0], [278.0, 561.0], [278.0, 572.0], [245.0, 572.0]], ('22.8/22.5', 0.9964065551757812)]\n",
      "[[[307.0, 557.0], [545.0, 557.0], [545.0, 570.0], [307.0, 570.0]], ('method is more flexible and robust to the selection of man-', 0.9805953502655029)]\n",
      "[[[81.0, 571.0], [134.0, 571.0], [134.0, 581.0], [81.0, 581.0]], ('UnrealText 10K', 0.9994044303894043)]\n",
      "[[[164.0, 570.0], [200.0, 570.0], [200.0, 580.0], [164.0, 580.0]], ('25.1/23.7', 0.9850291013717651)]\n",
      "[[[201.0, 569.0], [238.0, 569.0], [238.0, 580.0], [201.0, 580.0]], ('50.1/49.2', 0.9970404505729675)]\n",
      "[[[202.0, 578.0], [238.0, 580.0], [237.0, 591.0], [201.0, 589.0]], ('47.5/48.2', 0.9974191188812256)]\n",
      "[[[247.0, 570.0], [276.0, 570.0], [276.0, 580.0], [247.0, 580.0]], ('24/23.6', 0.9989557266235352)]\n",
      "[[[307.0, 569.0], [546.0, 569.0], [546.0, 582.0], [307.0, 582.0]], ('ual anchors. As for the Random Viewpoint Only method,', 0.9854164719581604)]\n",
      "[[[71.0, 581.0], [144.0, 581.0], [144.0, 591.0], [71.0, 591.0]], ('SynthText 800K (full)', 0.9752616882324219)]\n",
      "[[[165.0, 580.0], [199.0, 580.0], [199.0, 591.0], [165.0, 591.0]], ('19.6/20.3', 0.9985665082931519)]\n",
      "[[[244.0, 579.0], [280.0, 581.0], [279.0, 592.0], [243.0, 590.0]], ('24.2/24.8', 0.9850014448165894)]\n",
      "[[[70.0, 589.0], [146.0, 589.0], [146.0, 599.0], [70.0, 599.0]], ('UnrealText 600K (full)', 0.9570472836494446)]\n",
      "[[[166.0, 589.0], [198.0, 589.0], [198.0, 599.0], [166.0, 599.0]], ('26.2/25', 0.9823850989341736)]\n",
      "[[[202.0, 589.0], [238.0, 589.0], [238.0, 600.0], [202.0, 600.0]], ('51.5/52.2', 0.9989454746246338)]\n",
      "[[[244.0, 589.0], [279.0, 589.0], [279.0, 599.0], [244.0, 599.0]], ('27.8/27.3', 0.9977249503135681)]\n",
      "[[[306.0, 580.0], [545.0, 582.0], [544.0, 595.0], [306.0, 593.0]], ('a large proportion of generated viewpoints are invalid, e.g.', 0.9995520114898682)]\n",
      "[[[54.0, 598.0], [242.0, 597.0], [242.0, 610.0], [54.0, 611.0]], ('UnreolTe full + SymhTexr ful  27.727.5  62.4/63.7 ', 0.8642587065696716)]\n",
      "[[[241.0, 600.0], [281.0, 600.0], [281.0, 610.0], [241.0, 610.0]], ('32.4/32.1', 0.9985278248786926)]\n",
      "[[[307.0, 595.0], [546.0, 595.0], [546.0, 605.0], [307.0, 605.0]], ('inside other object meshes, which is out-of-distribution for', 0.9966639876365662)]\n",
      "[[[48.0, 615.0], [287.0, 615.0], [287.0, 628.0], [48.0, 628.0]], ('Table 3: Detection results (Box-AP/Mask-AP) of Mask-', 0.9917768836021423)]\n",
      "[[[306.0, 616.0], [354.0, 618.0], [353.0, 629.0], [306.0, 627.0]], ('formances.', 0.9991897344589233)]\n",
      "[[[308.0, 607.0], [545.0, 607.0], [545.0, 617.0], [308.0, 617.0]], ('real images. This explains why it results in the worst per-', 0.9936768412590027)]\n",
      "[[[47.0, 627.0], [249.0, 628.0], [249.0, 641.0], [47.0, 640.0]], ('RCNN models trained on different synthetic data.', 0.9694719314575195)]\n",
      "[[[319.0, 631.0], [546.0, 631.0], [546.0, 644.0], [319.0, 644.0]], ('From Fig. 5 (b), the major observation is that environ-', 0.9849430322647095)]\n",
      "[[[307.0, 643.0], [545.0, 643.0], [545.0, 653.0], [307.0, 653.0]], ('ment randomization module improves performances over', 0.9996844530105591)]\n",
      "[[[48.0, 658.0], [220.0, 659.0], [220.0, 673.0], [48.0, 672.0]], ('4.3. Module Level Ablation Analysis', 0.9763057231903076)]\n",
      "[[[307.0, 654.0], [546.0, 654.0], [546.0, 667.0], [307.0, 667.0]], ('different scene numbers consistently. Besides, the improve-', 0.9992588758468628)]\n",
      "[[[58.0, 677.0], [287.0, 678.0], [287.0, 691.0], [58.0, 690.0]], ('One reasonable concern about synthesizing from 3D vir-', 0.9874725341796875)]\n",
      "[[[308.0, 668.0], [545.0, 668.0], [545.0, 678.0], [308.0, 678.0]], ('ment is more significant as we use fewer scenes. Therefore,', 0.991470217704773)]\n",
      "[[[308.0, 680.0], [545.0, 680.0], [545.0, 690.0], [308.0, 690.0]], ('we can draw a conclusion that, the environment random-', 0.9986564517021179)]\n",
      "[[[49.0, 692.0], [288.0, 692.0], [288.0, 702.0], [49.0, 702.0]], ('tual scenes lies in the scene diversity. In this section, we', 0.9930869936943054)]\n",
      "[[[306.0, 691.0], [544.0, 691.0], [544.0, 701.0], [306.0, 701.0]], ('ization helps increase image diversity and at the same time.', 0.9917828440666199)]\n",
      "[[[49.0, 704.0], [287.0, 704.0], [287.0, 714.0], [49.0, 714.0]], ('address the importance of the proposed view finding mod-', 0.9952572584152222)]\n",
      "[[[306.0, 703.0], [546.0, 703.0], [546.0, 713.0], [306.0, 713.0]], ('can reduce the number of scenes needed. Furthermore, the', 0.993036150932312)]\n",
      "[2024/10/01 22:15:01] ppocr DEBUG: dt_boxes num : 127, elapsed : 0.27492499351501465\n",
      "[2024/10/01 22:15:02] ppocr DEBUG: cls num  : 127, elapsed : 0.777557373046875\n",
      "[2024/10/01 22:15:30] ppocr DEBUG: rec_res num  : 127, elapsed : 27.94307518005371\n",
      "[[[49.0, 74.0], [286.0, 74.0], [286.0, 84.0], [49.0, 84.0]], ('accurate. EAST also forms the basis of several widely rec-', 0.9862017035484314)]\n",
      "[[[343.0, 74.0], [400.0, 74.0], [400.0, 85.0], [343.0, 85.0]], ('Training Data', 0.9999640583992004)]\n",
      "[[[435.0, 72.0], [551.0, 72.0], [551.0, 85.0], [435.0, 85.0]], ('IC15IC13 MLT 2017', 0.9722213745117188)]\n",
      "[[[48.0, 85.0], [288.0, 85.0], [288.0, 98.0], [48.0, 98.0]], ('ognized end-to-end text spotting models [18, 7]. We adopt', 0.9882643222808838)]\n",
      "[[[339.0, 86.0], [402.0, 86.0], [402.0, 97.0], [339.0, 97.0]], ('SynthText 10K', 0.966608464717865)]\n",
      "[[[436.0, 86.0], [461.0, 86.0], [461.0, 97.0], [436.0, 97.0]], ('46.3', 0.9860502481460571)]\n",
      "[[[468.0, 86.0], [491.0, 86.0], [491.0, 97.0], [468.0, 97.0]], ('60.8', 0.9971343278884888)]\n",
      "[[[514.0, 85.0], [536.0, 85.0], [536.0, 97.0], [514.0, 97.0]], ('38.9', 0.9980322122573853)]\n",
      "[[[48.0, 97.0], [288.0, 97.0], [288.0, 110.0], [48.0, 110.0]], ('an opensource implementation. In all experiments, models', 0.9852954745292664)]\n",
      "[[[336.0, 98.0], [404.0, 98.0], [404.0, 109.0], [336.0, 109.0]], ('VISD 10K (full)', 0.9997186064720154)]\n",
      "[[[438.0, 97.0], [460.0, 97.0], [460.0, 109.0], [438.0, 109.0]], ('64.3', 0.9978721737861633)]\n",
      "[[[470.0, 97.0], [491.0, 97.0], [491.0, 109.0], [470.0, 109.0]], ('74.8', 0.9992504715919495)]\n",
      "[[[512.0, 96.0], [537.0, 96.0], [537.0, 111.0], [512.0, 111.0]], ('51.4', 0.9947364926338196)]\n",
      "[[[48.0, 109.0], [289.0, 109.0], [289.0, 122.0], [48.0, 122.0]], ('are trained on 4 GPU with a batch size of 56. During the', 0.9949330687522888)]\n",
      "[[[321.0, 109.0], [421.0, 109.0], [421.0, 122.0], [321.0, 122.0]], ('SynthText3D 10K (full)', 0.990073025226593)]\n",
      "[[[439.0, 110.0], [460.0, 110.0], [460.0, 121.0], [439.0, 121.0]], ('63.4', 0.9991986155509949)]\n",
      "[[[471.0, 110.0], [491.0, 110.0], [491.0, 121.0], [471.0, 121.0]], ('75.6', 0.9995988607406616)]\n",
      "[[[514.0, 110.0], [536.0, 110.0], [536.0, 122.0], [514.0, 122.0]], ('48.3', 0.9979832172393799)]\n",
      "[[[49.0, 123.0], [287.0, 123.0], [287.0, 133.0], [49.0, 133.0]], ('evaluation, the test images are resized to match a short side', 0.9912999272346497)]\n",
      "[[[338.0, 122.0], [404.0, 122.0], [404.0, 133.0], [338.0, 133.0]], ('UnrealText 10K', 0.9997324347496033)]\n",
      "[[[438.0, 122.0], [459.0, 122.0], [459.0, 133.0], [438.0, 133.0]], ('65.2', 0.9998747706413269)]\n",
      "[[[471.0, 122.0], [491.0, 122.0], [491.0, 133.0], [471.0, 133.0]], ('78.3', 0.9995772838592529)]\n",
      "[[[513.0, 120.0], [537.0, 120.0], [537.0, 135.0], [513.0, 135.0]], ('54.2', 0.9881390333175659)]\n",
      "[[[48.0, 133.0], [288.0, 133.0], [288.0, 146.0], [48.0, 146.0]], ('length of 800 pixels. For each experiment setting, we report', 0.9876168370246887)]\n",
      "[[[325.0, 132.0], [418.0, 132.0], [418.0, 145.0], [325.0, 145.0]], ('SynthText 800K (full)', 0.9993782043457031)]\n",
      "[[[439.0, 134.0], [460.0, 134.0], [460.0, 145.0], [439.0, 145.0]], ('58.0', 0.9991506934165955)]\n",
      "[[[514.0, 134.0], [536.0, 134.0], [536.0, 146.0], [514.0, 146.0]], ('44.8', 0.9933905005455017)]\n",
      "[[[48.0, 146.0], [234.0, 146.0], [234.0, 158.0], [48.0, 158.0]], ('the mean performance in 5 independent trials.', 0.9692329168319702)]\n",
      "[[[323.0, 145.0], [419.0, 145.0], [419.0, 157.0], [323.0, 157.0]], ('UnrealText 600K (full)', 0.9983937740325928)]\n",
      "[[[438.0, 146.0], [460.0, 146.0], [460.0, 156.0], [438.0, 156.0]], ('67.8', 0.9991415739059448)]\n",
      "[[[470.0, 146.0], [491.0, 146.0], [491.0, 156.0], [470.0, 156.0]], ('80.6', 0.9956314563751221)]\n",
      "[[[513.0, 144.0], [537.0, 144.0], [537.0, 158.0], [513.0, 158.0]], ('56.3', 0.9978878498077393)]\n",
      "[[[48.0, 157.0], [289.0, 157.0], [289.0, 170.0], [48.0, 170.0]], ('Benchmark Datasets We use the following scene text', 0.9914355278015137)]\n",
      "[[[310.0, 158.0], [429.0, 158.0], [429.0, 171.0], [310.0, 171.0]], ('SynthText3D 5K + VISD 5K', 0.989469051361084)]\n",
      "[[[438.0, 159.0], [460.0, 159.0], [460.0, 170.0], [438.0, 170.0]], ('65.4', 0.9989876747131348)]\n",
      "[[[472.0, 159.0], [491.0, 159.0], [491.0, 170.0], [472.0, 170.0]], ('78.6', 0.9936504364013672)]\n",
      "[[[513.0, 157.0], [537.0, 157.0], [537.0, 172.0], [513.0, 172.0]], ('52.2', 0.999056875705719)]\n",
      "[[[49.0, 170.0], [286.0, 170.0], [286.0, 180.0], [49.0, 180.0]], ('detection datasets for evaluation: (1) ICDAR 2013 Fo-', 0.9660741686820984)]\n",
      "[[[317.0, 171.0], [425.0, 171.0], [425.0, 181.0], [317.0, 181.0]], ('UnrealText 5K + VISD 5K', 0.9910503029823303)]\n",
      "[[[439.0, 172.0], [458.0, 172.0], [458.0, 180.0], [439.0, 180.0]], ('66.9', 0.995753288269043)]\n",
      "[[[470.0, 171.0], [490.0, 171.0], [490.0, 179.0], [470.0, 179.0]], ('80.4', 0.9861587285995483)]\n",
      "[[[514.0, 170.0], [534.0, 170.0], [534.0, 181.0], [514.0, 181.0]], ('55.7', 0.9991360306739807)]\n",
      "[[[48.0, 180.0], [287.0, 180.0], [287.0, 193.0], [48.0, 193.0]], ('cused Scene Texr (IC13) [14] containing horizontal text with', 0.9777847528457642)]\n",
      "[[[48.0, 193.0], [288.0, 193.0], [288.0, 206.0], [48.0, 206.0]], ('zoomed-in views. (2) ICDAR 2015 Incidental Scene Text', 0.9932851791381836)]\n",
      "[[[308.0, 189.0], [546.0, 189.0], [546.0, 199.0], [308.0, 199.0]], ('Table 1: Detection results (F1-scores) of EAST models', 0.9768677949905396)]\n",
      "[[[49.0, 206.0], [287.0, 206.0], [287.0, 216.0], [49.0, 216.0]], ('(IC15) [13] consisting of images taken without carefulness', 0.9958034157752991)]\n",
      "[[[308.0, 202.0], [445.0, 202.0], [445.0, 212.0], [308.0, 212.0]], ('trained on different synthetic data.', 0.9995276927947998)]\n",
      "[[[49.0, 218.0], [286.0, 218.0], [286.0, 228.0], [49.0, 228.0]], ('with Google Glass. Images are blurred and text are small.', 0.9951044321060181)]\n",
      "[[[47.0, 228.0], [288.0, 229.0], [288.0, 242.0], [47.0, 241.0]], ('(3) MLT 2017 [27] for multilingual scene text detection,', 0.9983965754508972)]\n",
      "[[[48.0, 241.0], [286.0, 241.0], [286.0, 253.0], [48.0, 253.0]], ('which is composed of scene text images of 9 languages.', 0.9857880473136902)]\n",
      "[[[308.0, 237.0], [545.0, 237.0], [545.0, 247.0], [308.0, 247.0]], ('of UnrealText and VISD is also superior to the combina-', 0.9911901950836182)]\n",
      "[[[48.0, 252.0], [288.0, 252.0], [288.0, 265.0], [48.0, 265.0]], ('Note that the images in IC13 and MLT17 have varying res-', 0.9991360902786255)]\n",
      "[[[306.0, 248.0], [547.0, 248.0], [547.0, 260.0], [306.0, 260.0]], ('tion of SynthText3D and VISD. This result demonstrates', 0.9932049512863159)]\n",
      "[[[48.0, 264.0], [289.0, 264.0], [289.0, 277.0], [48.0, 277.0]], ('olutions. Therefore, it is necessary to resize them to the', 0.9828288555145264)]\n",
      "[[[306.0, 259.0], [546.0, 259.0], [546.0, 272.0], [306.0, 272.0]], ('that, our UnrealText is complementary to existing syn-', 0.9984311461448669)]\n",
      "[[[49.0, 278.0], [223.0, 278.0], [223.0, 288.0], [49.0, 288.0]], ('same level of resolutions before evaluation.', 0.9889705777168274)]\n",
      "[[[307.0, 271.0], [547.0, 271.0], [547.0, 284.0], [307.0, 284.0]], ('thetic datasets that use real images as backgrounds. While', 0.9953278303146362)]\n",
      "[[[307.0, 283.0], [548.0, 283.0], [548.0, 296.0], [307.0, 296.0]], ('UnrealText simulates photo-realistic effects, synthetic data', 0.995202362537384)]\n",
      "[[[49.0, 296.0], [167.0, 296.0], [167.0, 309.0], [49.0, 309.0]], ('4.2. Experiments Results', 0.9793972969055176)]\n",
      "[[[307.0, 295.0], [548.0, 295.0], [548.0, 308.0], [307.0, 308.0]], ('with real background images can help adapt to real-world', 0.9990954995155334)]\n",
      "[[[48.0, 314.0], [289.0, 314.0], [289.0, 327.0], [48.0, 327.0]], ('Pure Synthetic Data We first train the EAST models on', 0.976358950138092)]\n",
      "[[[307.0, 309.0], [344.0, 309.0], [344.0, 319.0], [307.0, 319.0]], ('datasets.', 0.9999114274978638)]\n",
      "[[[48.0, 327.0], [289.0, 327.0], [289.0, 340.0], [48.0, 340.0]], ('different synthetic datasets alone, to compare our method', 0.9889649748802185)]\n",
      "[[[308.0, 322.0], [546.0, 322.0], [546.0, 332.0], [308.0, 332.0]], ('Combining Synthetic and Real Data One important role', 0.9987201690673828)]\n",
      "[[[48.0, 339.0], [289.0, 339.0], [289.0, 351.0], [48.0, 351.0]], ('with previous ones in a direct and quantitative way. Note', 0.9912868142127991)]\n",
      "[[[307.0, 334.0], [546.0, 334.0], [546.0, 344.0], [307.0, 344.0]], ('of synthetic data is to serve as data for pretraining, and to', 0.9914769530296326)]\n",
      "[[[47.0, 349.0], [289.0, 350.0], [289.0, 363.0], [47.0, 362.0]], ('that UnrealText, SynthText3D, SynthText, and VISD have', 0.999325156211853)]\n",
      "[[[306.0, 344.0], [546.0, 344.0], [546.0, 356.0], [306.0, 356.0]], ('further improve the performance on domain specific real', 0.9776491522789001)]\n",
      "[[[48.0, 362.0], [289.0, 362.0], [289.0, 375.0], [48.0, 375.0]], ('different numbers of images, so we also need to control the', 0.9994242787361145)]\n",
      "[[[306.0, 356.0], [547.0, 356.0], [547.0, 369.0], [306.0, 369.0]], ('datasets. We first pretrain the EAST models with differ-', 0.9894289374351501)]\n",
      "[[[49.0, 374.0], [286.0, 374.0], [286.0, 384.0], [49.0, 384.0]], ('number of images used in experiments. Results are summa-', 0.9941282272338867)]\n",
      "[[[306.0, 368.0], [547.0, 368.0], [547.0, 381.0], [306.0, 381.0]], ('ent synthetic data, and then use domain data to finetune the', 0.996023952960968)]\n",
      "[[[48.0, 386.0], [111.0, 386.0], [111.0, 397.0], [48.0, 397.0]], ('rized in Tab. 1.', 0.9751414060592651)]\n",
      "[[[307.0, 380.0], [547.0, 380.0], [547.0, 393.0], [307.0, 393.0]], ('models. The results are summarized in Tab. 2. On all', 0.9676940441131592)]\n",
      "[[[59.0, 397.0], [289.0, 397.0], [289.0, 410.0], [59.0, 410.0]], ('Firstly, we control the total number of images to', 0.9746085405349731)]\n",
      "[[[308.0, 394.0], [546.0, 394.0], [546.0, 404.0], [308.0, 404.0]], ('domain-specific datasets, models pretrained with our syn-', 0.9877126812934875)]\n",
      "[[[50.0, 411.0], [287.0, 411.0], [287.0, 421.0], [50.0, 421.0]], ('10K, which is also the full size of the smallest synthetic', 0.9888054132461548)]\n",
      "[[[307.0, 406.0], [545.0, 406.0], [545.0, 416.0], [307.0, 416.0]], ('thetic dataset surpasses others by considerable margins, ver-', 0.9987149238586426)]\n",
      "[[[49.0, 423.0], [286.0, 423.0], [286.0, 433.0], [49.0, 433.0]], ('datasets, VISD and SynthText3D. We observe a consider-', 0.9848107099533081)]\n",
      "[[[308.0, 418.0], [545.0, 418.0], [545.0, 428.0], [308.0, 428.0]], ('ifying the effectiveness of our synthesis method in the con-', 0.9955212473869324)]\n",
      "[[[49.0, 435.0], [287.0, 435.0], [287.0, 445.0], [49.0, 445.0]], ('able improvement on IC15 over previous state-of-the-art by', 0.99910968542099)]\n",
      "[[[306.0, 429.0], [541.0, 429.0], [541.0, 442.0], [306.0, 442.0]], ('text of boosting performance on domain specific datasets.', 0.9882780909538269)]\n",
      "[[[48.0, 446.0], [288.0, 446.0], [288.0, 458.0], [48.0, 458.0]], ('+0.9% in F1-score, and significant improvements on IC13', 0.9987494945526123)]\n",
      "[[[307.0, 442.0], [548.0, 442.0], [548.0, 454.0], [307.0, 454.0]], ('Pretraining on Full Dataset As shown in the last rows', 0.9996553063392639)]\n",
      "[[[48.0, 457.0], [289.0, 457.0], [289.0, 470.0], [48.0, 470.0]], ('(+2.7%) and MLT 2017 (+2.8%). Secondly, we also train', 0.9886283874511719)]\n",
      "[[[306.0, 453.0], [547.0, 453.0], [547.0, 466.0], [306.0, 466.0]], ('of Tab. 2, when we pretrain the detector models with our', 0.9978903532028198)]\n",
      "[[[48.0, 469.0], [287.0, 469.0], [287.0, 482.0], [48.0, 482.0]], ('models on the full set of SynthText and ours, since scalabil-', 0.9868883490562439)]\n",
      "[[[306.0, 465.0], [546.0, 465.0], [546.0, 478.0], [306.0, 478.0]], ('full dataset, the performances are improved significantly,', 0.9867590069770813)]\n",
      "[[[47.0, 481.0], [287.0, 481.0], [287.0, 494.0], [47.0, 494.0]], ('ity is also an important factor for synthetic scene text im-', 0.9910121560096741)]\n",
      "[[[305.0, 477.0], [545.0, 477.0], [545.0, 490.0], [305.0, 490.0]], ('demonstrating the advantage of the scalability of our en-', 0.9927205443382263)]\n",
      "[[[49.0, 495.0], [286.0, 495.0], [286.0, 505.0], [49.0, 505.0]], ('ages, especially when considering the demand to train rec-', 0.9919713735580444)]\n",
      "[[[307.0, 489.0], [547.0, 489.0], [547.0, 502.0], [307.0, 502.0]], ('gine. Especially, The EAST model achieves an F1 score', 0.995508074760437)]\n",
      "[[[48.0, 505.0], [288.0, 505.0], [288.0, 518.0], [48.0, 518.0]], ('ognizers. Extra training images further improve F1 scores', 0.9955959916114807)]\n",
      "[[[307.0, 503.0], [545.0, 503.0], [545.0, 513.0], [307.0, 513.0]], ('of 74.1 on MLT17, which is even better than recent state-', 0.987564206123352)]\n",
      "[[[49.0, 518.0], [285.0, 518.0], [285.0, 528.0], [49.0, 528.0]], ('on IC15, IC13, and MLT by +2.6%, +2.3%, and +2.1%.', 0.9826793074607849)]\n",
      "[[[307.0, 515.0], [545.0, 515.0], [545.0, 525.0], [307.0, 525.0]], ('of-the-art results, including 73.9 by CRAFT[2] and 73.1 by', 0.9992483854293823)]\n",
      "[[[49.0, 530.0], [287.0, 530.0], [287.0, 540.0], [49.0, 540.0]], ('Models trained with our UnrealText data outperform all', 0.9953193068504333)]\n",
      "[[[307.0, 527.0], [546.0, 527.0], [546.0, 537.0], [307.0, 537.0]], ('LOMO [52]. Although the margin is not great, it suffices', 0.9768355488777161)]\n",
      "[[[47.0, 541.0], [288.0, 542.0], [288.0, 554.0], [47.0, 553.0]], ('other synthetic datasets. Besides, the subset of 10K images', 0.9948476552963257)]\n",
      "[[[307.0, 538.0], [544.0, 538.0], [544.0, 547.0], [307.0, 547.0]], ('to claim that the EAST model revives and reclaims state-of', 0.9870743155479431)]\n",
      "[[[48.0, 553.0], [289.0, 553.0], [289.0, 566.0], [48.0, 566.0]], ('with our method even surpasses 800K SynthText images', 0.9839595556259155)]\n",
      "[[[307.0, 549.0], [541.0, 549.0], [541.0, 562.0], [307.0, 562.0]], ('the-art performance with the help of our synthetic dataset.', 0.9916670918464661)]\n",
      "[[[49.0, 567.0], [286.0, 567.0], [286.0, 577.0], [49.0, 577.0]], ('significantly on all datasets. The experiment results demon-', 0.998586893081665)]\n",
      "[[[307.0, 562.0], [547.0, 562.0], [547.0, 575.0], [307.0, 575.0]], ('Results with Mask-RCNN As the EAST algorithm we use', 0.9910691976547241)]\n",
      "[[[49.0, 579.0], [288.0, 579.0], [288.0, 589.0], [49.0, 589.0]], ('strate the effectiveness of our proposed synthetic engine and', 0.9958158135414124)]\n",
      "[[[307.0, 574.0], [547.0, 574.0], [547.0, 587.0], [307.0, 587.0]], ('above is specifically designed for scene text and that the', 0.987468957901001)]\n",
      "[[[49.0, 591.0], [85.0, 591.0], [85.0, 601.0], [49.0, 601.0]], ('datasets.', 0.9998570680618286)]\n",
      "[[[306.0, 585.0], [546.0, 585.0], [546.0, 598.0], [306.0, 598.0]], ('evaluation with F1 scores may not be comprehensive, we', 0.9864100217819214)]\n",
      "[[[49.0, 602.0], [286.0, 602.0], [286.0, 612.0], [49.0, 612.0]], ('Complementary Synthetic Data One unique characteristic', 0.9988502264022827)]\n",
      "[[[308.0, 600.0], [546.0, 600.0], [546.0, 610.0], [308.0, 610.0]], ('provide results with Mask-RCNN [?] which is a general', 0.9972668290138245)]\n",
      "[[[48.0, 614.0], [289.0, 614.0], [289.0, 627.0], [48.0, 627.0]], ('of the proposed UnrealText is that, the images are generated', 0.9924915432929993)]\n",
      "[[[305.0, 610.0], [547.0, 611.0], [547.0, 624.0], [305.0, 623.0]], ('object detector. We evaluate the models using the Average', 0.9841017723083496)]\n",
      "[[[48.0, 626.0], [288.0, 626.0], [288.0, 639.0], [48.0, 639.0]], ('from 3D scene models, instead of real background images,', 0.9944514632225037)]\n",
      "[[[306.0, 623.0], [548.0, 623.0], [548.0, 636.0], [306.0, 636.0]], ('Precision (AP) metrics which are more comprehensive and', 0.9769658446311951)]\n",
      "[[[48.0, 638.0], [288.0, 638.0], [288.0, 650.0], [48.0, 650.0]], ('resulting in potential domain gap due to different artistic', 0.9977118372917175)]\n",
      "[[[306.0, 635.0], [548.0, 635.0], [548.0, 647.0], [306.0, 647.0]], ('less affected by the tricky choice of threshold values. We', 0.9917057752609253)]\n",
      "[[[48.0, 648.0], [287.0, 648.0], [287.0, 661.0], [48.0, 661.0]], ('styles. We conduct experiments by training on both Unre-', 0.9958581924438477)]\n",
      "[[[307.0, 645.0], [547.0, 645.0], [547.0, 658.0], [307.0, 658.0]], ('use the opensource implementation Detectron2 4. The ro-', 0.9875916242599487)]\n",
      "[[[49.0, 662.0], [278.0, 662.0], [278.0, 672.0], [49.0, 672.0]], ('alText data (5K) and VISD (5K), as also shown in Tab.', 0.9954068064689636)]\n",
      "[[[305.0, 657.0], [547.0, 656.0], [547.0, 669.0], [305.0, 670.0]], ('tated bounding boxes of text instances are used as the mask', 0.9998595714569092)]\n",
      "[[[48.0, 672.0], [288.0, 672.0], [288.0, 685.0], [48.0, 685.0]], ('(last row, marked with italics), which achieves better perfor-', 0.9848881363868713)]\n",
      "[[[307.0, 669.0], [547.0, 669.0], [547.0, 682.0], [307.0, 682.0]], ('annotations. We select a default Mask-RCNN configuration', 0.9899330139160156)]\n",
      "[[[49.0, 686.0], [288.0, 686.0], [288.0, 696.0], [49.0, 696.0]], ('mance than other 10K synthetic datasets. The combination', 0.9888691902160645)]\n",
      "[[[308.0, 682.0], [546.0, 682.0], [546.0, 692.0], [308.0, 692.0]], ('with ResNet-50+FPN as the backbone and train the model', 0.9984063506126404)]\n",
      "[[[60.0, 704.0], [208.0, 704.0], [208.0, 714.0], [60.0, 714.0]], ('https://github.con/argnan/EAsT', 0.9521198272705078)]\n",
      "[[[319.0, 704.0], [544.0, 704.0], [544.0, 714.0], [319.0, 714.0]], ('*https://github.con/facebookresearch/detectron2', 0.9753909111022949)]\n",
      "[2024/10/01 22:15:31] ppocr DEBUG: dt_boxes num : 115, elapsed : 0.26538681983947754\n",
      "[2024/10/01 22:15:32] ppocr DEBUG: cls num  : 115, elapsed : 0.6240189075469971\n",
      "[2024/10/01 22:15:57] ppocr DEBUG: rec_res num  : 115, elapsed : 25.33113193511963\n",
      "[[[52.0, 73.0], [289.0, 73.0], [289.0, 86.0], [52.0, 86.0]], ('[9] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term', 0.9986599683761597)]\n",
      "[[[307.0, 73.0], [546.0, 73.0], [546.0, 86.0], [307.0, 86.0]], ('[23] Pengyuan Lyu, Zhicheng Yang, Xinhang Leng, Xiaojun Wu,', 0.9841651320457458)]\n",
      "[[[68.0, 84.0], [264.0, 83.0], [264.0, 96.0], [68.0, 97.0]], ('memory. Nearal computation, 9(8):17351780, 1997.', 0.9973848462104797)]\n",
      "[[[326.0, 84.0], [548.0, 84.0], [548.0, 97.0], [326.0, 97.0]], ('Ruiyu Li, and Xiaoyong Shen. 2d attentional irregular scene', 0.9965390563011169)]\n",
      "[[[48.0, 98.0], [287.0, 98.0], [287.0, 108.0], [48.0, 108.0]], ('[10] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and An-', 0.9910761117935181)]\n",
      "[[[326.0, 96.0], [529.0, 95.0], [529.0, 106.0], [326.0, 107.0]], ('text recognizer. arXiv preprint arXiv: 1906.05708, 2019', 0.9808489680290222)]\n",
      "[[[69.0, 109.0], [287.0, 109.0], [287.0, 119.0], [69.0, 119.0]], ('drew Zisserman. Synthetic data and artificial neural net-', 0.9740583896636963)]\n",
      "[[[307.0, 108.0], [548.0, 108.0], [548.0, 121.0], [307.0, 121.0]], ('[24] John McCormac, Ankur Handa, Stefan Leutenegger, and', 0.9924095869064331)]\n",
      "[[[69.0, 120.0], [232.0, 120.0], [232.0, 130.0], [69.0, 130.0]], ('works for natural scene text recognition.', 0.9559382796287537)]\n",
      "[[[227.0, 120.0], [288.0, 120.0], [288.0, 130.0], [227.0, 130.0]], ('arXiv preprint', 0.9800766706466675)]\n",
      "[[[329.0, 120.0], [546.0, 120.0], [546.0, 130.0], [329.0, 130.0]], ('Andrew J. Davison.Scenenet RGB-D: 5m photorealistic', 0.9875079393386841)]\n",
      "[[[68.0, 131.0], [155.0, 131.0], [155.0, 141.0], [68.0, 141.0]], ('arXiv: 1406.2227, 2014', 0.9836540222167969)]\n",
      "[[[327.0, 130.0], [547.0, 130.0], [547.0, 143.0], [327.0, 143.0]], ('images of synthetic indoor trajectories with ground truth.', 0.9918633699417114)]\n",
      "[[[68.0, 151.0], [288.0, 152.0], [288.0, 165.0], [68.0, 164.0]], ('Wei Li, Hua Wang, Pei Fu, and Zhenbo Luo. R2cnn: rota-', 0.9849165678024292)]\n",
      "[[[328.0, 142.0], [437.0, 142.0], [437.0, 151.0], [328.0, 151.0]], ('CoRR, abs/1612.05079, 2016.', 0.9852273464202881)]\n",
      "[[[308.0, 153.0], [546.0, 153.0], [546.0, 163.0], [308.0, 163.0]], ('[25]Anand Mishra, Karteek Alahari, and CV Jawahar. Scene text', 0.9947567582130432)]\n",
      "[[[69.0, 165.0], [287.0, 165.0], [287.0, 175.0], [69.0, 175.0]], ('tional region cnn for orientation robust scene text detection.', 0.9917739033699036)]\n",
      "[[[327.0, 163.0], [546.0, 162.0], [546.0, 175.0], [327.0, 176.0]], ('recognition using higher order language priors. In BMVC.', 0.9696617722511292)]\n",
      "[[[68.0, 175.0], [215.0, 174.0], [215.0, 185.0], [68.0, 186.0]], ('arXiv preprint asXiv:1706.09579, 2017.', 0.983363687992096)]\n",
      "[[[327.0, 174.0], [510.0, 174.0], [510.0, 187.0], [327.0, 187.0]], ('British Machine Vision Conference. BMVA, 2012.', 0.9977937936782837)]\n",
      "[[[49.0, 188.0], [287.0, 188.0], [287.0, 198.0], [49.0, 198.0]], ('[12] Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci,', 0.9826347827911377)]\n",
      "[[[307.0, 187.0], [545.0, 186.0], [545.0, 197.0], [307.0, 198.0]], ('[26] Nibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Chowd-', 0.9921523928642273)]\n",
      "[[[68.0, 199.0], [286.0, 199.0], [286.0, 209.0], [68.0, 209.0]], ('Justin Yuan, Matt Rusiniak, David Acuna, Antonio Torralba,', 0.9882864952087402)]\n",
      "[[[327.0, 208.0], [546.0, 208.0], [546.0, 221.0], [327.0, 221.0]], ('pada Pal, Jean-Christophe Burie, Cheng-lin Liu, et al. Ic-', 0.9814026951789856)]\n",
      "[[[328.0, 199.0], [545.0, 199.0], [545.0, 209.0], [328.0, 209.0]], ('hury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Uma-', 0.9933608770370483)]\n",
      "[[[69.0, 210.0], [287.0, 210.0], [287.0, 220.0], [69.0, 220.0]], ('and Sanja Fidler. Meta-sim: Learning to generate synthetic', 0.9986488223075867)]\n",
      "[[[67.0, 220.0], [249.0, 220.0], [249.0, 233.0], [67.0, 233.0]], ('datasets. arXiv preprint arXiv: 1904.11621, 2019.', 0.9755178689956665)]\n",
      "[[[326.0, 220.0], [548.0, 220.0], [548.0, 233.0], [326.0, 233.0]], ('dar2019 robust reading challenge on multi-lingual scene', 0.9921070337295532)]\n",
      "[[[48.0, 232.0], [288.0, 232.0], [288.0, 245.0], [48.0, 245.0]], ('[13] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos', 0.9855210185050964)]\n",
      "[[[326.0, 231.0], [547.0, 231.0], [547.0, 244.0], [326.0, 244.0]], ('text detection and recognitionrrc-mlt-2019. arXiv preprint', 0.9788838624954224)]\n",
      "[[[69.0, 244.0], [286.0, 244.0], [286.0, 253.0], [69.0, 253.0]], ('Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwa-', 0.9960580468177795)]\n",
      "[[[327.0, 243.0], [418.0, 243.0], [418.0, 252.0], [327.0, 252.0]], ('arXiv:1907.00945, 2019', 0.9975168108940125)]\n",
      "[[[69.0, 255.0], [287.0, 255.0], [287.0, 265.0], [69.0, 265.0]], ('mura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chan-', 0.9947971105575562)]\n",
      "[[[307.0, 253.0], [547.0, 253.0], [547.0, 266.0], [307.0, 266.0]], ('[27] Nibal Nayef, Fei Yin, Imen Bizid, Hyunsoo Choi, Yuan', 0.9902507662773132)]\n",
      "[[[67.0, 263.0], [287.0, 263.0], [287.0, 276.0], [67.0, 276.0]], ('drasekhar, Shijian Lu, et al. Icdar 2015 competition on robust', 0.9846012592315674)]\n",
      "[[[327.0, 264.0], [547.0, 264.0], [547.0, 277.0], [327.0, 277.0]], ('Feng, Dimosthenis Karatzas, Zhenbo Luo, Umapada Pal.', 0.9827226996421814)]\n",
      "[[[67.0, 275.0], [287.0, 275.0], [287.0, 288.0], [67.0, 288.0]], ('reading. In 2015 13th International Conference on Docu-', 0.9809139370918274)]\n",
      "[[[326.0, 275.0], [547.0, 275.0], [547.0, 288.0], [326.0, 288.0]], ('Christophe Rigaud, Joseph Chazalon, et al. Icdar2017 ro-', 0.9960324168205261)]\n",
      "[[[67.0, 286.0], [287.0, 285.0], [287.0, 298.0], [67.0, 299.0]], ('ment Analysis and Recognition (ICDAR), pages 11561160.', 0.9917010068893433)]\n",
      "[[[327.0, 288.0], [547.0, 288.0], [547.0, 298.0], [327.0, 298.0]], ('bust reading challenge on multi-lingual scene text detection', 0.9904671907424927)]\n",
      "[[[69.0, 298.0], [114.0, 298.0], [114.0, 308.0], [69.0, 308.0]], ('IEEE, 2015.', 0.9996162056922913)]\n",
      "[[[328.0, 299.0], [546.0, 299.0], [546.0, 309.0], [328.0, 309.0]], ('and script identification-trc-mlt. In Proc. ICDAR, volume 1,', 0.9851788282394409)]\n",
      "[[[49.0, 311.0], [286.0, 311.0], [286.0, 321.0], [49.0, 321.0]], ('[14] DimosthenisKaratzas,Faisal Shafait, SeiichiUchida,', 0.9772179126739502)]\n",
      "[[[327.0, 309.0], [441.0, 307.0], [441.0, 318.0], [327.0, 320.0]], ('pages 14541459. IEEE, 2017.', 0.9950038194656372)]\n",
      "[[[68.0, 322.0], [288.0, 322.0], [288.0, 332.0], [68.0, 332.0]], ('Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles', 0.9958125352859497)]\n",
      "[[[306.0, 320.0], [547.0, 321.0], [547.0, 334.0], [306.0, 333.0]], ('[28] Jeremie Papon and Markus Schoeler. Semantic pose using', 0.9765431880950928)]\n",
      "[[[68.0, 333.0], [287.0, 333.0], [287.0, 343.0], [68.0, 343.0]], ('Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Al-', 0.999513566493988)]\n",
      "[[[327.0, 331.0], [546.0, 331.0], [546.0, 344.0], [327.0, 344.0]], ('deep networks trained on synthetic rgb-d. In Proc. ICCV,', 0.9794647097587585)]\n",
      "[[[69.0, 344.0], [287.0, 344.0], [287.0, 353.0], [69.0, 353.0]], ('mazan, and Lluis Pere de las Heras. Icdar 2013 robust read-', 0.9966536164283752)]\n",
      "[[[326.0, 343.0], [408.0, 341.0], [408.0, 352.0], [326.0, 353.0]], ('pages 774782, 2015.', 0.9981792569160461)]\n",
      "[[[67.0, 353.0], [289.0, 353.0], [289.0, 366.0], [67.0, 366.0]], ('ing competition. In 2013 12th International Conference on', 0.9849244356155396)]\n",
      "[[[67.0, 363.0], [287.0, 363.0], [287.0, 376.0], [67.0, 376.0]], ('Document Analysis and Recognition (ICDAR), pages 1484', 0.98995441198349)]\n",
      "[[[307.0, 354.0], [546.0, 354.0], [546.0, 367.0], [307.0, 367.0]], ('[29] Xingchao Peng, Baochen Sun, Karim Ali, and Kate Saenko.', 0.9931734204292297)]\n",
      "[[[327.0, 365.0], [545.0, 364.0], [545.0, 375.0], [327.0, 376.0]], ('Learning deep object detectors from 3d models. In Proc.', 0.9885181784629822)]\n",
      "[[[69.0, 376.0], [136.0, 376.0], [136.0, 386.0], [69.0, 386.0]], ('1493. IEEE, 2013.', 0.9781253933906555)]\n",
      "[[[328.0, 376.0], [441.0, 375.0], [441.0, 386.0], [328.0, 387.0]], ('ICCV, pages 12781286, 2015.', 0.992172360420227)]\n",
      "[[[48.0, 386.0], [287.0, 387.0], [287.0, 400.0], [48.0, 399.0]], ('[15] Hui Li, Peng Wang, Chunhua Shen, and Guyu Zhang. Show,', 0.9946562051773071)]\n",
      "[[[305.0, 386.0], [547.0, 387.0], [547.0, 400.0], [305.0, 399.0]], ('[30] Siyang Qin, Alessandro Bissacco, Michalis Raptis, Yasuhisa', 0.9863076210021973)]\n",
      "[[[70.0, 400.0], [287.0, 400.0], [287.0, 410.0], [70.0, 410.0]], ('attend and read: A simple and strong baseline for irregular', 0.9813033938407898)]\n",
      "[[[328.0, 400.0], [546.0, 400.0], [546.0, 410.0], [328.0, 410.0]], ('Fujii, and Ying Xiao. Towards unconstrained end-to-end text', 0.9994205832481384)]\n",
      "[[[69.0, 411.0], [176.0, 411.0], [176.0, 421.0], [69.0, 421.0]], ('text recognition. AAAI, 2019.', 0.9957613348960876)]\n",
      "[[[326.0, 410.0], [546.0, 409.0], [546.0, 422.0], [326.0, 423.0]], ('spotting. In Proceedings of the IEEE International Confer-', 0.9866629838943481)]\n",
      "[[[47.0, 422.0], [288.0, 422.0], [288.0, 435.0], [47.0, 435.0]], ('[16] Minghui Liao, Boyu Song, Shangbang Long, Minghang He,', 0.9751248955726624)]\n",
      "[[[327.0, 421.0], [512.0, 421.0], [512.0, 434.0], [327.0, 434.0]], ('ence on Computer Vision, pages 47044714, 2019.', 0.9718613028526306)]\n",
      "[[[67.0, 433.0], [288.0, 433.0], [288.0, 446.0], [67.0, 446.0]], ('Cong Yao, and Xiang Bai. Synthtext3d: synthesizing scene', 0.9886069893836975)]\n",
      "[[[306.0, 432.0], [546.0, 433.0], [546.0, 446.0], [306.0, 444.0]], ('[31] Weichao Qiu and Alan Yuille. Unrealcv: Connecting com-', 0.988833487033844)]\n",
      "[[[66.0, 444.0], [287.0, 443.0], [287.0, 455.0], [66.0, 456.0]], ('text images from 3d virtual worlds. Science China Irforma-', 0.9791797399520874)]\n",
      "[[[327.0, 444.0], [546.0, 444.0], [546.0, 456.0], [327.0, 456.0]], ('puter vision to unreal engine. In Proc. ECCV, pages 909', 0.9899461269378662)]\n",
      "[[[68.0, 455.0], [195.0, 454.0], [195.0, 465.0], [69.0, 466.0]], ('tiox Sciences, 63(2):120105, 2020.', 0.9523069858551025)]\n",
      "[[[328.0, 455.0], [367.0, 455.0], [367.0, 465.0], [328.0, 465.0]], ('916, 2016.', 0.9991159439086914)]\n",
      "[[[48.0, 466.0], [288.0, 466.0], [288.0, 479.0], [48.0, 479.0]], ('[17] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman,', 0.9912095069885254)]\n",
      "[[[307.0, 466.0], [547.0, 466.0], [547.0, 479.0], [307.0, 479.0]], ('[32] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan', 0.9860413670539856)]\n",
      "[[[68.0, 477.0], [288.0, 477.0], [288.0, 490.0], [68.0, 490.0]], ('and Simon Lucey. St-gan: Spatial transformer generative', 0.9838529229164124)]\n",
      "[[[327.0, 477.0], [547.0, 477.0], [547.0, 490.0], [327.0, 490.0]], ('Tian, and Chew Lim Tan. Recognizing text with perspective', 0.9920544624328613)]\n",
      "[[[69.0, 490.0], [287.0, 490.0], [287.0, 500.0], [69.0, 500.0]], ('adversarial networks for image compositing. In Proceed-', 0.9822676181793213)]\n",
      "[[[327.0, 498.0], [353.0, 501.0], [351.0, 512.0], [326.0, 508.0]], ('2013.', 0.9958878755569458)]\n",
      "[[[328.0, 490.0], [546.0, 490.0], [546.0, 500.0], [328.0, 500.0]], ('distortion in natural scenes. In Proc. ICCV, pages 569576,', 0.9989631175994873)]\n",
      "[[[68.0, 501.0], [287.0, 501.0], [287.0, 511.0], [68.0, 511.0]], ('ings of the IEEE Conference on Computer Vision and Pattern', 0.9934650659561157)]\n",
      "[[[68.0, 511.0], [206.0, 511.0], [206.0, 524.0], [68.0, 524.0]], ('Recognition, pages 94559464, 2018.', 0.9781203269958496)]\n",
      "[[[308.0, 513.0], [546.0, 513.0], [546.0, 523.0], [308.0, 523.0]], ('[33] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen', 0.9945249557495117)]\n",
      "[[[49.0, 524.0], [288.0, 524.0], [288.0, 534.0], [49.0, 534.0]], ('[18] Xuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and', 0.9918243885040283)]\n",
      "[[[327.0, 523.0], [547.0, 523.0], [547.0, 536.0], [327.0, 536.0]], ('Koltun.  Playing for data: Ground truth from computer', 0.9597072005271912)]\n",
      "[[[69.0, 535.0], [288.0, 535.0], [288.0, 544.0], [69.0, 544.0]], ('Junjie Yan. Fots: Fast oriented text spotting with a unified', 0.9746332764625549)]\n",
      "[[[327.0, 534.0], [547.0, 534.0], [547.0, 546.0], [327.0, 546.0]], ('games. In European conference on computer vision, pages', 0.9876905679702759)]\n",
      "[[[67.0, 544.0], [173.0, 544.0], [173.0, 554.0], [67.0, 554.0]], ('network. Proc. CVPR, 2018.', 0.9576246738433838)]\n",
      "[[[327.0, 544.0], [422.0, 544.0], [422.0, 557.0], [327.0, 557.0]], ('102118. Springer, 2016.', 0.99756920337677)]\n",
      "[[[48.0, 556.0], [286.0, 557.0], [286.0, 568.0], [48.0, 567.0]], ('[19] Yuliang Liu and Lianwen Jin. Deep matching prior network:', 0.9886859059333801)]\n",
      "[[[307.0, 556.0], [547.0, 556.0], [547.0, 569.0], [307.0, 569.0]], ('[34] Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng', 0.993896484375)]\n",
      "[[[68.0, 567.0], [287.0, 567.0], [287.0, 580.0], [68.0, 580.0]], ('Toward tighter multi-oriented text detection. In Proc. CVPR.', 0.980908989906311)]\n",
      "[[[325.0, 566.0], [548.0, 567.0], [548.0, 580.0], [325.0, 579.0]], ('Chan, and Chew Lim Tan. A robust arbitrary text detection', 0.9883137345314026)]\n",
      "[[[68.0, 579.0], [92.0, 579.0], [92.0, 590.0], [68.0, 590.0]], ('2017.', 0.9340945482254028)]\n",
      "[[[328.0, 580.0], [545.0, 580.0], [545.0, 590.0], [328.0, 590.0]], ('system for natural scene images. Expert Systems with Appfi-', 0.9507513046264648)]\n",
      "[[[49.0, 592.0], [287.0, 592.0], [287.0, 602.0], [49.0, 602.0]], ('[20] Shangbang Long, Yushuo Guan, Bingxuan Wang, Kaigui', 0.9966017007827759)]\n",
      "[[[328.0, 591.0], [450.0, 591.0], [450.0, 601.0], [328.0, 601.0]], ('cations, 41(18):80278048, 2014.', 0.9914265275001526)]\n",
      "[[[69.0, 603.0], [287.0, 603.0], [287.0, 613.0], [69.0, 613.0]], ('Bian, and Cong Yao. Alchemy: Techniques for rectifica-', 0.9845991730690002)]\n",
      "[[[308.0, 603.0], [547.0, 603.0], [547.0, 613.0], [308.0, 613.0]], ('[35] German Ros, Laura Sellart, Joanna Materzynska, David', 0.9959208965301514)]\n",
      "[[[69.0, 614.0], [288.0, 614.0], [288.0, 624.0], [69.0, 624.0]], ('tion based irregular scene text recognition. arXiv preprint', 0.9818100333213806)]\n",
      "[[[327.0, 612.0], [547.0, 613.0], [547.0, 626.0], [327.0, 625.0]], ('Vazquez, and Antonio M Lopez. The synthia dataset: A large', 0.9885063171386719)]\n",
      "[[[68.0, 625.0], [161.0, 625.0], [161.0, 635.0], [68.0, 635.0]], ('arXiv: 1908.11834, 2019.', 0.9916741847991943)]\n",
      "[[[326.0, 623.0], [547.0, 624.0], [547.0, 637.0], [326.0, 636.0]], ('collection of synthetic images for semantic segmentation of', 0.9866896867752075)]\n",
      "[[[48.0, 636.0], [287.0, 636.0], [287.0, 648.0], [48.0, 648.0]], ('[21] Shangbang Long, Xin He, and Cong Yao. Scene text detec-', 0.9970420598983765)]\n",
      "[[[326.0, 635.0], [526.0, 635.0], [526.0, 647.0], [326.0, 647.0]], ('urban scenes. In Proc. CVPR, pages 32343243, 2016.', 0.9803003668785095)]\n",
      "[[[68.0, 646.0], [289.0, 646.0], [289.0, 659.0], [68.0, 659.0]], ('tion and recognition: The deep learning era. arXiv preprint', 0.982840895652771)]\n",
      "[[[307.0, 645.0], [546.0, 646.0], [546.0, 659.0], [307.0, 658.0]], ('[36] Scott D Roth. Ray casting for modeling solids. Compater', 0.9692736864089966)]\n",
      "[[[69.0, 658.0], [161.0, 658.0], [161.0, 668.0], [69.0, 668.0]], ('arXiv: 1811.04256, 2018.', 0.9749743938446045)]\n",
      "[[[326.0, 657.0], [520.0, 656.0], [520.0, 669.0], [326.0, 670.0]], ('Graphics & Image Processing, 18(2):109144, 1982.', 0.9940404891967773)]\n",
      "[[[49.0, 671.0], [287.0, 671.0], [287.0, 681.0], [49.0, 681.0]], ('[22] Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He,', 0.9890971779823303)]\n",
      "[[[307.0, 669.0], [546.0, 669.0], [546.0, 682.0], [307.0, 682.0]], ('[37] Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian,', 0.9917542934417725)]\n",
      "[[[68.0, 679.0], [286.0, 680.0], [286.0, 693.0], [68.0, 692.0]], ('Wenhao Wu, and Cong Yao. Textsnake: A flexible represen', 0.9990274310112)]\n",
      "[[[328.0, 682.0], [545.0, 682.0], [545.0, 692.0], [328.0, 692.0]], ('Mathieu Salzmann, Lars Petersson, and Jose M Alvarez. Ef-', 0.9939658045768738)]\n",
      "[[[68.0, 701.0], [91.0, 703.0], [90.0, 714.0], [67.0, 711.0]], ('2018.', 0.9175688028335571)]\n",
      "[[[69.0, 693.0], [288.0, 693.0], [288.0, 703.0], [69.0, 703.0]], ('tation for detecting text of arbitrary shapes. In Proc. ECCV,', 0.9950589537620544)]\n",
      "[[[328.0, 693.0], [545.0, 693.0], [545.0, 703.0], [328.0, 703.0]], ('fective use of synthetic data for urban scene semantic seg-', 0.9919906258583069)]\n",
      "[[[327.0, 704.0], [503.0, 704.0], [503.0, 714.0], [327.0, 714.0]], ('mentation. In Proc. ECCV, pages 86103, 2018.', 0.9846940636634827)]\n",
      "[2024/10/01 22:15:58] ppocr DEBUG: dt_boxes num : 67, elapsed : 0.24358582496643066\n",
      "[2024/10/01 22:15:58] ppocr DEBUG: cls num  : 67, elapsed : 0.35264134407043457\n",
      "[2024/10/01 22:16:13] ppocr DEBUG: rec_res num  : 67, elapsed : 14.849129915237427\n",
      "[[[48.0, 73.0], [289.0, 73.0], [289.0, 86.0], [48.0, 86.0]], ('[38] Baoguang Shi, Xiang Bai, and Serge Belongie. Detecting', 0.981089174747467)]\n",
      "[[[328.0, 74.0], [545.0, 74.0], [545.0, 84.0], [328.0, 84.0]], ('Conference on Computer Vision and Pattern Recognition,', 0.983593761920929)]\n",
      "[[[68.0, 84.0], [289.0, 84.0], [289.0, 97.0], [68.0, 97.0]], ('oriented text in natural images by linking segments. In The', 0.977222740650177)]\n",
      "[[[326.0, 85.0], [418.0, 84.0], [418.0, 95.0], [326.0, 96.0]], ('pages 36533662, 2019.', 0.9919434785842896)]\n",
      "[[[68.0, 95.0], [288.0, 95.0], [288.0, 108.0], [68.0, 108.0]], ('IEEE Conference on Computer Vision and Pattern Recogni-', 0.9835411310195923)]\n",
      "[[[306.0, 95.0], [547.0, 96.0], [547.0, 110.0], [306.0, 109.0]], ('[52] Chengquan Zhang, Borong Liang, Zuming Huang, Mengyi', 0.9944654107093811)]\n",
      "[[[69.0, 108.0], [139.0, 108.0], [139.0, 118.0], [69.0, 118.0]], ('tion (CVPR), 2017.', 0.9969325661659241)]\n",
      "[[[326.0, 118.0], [546.0, 119.0], [546.0, 132.0], [326.0, 131.0]], ('than once: An accurate detector for text of arbitrary shapes.', 0.9864867925643921)]\n",
      "[[[328.0, 109.0], [546.0, 109.0], [546.0, 119.0], [328.0, 119.0]], ('En, Junyu Han, Errui Ding, and Xinghao Ding. Look more', 0.9930960536003113)]\n",
      "[[[49.0, 120.0], [287.0, 120.0], [287.0, 130.0], [49.0, 130.0]], ('[39] Baoguang Shi, Mingkun Yang, XingGang Wang, Pengyuan', 0.9925626516342163)]\n",
      "[[[68.0, 131.0], [287.0, 131.0], [287.0, 141.0], [68.0, 141.0]], ('Lyu, Xiang Bai, and Cong Yao. Aster: An attentional scene.', 0.9789591431617737)]\n",
      "[[[328.0, 131.0], [547.0, 131.0], [547.0, 141.0], [328.0, 141.0]], ('Proceedings of the IEEE Conference on Computer Visionn.', 0.9683377146720886)]\n",
      "[[[67.0, 141.0], [289.0, 141.0], [289.0, 153.0], [67.0, 153.0]], ('text recognizer with flexible rectification. IEEE transactions', 0.9777979254722595)]\n",
      "[[[327.0, 141.0], [472.0, 141.0], [472.0, 153.0], [327.0, 153.0]], ('and Pattem Recognition (CVPR), 2019.', 0.9667641520500183)]\n",
      "[[[67.0, 152.0], [286.0, 151.0], [286.0, 162.0], [67.0, 163.0]], ('on pattern analysis and machine intelligence, 31(11):855', 0.9819123148918152)]\n",
      "[[[307.0, 152.0], [547.0, 152.0], [547.0, 165.0], [307.0, 165.0]], ('[53] Xinyu Zhou, Cong Yao, He Wen, Yuzhi Wang, Shuchang', 0.9982225298881531)]\n",
      "[[[69.0, 163.0], [109.0, 163.0], [109.0, 173.0], [69.0, 173.0]], ('868, 2018.', 0.9968101382255554)]\n",
      "[[[326.0, 163.0], [547.0, 163.0], [547.0, 176.0], [326.0, 176.0]], ('Zhou, Weiran He, and Jiajun Liang. EAST: An efficient and', 0.9858408570289612)]\n",
      "[[[48.0, 174.0], [289.0, 174.0], [289.0, 187.0], [48.0, 187.0]], ('[40] Zhuotao Tian, Michelle Shu, Pengyuan Lyu, Ruiyu Li, Chao', 0.9844695925712585)]\n",
      "[[[68.0, 184.0], [288.0, 185.0], [288.0, 198.0], [68.0, 197.0]], ('Zhou, Xiaoyong Shen, and Jiaya Jia. Learning shape-aware', 0.9993064999580383)]\n",
      "[[[328.0, 175.0], [510.0, 175.0], [510.0, 185.0], [328.0, 185.0]], ('accurate scene text detector. In Proc. CVPR, 2017.', 0.9877006411552429)]\n",
      "[[[67.0, 196.0], [287.0, 196.0], [287.0, 209.0], [67.0, 209.0]], ('embedding for scene text detection. In Proceedings of the', 0.9921107888221741)]\n",
      "[[[68.0, 206.0], [288.0, 208.0], [288.0, 221.0], [67.0, 219.0]], ('IEEE Conference on Computer Vision and Pattern Recogni-', 0.9839040637016296)]\n",
      "[[[69.0, 220.0], [176.0, 220.0], [176.0, 230.0], [69.0, 230.0]], ('tion, pages 42344243, 2019.', 0.9957499504089355)]\n",
      "[[[49.0, 232.0], [286.0, 232.0], [286.0, 242.0], [49.0, 242.0]], ('[41] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Woj-', 0.988135814666748)]\n",
      "[[[69.0, 243.0], [287.0, 243.0], [287.0, 252.0], [69.0, 252.0]], ('ciech Zaremba, and Pieter Abbeel. Domain randomization', 0.9906203150749207)]\n",
      "[[[69.0, 253.0], [287.0, 253.0], [287.0, 263.0], [69.0, 263.0]], ('for transferring deep neural networks from simulation to the', 0.9841764569282532)]\n",
      "[[[68.0, 264.0], [287.0, 264.0], [287.0, 274.0], [68.0, 274.0]], ('real world. In 2017 IEEE/RSJ International Conference on', 0.9902260899543762)]\n",
      "[[[68.0, 274.0], [288.0, 274.0], [288.0, 287.0], [68.0, 287.0]], ('Intelligent Robots and Systems (IROS), pages 2330. IEEE,', 0.9889612197875977)]\n",
      "[[[69.0, 287.0], [91.0, 287.0], [91.0, 295.0], [69.0, 295.0]], ('2017.', 0.9986038208007812)]\n",
      "[[[49.0, 299.0], [287.0, 299.0], [287.0, 309.0], [49.0, 309.0]], ('[42] Jonathan Tremblay, Thang To, and Stan Birchfield. Falling', 0.981905996799469)]\n",
      "[[[68.0, 319.0], [288.0, 319.0], [288.0, 332.0], [68.0, 332.0]], ('estimation. In Proc. CVPR Workshops, pages 20382041,', 0.9808975458145142)]\n",
      "[[[69.0, 310.0], [287.0, 310.0], [287.0, 320.0], [69.0, 320.0]], ('things: A synthetic dataset for 3d object detection and pose', 0.9950471520423889)]\n",
      "[[[68.0, 331.0], [91.0, 331.0], [91.0, 342.0], [68.0, 342.0]], ('2018.', 0.996295154094696)]\n",
      "[[[48.0, 343.0], [285.0, 343.0], [285.0, 352.0], [48.0, 352.0]], ('[43] Gul Varol, Javier Romero, Xavier Martin, Naureen Mah', 0.985842227935791)]\n",
      "[[[68.0, 353.0], [288.0, 353.0], [288.0, 366.0], [68.0, 366.0]], ('mood, Michael J Black, Ivan Laptev, and Cordelia Schmid.', 0.9897324442863464)]\n",
      "[[[66.0, 363.0], [288.0, 364.0], [288.0, 377.0], [66.0, 376.0]], ('Learning from synthetic humans. In Proc. CVPR, pages 109', 0.9877805709838867)]\n",
      "[[[69.0, 376.0], [109.0, 376.0], [109.0, 386.0], [69.0, 386.0]], ('117, 2017.', 0.9933309555053711)]\n",
      "[[[48.0, 387.0], [287.0, 387.0], [287.0, 400.0], [48.0, 400.0]], ('[44] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-', 0.9800135493278503)]\n",
      "[[[68.0, 398.0], [288.0, 398.0], [288.0, 411.0], [68.0, 411.0]], ('reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Ilia', 0.9741635918617249)]\n",
      "[[[68.0, 408.0], [288.0, 410.0], [288.0, 423.0], [67.0, 421.0]], ('Polosukhin. Attention is all you need. In Proc. NIPS, pages', 0.984136700630188)]\n",
      "[[[68.0, 421.0], [136.0, 421.0], [136.0, 431.0], [68.0, 431.0]], ('59986008, 2017.', 0.9998801946640015)]\n",
      "[[[49.0, 434.0], [288.0, 434.0], [288.0, 444.0], [49.0, 444.0]], ('[45] Kai Wang, Boris Babenko, and Serge Belongie. End-to-end', 0.9939178824424744)]\n",
      "[[[68.0, 445.0], [286.0, 444.0], [286.0, 454.0], [68.0, 455.0]], ('scene text recognition. In 2011 IEEE International Confer-', 0.9824931025505066)]\n",
      "[[[68.0, 454.0], [288.0, 454.0], [288.0, 467.0], [68.0, 467.0]], ('ence on Computer Vision (ICCV)., pages 14571464. IEEE,', 0.9855703711509705)]\n",
      "[[[69.0, 467.0], [91.0, 467.0], [91.0, 475.0], [69.0, 475.0]], ('2011.', 0.9981307983398438)]\n",
      "[[[47.0, 477.0], [287.0, 477.0], [287.0, 490.0], [47.0, 490.0]], ('[46] Tao Wang, David J Wu, Adam Coates, and Andrew Y Ng.', 0.9947828650474548)]\n",
      "[[[68.0, 487.0], [286.0, 488.0], [286.0, 501.0], [68.0, 500.0]], ('End-to-end text recognition with convolutional neural net-', 0.9849535226821899)]\n",
      "[[[69.0, 501.0], [287.0, 501.0], [287.0, 511.0], [69.0, 511.0]], ('works.In 2012 21st International Conference on Pattern', 0.9664233326911926)]\n",
      "[[[69.0, 512.0], [257.0, 512.0], [257.0, 522.0], [69.0, 522.0]], ('Recognition (ICPR), pages 33043308. IEEE, 2012.', 0.9880173802375793)]\n",
      "[[[49.0, 524.0], [287.0, 524.0], [287.0, 534.0], [49.0, 534.0]], ('[47] Xiaobing Wang, Yingying Jiang, Zhenbo Luo, Cheng-Lin', 0.9753451347351074)]\n",
      "[[[67.0, 544.0], [288.0, 544.0], [288.0, 557.0], [67.0, 557.0]], ('text detection with adaptive text region representation. In', 0.9964331388473511)]\n",
      "[[[69.0, 535.0], [287.0, 535.0], [287.0, 544.0], [69.0, 544.0]], ('Liu, Hyunsoo Choi, and Sungjin Kim. Arbitrary shape scene', 0.997795820236206)]\n",
      "[[[68.0, 555.0], [289.0, 555.0], [289.0, 568.0], [68.0, 568.0]], ('Proceedings of the IEEE Conference on Computer Vision', 0.9831122756004333)]\n",
      "[[[69.0, 566.0], [249.0, 565.0], [249.0, 578.0], [69.0, 579.0]], ('and Pattem Recognition, pages 64496458, 2019.', 0.9898244142532349)]\n",
      "[[[48.0, 577.0], [287.0, 577.0], [287.0, 590.0], [48.0, 590.0]], ('[48] Xinlong Wang, Zhipeng Man, Mingyu You, and Chunhua', 0.9979979991912842)]\n",
      "[[[68.0, 600.0], [288.0, 600.0], [288.0, 613.0], [68.0, 613.0]], ('cations to moving vehicle license plate recognition. arXiv', 0.9771945476531982)]\n",
      "[[[69.0, 591.0], [286.0, 591.0], [286.0, 601.0], [69.0, 601.0]], ('Shen. Adversarial generation of training examples: Appli-', 0.9792717099189758)]\n",
      "[[[67.0, 612.0], [191.0, 611.0], [191.0, 622.0], [68.0, 623.0]], ('preprint arXiv:1707.03124, 2017', 0.9667039513587952)]\n",
      "[[[47.0, 623.0], [287.0, 624.0], [287.0, 637.0], [47.0, 636.0]], ('[49] Qixiang Ye and David Doermann. Text detection and recog-', 0.9808995723724365)]\n",
      "[[[68.0, 635.0], [288.0, 635.0], [288.0, 647.0], [68.0, 647.0]], ('nition in imagery: A survey. IEEE transactions on pattern', 0.9916686415672302)]\n",
      "[[[70.0, 646.0], [283.0, 646.0], [283.0, 656.0], [70.0, 656.0]], ('analysis and machine intelligence, 37(7): 14801500, 2015.', 0.9726215600967407)]\n",
      "[[[48.0, 657.0], [288.0, 657.0], [288.0, 670.0], [48.0, 670.0]], ('[50] Fangneng Zhan, Shijian Lu, and Chuhui Xue. Verisimilar', 0.9736257791519165)]\n",
      "[[[70.0, 670.0], [288.0, 670.0], [288.0, 680.0], [70.0, 680.0]], ('image synthesis for accurate detection and recognition of', 0.9770708084106445)]\n",
      "[[[68.0, 680.0], [205.0, 679.0], [205.0, 690.0], [69.0, 691.0]], ('texts in scenes. In Proc. ECCV, 2018.', 0.9982634782791138)]\n",
      "[[[48.0, 690.0], [286.0, 690.0], [286.0, 703.0], [48.0, 703.0]], ('[51] Fangneng Zhan, Hongyuan Zhu, and Shijian Lu. Spatial fu-', 0.9797888994216919)]\n",
      "[[[66.0, 702.0], [289.0, 701.0], [289.0, 714.0], [66.0, 715.0]], ('sion gan for image synthesis. In Proceedings of the IEEE', 0.9894176125526428)]\n",
      "[2024/10/01 22:16:14] ppocr DEBUG: dt_boxes num : 103, elapsed : 0.2501521110534668\n",
      "[2024/10/01 22:16:15] ppocr DEBUG: cls num  : 103, elapsed : 0.5474562644958496\n",
      "[2024/10/01 22:17:45] ppocr DEBUG: rec_res num  : 103, elapsed : 90.62156367301941\n",
      "[[[67.0, 138.0], [107.0, 138.0], [107.0, 148.0], [67.0, 148.0]], ('Urban City', 0.9529255032539368)]\n",
      "[[[326.0, 129.0], [343.0, 129.0], [343.0, 137.0], [326.0, 137.0]], ('Link', 0.9995774030685425)]\n",
      "[[[57.0, 145.0], [115.0, 147.0], [115.0, 157.0], [57.0, 156.0]], ('Medseval Village', 0.9375678896903992)]\n",
      "[[[169.0, 148.0], [501.0, 148.0], [501.0, 157.0], [169.0, 157.0]], ('https://wwv.unrealengine.con/marketplace/en-Us/product/nedieval-village', 0.9572446346282959)]\n",
      "[[[183.0, 140.0], [484.0, 140.0], [484.0, 147.0], [183.0, 147.0]], ('https://wvw.unrealengine.com/marketplace/en-Us/product/urban-eit', 0.9406954646110535)]\n",
      "[[[79.0, 156.0], [96.0, 156.0], [96.0, 167.0], [79.0, 167.0]], ('Loft', 0.9999463558197021)]\n",
      "[[[65.0, 166.0], [109.0, 166.0], [109.0, 176.0], [65.0, 176.0]], ('Desert Town', 0.999260663986206)]\n",
      "[[[204.0, 158.0], [465.0, 158.0], [465.0, 168.0], [204.0, 168.0]], ('https://ue4arch.con/shop/conplete-projects/archviz/loft/', 0.9683300852775574)]\n",
      "[[[64.0, 176.0], [110.0, 176.0], [110.0, 186.0], [64.0, 186.0]], ('Archinterior 1', 0.9780606627464294)]\n",
      "[[[385.0, 168.0], [486.0, 168.0], [486.0, 175.0], [385.0, 175.0]], ('Us/product/desert-tow', 0.8786853551864624)]\n",
      "[[[51.0, 186.0], [119.0, 186.0], [119.0, 196.0], [51.0, 196.0]], ('Desert Gas Station', 0.9967328310012817)]\n",
      "[[[59.0, 195.0], [115.0, 195.0], [115.0, 205.0], [59.0, 205.0]], ('Modular School', 0.9996453523635864)]\n",
      "[[[164.0, 186.0], [507.0, 186.0], [507.0, 196.0], [164.0, 196.0]], ('https: //wwv.unrealengine.con/marketplace/en-Ds/produet/desert-gas-station', 0.9661296010017395)]\n",
      "[[[61.0, 204.0], [114.0, 205.0], [114.0, 216.0], [60.0, 215.0]], ('Factory District', 0.9405392408370972)]\n",
      "[[[162.0, 196.0], [508.0, 196.0], [508.0, 206.0], [162.0, 206.0]], ('httpa://www.unrealengine.con/marketplace/en-Us/product/modular-school-pack', 0.9549223184585571)]\n",
      "[[[52.0, 215.0], [120.0, 215.0], [120.0, 225.0], [52.0, 225.0]], ('Abandoned Factory', 0.9709509611129761)]\n",
      "[[[147.0, 214.0], [521.0, 216.0], [521.0, 226.0], [147.0, 224.0]], ('httpa://wvw.unrealenglne,con/narketplace/en-Us/product/modular-abandoned-factory', 0.9667426943778992)]\n",
      "[[[61.0, 233.0], [112.0, 234.0], [112.0, 245.0], [60.0, 243.0]], ('Castle Fortress', 0.9387912750244141)]\n",
      "[[[71.0, 225.0], [103.0, 225.0], [103.0, 235.0], [71.0, 235.0]], ('Buddhist', 0.9993401169776917)]\n",
      "[[[65.0, 244.0], [110.0, 244.0], [110.0, 253.0], [65.0, 253.0]], ('Desert Ruin', 0.9871870279312134)]\n",
      "[[[170.0, 235.0], [498.0, 235.0], [498.0, 245.0], [170.0, 245.0]], ('https://www.unrealengine.com/marketplace/en-Us/product/castle-fortrese', 0.9529363512992859)]\n",
      "[[[65.0, 251.0], [110.0, 254.0], [109.0, 265.0], [64.0, 262.0]], ('HALAschviz', 0.9760708808898926)]\n",
      "[[[154.0, 252.0], [515.0, 254.0], [515.0, 264.0], [154.0, 262.0]], ('https://www.unrealengine.com/narketplace/en-Vs/product/hal-archviz-toolkit-v1', 0.961223840713501)]\n",
      "[[[69.0, 271.0], [105.0, 273.0], [104.0, 284.0], [68.0, 282.0]], ('HQ Howse', 0.9538204669952393)]\n",
      "[[[73.0, 263.0], [103.0, 263.0], [103.0, 273.0], [73.0, 273.0]], ('Hospital', 0.9999284744262695)]\n",
      "[[[153.0, 264.0], [517.0, 264.0], [517.0, 274.0], [153.0, 274.0]], ('https://www.unrealengine.com/msrketplace/en-Us/product/nodular-sci-fi-hospits', 0.9379335641860962)]\n",
      "[[[63.0, 282.0], [111.0, 282.0], [111.0, 292.0], [63.0, 292.0]], ('Industrial City', 0.9601056575775146)]\n",
      "[[[160.0, 273.0], [511.0, 273.0], [511.0, 283.0], [160.0, 283.0]], ('https://www.unrealengine.com/marketplace/en-Us/product/hg-residentia1-house', 0.9487301707267761)]\n",
      "[[[171.0, 281.0], [499.0, 282.0], [499.0, 293.0], [171.0, 292.0]], ('https://waw.unrealengine.com/marketplace/en-Us/product/industria1-city', 0.9530129432678223)]\n",
      "[[[64.0, 292.0], [110.0, 292.0], [110.0, 302.0], [64.0, 302.0]], ('Archinterior 2', 0.9943822026252747)]\n",
      "[[[71.0, 300.0], [99.0, 302.0], [98.0, 312.0], [70.0, 310.0]], ('Ofice', 0.8471459150314331)]\n",
      "[[[141.0, 294.0], [515.0, 294.0], [515.0, 301.0], [141.0, 301.0]], ('https://www.unrealengine.com/marketplace/en-Os/product/archinteriors-vol-4-scene', 0.9433662295341492)]\n",
      "[[[61.0, 310.0], [113.0, 311.0], [113.0, 322.0], [60.0, 321.0]], ('Mecting Room', 0.9668609499931335)]\n",
      "[[[151.0, 302.0], [508.0, 302.0], [508.0, 312.0], [151.0, 312.0]], ('https ://www.unrealengine.con/marketplace/en-Ds/product/retro-office-environm', 0.9509806632995605)]\n",
      "[[[65.0, 319.0], [107.0, 321.0], [106.0, 332.0], [64.0, 329.0]], ('Old Village', 0.9797300100326538)]\n",
      "[[[182.0, 312.0], [488.0, 312.0], [488.0, 322.0], [182.0, 322.0]], ('http5://drive-goog1e.con/fi1e/d/0B_mjKk7NOcnEUWEuRDVFQ09SIE0/v1ew', 0.8873593211174011)]\n",
      "[[[51.0, 325.0], [116.0, 329.0], [115.0, 343.0], [50.0, 339.0]], ('Modalar Baslding', 0.8621034622192383)]\n",
      "[[[159.0, 332.0], [509.0, 332.0], [509.0, 342.0], [159.0, 342.0]], ('https://www.unrealengine.com/marketplace/en-Us/product/modular-building-set', 0.956679105758667)]\n",
      "[[[182.0, 323.0], [488.0, 323.0], [488.0, 330.0], [182.0, 330.0]], ('https://www.unrealengine.com/marketplace/en-US/product/old-village', 0.9310383200645447)]\n",
      "[[[61.0, 338.0], [111.0, 341.0], [110.0, 352.0], [60.0, 349.0]], ('Modular Hone', 0.9179233908653259)]\n",
      "[[[72.0, 348.0], [103.0, 351.0], [102.0, 361.0], [71.0, 358.0]], ('Dungeon', 0.9445874094963074)]\n",
      "[[[151.0, 340.0], [519.0, 341.0], [519.0, 350.0], [151.0, 349.0]], ('https://www.unrealengine.con/marketplace/en-Us/product/supergenius-modular-home', 0.9542199373245239)]\n",
      "[[[70.0, 357.0], [105.0, 360.0], [104.0, 371.0], [69.0, 368.0]], ('Old Town', 0.9701358079910278)]\n",
      "[[[141.0, 350.0], [526.0, 350.0], [526.0, 360.0], [141.0, 360.0]], ('https://www.unrealengine.com/marketplace/en-Us/product/top-down-multsstory-dungeon', 0.9543538093566895)]\n",
      "[[[67.0, 369.0], [106.0, 369.0], [106.0, 379.0], [67.0, 379.0]], ('Root Cellar', 0.9998514652252197)]\n",
      "[[[71.0, 379.0], [103.0, 379.0], [103.0, 389.0], [71.0, 389.0]], ('Victorian', 0.98935866355896)]\n",
      "[[[182.0, 371.0], [488.0, 371.0], [488.0, 378.0], [182.0, 378.0]], ('https://www.unreslengine.com/narketplace/en-Us/product/zoot-cellas', 0.9529077410697937)]\n",
      "[[[70.0, 388.0], [104.0, 388.0], [104.0, 398.0], [70.0, 398.0]], ('Spaceship', 0.9993579387664795)]\n",
      "[[[61.0, 398.0], [113.0, 398.0], [113.0, 408.0], [61.0, 408.0]], ('Top-Down City', 0.9986318349838257)]\n",
      "[[[127.0, 390.0], [501.0, 390.0], [501.0, 397.0], [127.0, 397.0]], ('https://www.unrealengine.com/narketplace/en-Us/product/spaceship-interior-enviro', 0.9654809832572937)]\n",
      "[[[174.0, 397.0], [495.0, 398.0], [495.0, 408.0], [174.0, 407.0]], ('Aag3uncpdo3/anpoad/sn=ue/eotndaexaeu/uoa*eugbueeeaun*man// : edaay', 0.5259582996368408)]\n",
      "[[[65.0, 408.0], [107.0, 408.0], [107.0, 418.0], [65.0, 418.0]], ('Scene Name', 0.9976345896720886)]\n",
      "[[[65.0, 418.0], [109.0, 418.0], [109.0, 428.0], [65.0, 428.0]], ('Utopian City', 0.9738205075263977)]\n",
      "[[[177.0, 417.0], [493.0, 419.0], [493.0, 429.0], [177.0, 427.0]], ('https://wsw,unrealengine.com/msrketplace/en-Us/product/utopian-city', 0.9393461346626282)]\n",
      "[[[184.0, 410.0], [488.0, 410.0], [488.0, 417.0], [184.0, 417.0]], ('https://wvw.unrealengine.com/marketplace/en-Ds/product/urban-eity', 0.9302535653114319)]\n",
      "[[[186.0, 433.0], [407.0, 434.0], [407.0, 445.0], [186.0, 444.0]], ('Table 6: The list of 3D scene models used in this work.', 0.9904512763023376)]\n",
      "[[[96.0, 563.0], [168.0, 565.0], [168.0, 576.0], [96.0, 574.0]], ('Implementation', 0.9974708557128906)]\n",
      "[[[203.0, 564.0], [226.0, 564.0], [226.0, 576.0], [203.0, 576.0]], ('Case', 0.9999432563781738)]\n",
      "[[[263.0, 564.0], [284.0, 564.0], [284.0, 575.0], [263.0, 575.0]], ('IIIT', 0.9113329648971558)]\n",
      "[[[290.0, 565.0], [315.0, 565.0], [315.0, 574.0], [290.0, 574.0]], ('SVT', 0.999171257019043)]\n",
      "[[[324.0, 566.0], [349.0, 566.0], [349.0, 575.0], [324.0, 575.0]], ('IC13', 0.992962121963501)]\n",
      "[[[385.0, 564.0], [415.0, 564.0], [415.0, 574.0], [385.0, 574.0]], ('SVTP', 0.9994651079177856)]\n",
      "[[[420.0, 564.0], [465.0, 564.0], [465.0, 575.0], [420.0, 575.0]], ('CUTE80', 0.9970281720161438)]\n",
      "[[[468.0, 564.0], [498.0, 564.0], [498.0, 575.0], [468.0, 575.0]], ('Total', 0.9933941960334778)]\n",
      "[[[111.0, 578.0], [156.0, 578.0], [156.0, 589.0], [111.0, 589.0]], ('Long et al.', 0.9876909255981445)]\n",
      "[[[206.0, 576.0], [224.0, 576.0], [224.0, 589.0], [206.0, 589.0]], ('All', 0.98496013879776)]\n",
      "[[[262.0, 577.0], [284.0, 577.0], [284.0, 588.0], [262.0, 588.0]], ('81.2', 0.9997671842575073)]\n",
      "[[[291.0, 577.0], [314.0, 577.0], [314.0, 588.0], [291.0, 588.0]], ('71.2', 0.9994947910308838)]\n",
      "[[[324.0, 577.0], [346.0, 577.0], [346.0, 588.0], [324.0, 588.0]], ('86.9', 0.9980311989784241)]\n",
      "[[[355.0, 577.0], [378.0, 577.0], [378.0, 588.0], [355.0, 588.0]], ('62.0', 0.967599093914032)]\n",
      "[[[390.0, 577.0], [411.0, 577.0], [411.0, 588.0], [390.0, 588.0]], ('62.3', 0.9990494251251221)]\n",
      "[[[432.0, 576.0], [453.0, 576.0], [453.0, 588.0], [432.0, 588.0]], ('I59', 0.629099428653717)]\n",
      "[[[110.0, 586.0], [156.0, 588.0], [156.0, 602.0], [109.0, 600.0]], ('Beeel', 0.5262538194656372)]\n",
      "[[[389.0, 586.0], [412.0, 586.0], [412.0, 601.0], [389.0, 601.0]], ('62.6', 0.9981972575187683)]\n",
      "[[[473.0, 577.0], [494.0, 577.0], [494.0, 588.0], [473.0, 588.0]], ('44.7', 0.9905243515968323)]\n",
      "[[[206.0, 587.0], [223.0, 587.0], [223.0, 600.0], [206.0, 600.0]], ('All', 0.9842605590820312)]\n",
      "[[[262.0, 588.0], [283.0, 588.0], [283.0, 600.0], [262.0, 600.0]], ('81.5', 0.9829475283622742)]\n",
      "[[[292.0, 588.0], [313.0, 588.0], [313.0, 599.0], [292.0, 599.0]], ('71.7', 0.9989207983016968)]\n",
      "[[[324.0, 588.0], [345.0, 588.0], [345.0, 599.0], [324.0, 599.0]], ('88.9', 0.9995672106742859)]\n",
      "[[[356.0, 588.0], [376.0, 588.0], [376.0, 600.0], [356.0, 600.0]], ('62.1', 0.9997482299804688)]\n",
      "[[[432.0, 588.0], [453.0, 588.0], [453.0, 600.0], [432.0, 600.0]], ('64.9', 0.9993599057197571)]\n",
      "[[[472.0, 588.0], [494.0, 588.0], [494.0, 600.0], [472.0, 600.0]], ('41.5', 0.9979737401008606)]\n",
      "[[[112.0, 602.0], [155.0, 602.0], [155.0, 613.0], [112.0, 613.0]], ('Long et al.', 0.9963198304176331)]\n",
      "[[[175.0, 602.0], [252.0, 602.0], [252.0, 612.0], [175.0, 612.0]], ('lower case + digits', 0.9991323351860046)]\n",
      "[[[264.0, 603.0], [282.0, 603.0], [282.0, 611.0], [264.0, 611.0]], ('89.5', 0.9997192025184631)]\n",
      "[[[292.0, 602.0], [312.0, 602.0], [312.0, 613.0], [292.0, 613.0]], ('84.1', 0.9997279644012451)]\n",
      "[[[324.0, 601.0], [345.0, 601.0], [345.0, 612.0], [324.0, 612.0]], ('89.9', 0.999576985836029)]\n",
      "[[[355.0, 602.0], [376.0, 602.0], [376.0, 613.0], [355.0, 613.0]], ('68.8', 0.9997488260269165)]\n",
      "[[[391.0, 601.0], [411.0, 601.0], [411.0, 613.0], [391.0, 613.0]], ('73.5', 0.9929868578910828)]\n",
      "[[[432.0, 601.0], [452.0, 601.0], [452.0, 613.0], [432.0, 613.0]], ('76.3', 0.9996763467788696)]\n",
      "[[[472.0, 601.0], [494.0, 601.0], [494.0, 613.0], [472.0, 613.0]], ('58.2', 0.999404788017273)]\n",
      "[[[111.0, 611.0], [156.0, 613.0], [156.0, 627.0], [110.0, 624.0]], ('Baek et al', 0.9758213758468628)]\n",
      "[[[174.0, 613.0], [254.0, 614.0], [254.0, 625.0], [174.0, 624.0]], ('lower case + digits', 0.9938027262687683)]\n",
      "[[[262.0, 613.0], [284.0, 613.0], [284.0, 624.0], [262.0, 624.0]], ('86.5', 0.9982804656028748)]\n",
      "[[[291.0, 613.0], [313.0, 613.0], [313.0, 624.0], [291.0, 624.0]], ('83.5', 0.9991432428359985)]\n",
      "[[[323.0, 613.0], [345.0, 613.0], [345.0, 624.0], [323.0, 624.0]], ('93.0', 0.9965963959693909)]\n",
      "[[[355.0, 613.0], [376.0, 613.0], [376.0, 624.0], [355.0, 624.0]], ('70.3', 0.998284101486206)]\n",
      "[[[391.0, 613.0], [410.0, 613.0], [410.0, 624.0], [391.0, 624.0]], ('75.1', 0.9996007680892944)]\n",
      "[[[432.0, 613.0], [453.0, 613.0], [453.0, 624.0], [432.0, 624.0]], ('68.4', 0.9993353486061096)]\n",
      "[[[472.0, 612.0], [495.0, 612.0], [495.0, 624.0], [472.0, 624.0]], ('46.0', 0.985621988773346)]\n",
      "[[[48.0, 631.0], [546.0, 631.0], [546.0, 643.0], [48.0, 643.0]], ('Table 7: Results on English datasets (word level accuracy). All indicates that the evaluation considers lower case characters,', 0.988560140132904)]\n",
      "[[[47.0, 644.0], [302.0, 642.0], [302.0, 655.0], [47.0, 656.0]], ('upper case characters, numerical digits, and punctuation marks.', 0.9935306310653687)]\n",
      "[2024/10/01 22:17:51] ppocr DEBUG: dt_boxes num : 69, elapsed : 5.289295196533203\n",
      "[2024/10/01 22:17:52] ppocr DEBUG: cls num  : 69, elapsed : 0.48616886138916016\n",
      "[2024/10/01 22:22:17] ppocr DEBUG: rec_res num  : 69, elapsed : 265.22069001197815\n",
      "[[[49.0, 73.0], [135.0, 73.0], [135.0, 83.0], [49.0, 83.0]], ('A. Scene Models', 0.9675092101097107)]\n",
      "[[[311.0, 74.0], [336.0, 74.0], [336.0, 83.0], [311.0, 83.0]], ('Dataset', 0.9993898272514343)]\n",
      "[[[351.0, 74.0], [397.0, 74.0], [397.0, 84.0], [351.0, 84.0]], ('Sample Image', 0.9981042742729187)]\n",
      "[[[493.0, 75.0], [542.0, 75.0], [542.0, 82.0], [493.0, 82.0]], ('New Annotation', 0.9715968370437622)]\n",
      "[[[61.0, 93.0], [288.0, 93.0], [288.0, 103.0], [61.0, 103.0]], ('In this work, we use a total number of 30 scene models', 0.9896504282951355)]\n",
      "[[[309.0, 94.0], [338.0, 94.0], [338.0, 104.0], [309.0, 104.0]], ('CUTE80', 0.9971396923065186)]\n",
      "[[[429.0, 92.0], [453.0, 94.0], [452.0, 106.0], [427.0, 104.0]], ('TEAM', 0.9995269775390625)]\n",
      "[[[507.0, 94.0], [527.0, 94.0], [527.0, 106.0], [507.0, 106.0]], ('Team', 0.988610565662384)]\n",
      "[[[48.0, 104.0], [288.0, 104.0], [288.0, 117.0], [48.0, 117.0]], ('which are all obtained from the Internet. However, most of', 0.9905250072479248)]\n",
      "[[[48.0, 116.0], [287.0, 116.0], [287.0, 129.0], [48.0, 129.0]], ('these models are not free. Therefore, we are not allowed to', 0.9781576991081238)]\n",
      "[[[310.0, 125.0], [332.0, 125.0], [332.0, 136.0], [310.0, 136.0]], ('IIT5K', 0.9599459767341614)]\n",
      "[[[354.0, 120.0], [396.0, 120.0], [396.0, 138.0], [354.0, 138.0]], ('15%.', 0.9610542058944702)]\n",
      "[[[49.0, 130.0], [287.0, 130.0], [287.0, 140.0], [49.0, 140.0]], ('share the models themselves. Instead, we list the models we', 0.9888137578964233)]\n",
      "[[[435.0, 125.0], [446.0, 125.0], [446.0, 135.0], [435.0, 135.0]], ('15', 0.9980641603469849)]\n",
      "[[[508.0, 125.0], [526.0, 125.0], [526.0, 137.0], [508.0, 137.0]], ('15%.', 0.9965531229972839)]\n",
      "[[[49.0, 142.0], [163.0, 142.0], [163.0, 151.0], [49.0, 151.0]], ('use and their links in Tab. 6.', 0.9885517954826355)]\n",
      "[[[48.0, 163.0], [287.0, 163.0], [287.0, 176.0], [48.0, 176.0]], ('B. New Annotations for Scene Text Recogni-', 0.9776868224143982)]\n",
      "[[[309.0, 156.0], [327.0, 156.0], [327.0, 167.0], [309.0, 167.0]], ('SVT', 0.999408483505249)]\n",
      "[[[424.0, 156.0], [457.0, 156.0], [457.0, 166.0], [424.0, 166.0]], ('DONALD', 0.9997930526733398)]\n",
      "[[[504.0, 156.0], [529.0, 156.0], [529.0, 167.0], [504.0, 167.0]], ('Donald', 0.9989724159240723)]\n",
      "[[[48.0, 177.0], [117.0, 178.0], [117.0, 189.0], [48.0, 188.0]], ('tion Datasets', 0.9628291726112366)]\n",
      "[[[309.0, 187.0], [330.0, 187.0], [330.0, 198.0], [309.0, 198.0]], ('SVTP', 0.9911746978759766)]\n",
      "[[[419.0, 187.0], [462.0, 187.0], [462.0, 197.0], [419.0, 197.0]], ('MARLBORO', 0.9997438192367554)]\n",
      "[[[502.0, 186.0], [532.0, 188.0], [531.0, 198.0], [501.0, 196.0]], ('Marlbero', 0.7891457676887512)]\n",
      "[[[60.0, 197.0], [285.0, 197.0], [285.0, 210.0], [60.0, 210.0]], ('During the experiments of scene text recognition for En-', 0.997437596321106)]\n",
      "[[[48.0, 209.0], [288.0, 209.0], [288.0, 222.0], [48.0, 222.0]], ('glish scripts, we notice that among the most widely used', 0.9908959269523621)]\n",
      "[[[49.0, 223.0], [286.0, 223.0], [286.0, 233.0], [49.0, 233.0]], ('benchmark datasets, several have incomplete annotations.', 0.9961205124855042)]\n",
      "[[[337.0, 215.0], [513.0, 215.0], [513.0, 228.0], [337.0, 228.0]], ('Figure 6: Examples of the new annotations.', 0.9935643076896667)]\n",
      "[[[48.0, 233.0], [287.0, 233.0], [287.0, 246.0], [48.0, 246.0]], ('They are IIIT5K, SVT, SVTP, and CUTE-80. The annota-', 0.9879468679428101)]\n",
      "[[[48.0, 246.0], [285.0, 246.0], [285.0, 255.0], [48.0, 255.0]], ('tions of these datasets are case-insensitive, and ignore punc-', 0.9870139956474304)]\n",
      "[[[48.0, 257.0], [107.0, 257.0], [107.0, 267.0], [48.0, 267.0]], ('tuation marks.', 0.9975446462631226)]\n",
      "[[[307.0, 250.0], [542.0, 250.0], [542.0, 260.0], [307.0, 260.0]], ('that it may still be a challenge to recognize a larger vocabu', 0.9879502058029175)]\n",
      "[[[61.0, 269.0], [288.0, 269.0], [288.0, 279.0], [61.0, 279.0]], ('The common practice for recent scene text recognition', 0.9838630557060242)]\n",
      "[[[307.0, 262.0], [445.0, 262.0], [445.0, 272.0], [307.0, 272.0]], ('lary, and is worth further research.', 0.958018958568573)]\n",
      "[[[49.0, 280.0], [286.0, 280.0], [286.0, 290.0], [49.0, 290.0]], ('research is to convert both prediction and ground-truth text', 0.9940388202667236)]\n",
      "[[[48.0, 292.0], [288.0, 292.0], [288.0, 305.0], [48.0, 305.0]], ('strings to lower-case and then compare them. This means', 0.9873347282409668)]\n",
      "[[[48.0, 304.0], [288.0, 304.0], [288.0, 317.0], [48.0, 317.0]], ('that the current evaluation is flawed. It ignores letter case', 0.9863680601119995)]\n",
      "[[[49.0, 318.0], [286.0, 318.0], [286.0, 328.0], [49.0, 328.0]], ('and punctuation marks which are crucial to the understand-', 0.9971910715103149)]\n",
      "[[[49.0, 329.0], [286.0, 329.0], [286.0, 339.0], [49.0, 339.0]], ('ing of the text contents. Besides, evaluating on a much', 0.9688020348548889)]\n",
      "[[[49.0, 342.0], [286.0, 342.0], [286.0, 351.0], [49.0, 351.0]], ('smaller vocabulary set results in over-optimism of the per-', 0.9991984963417053)]\n",
      "[[[49.0, 353.0], [194.0, 353.0], [194.0, 363.0], [49.0, 363.0]], ('formance of the recognition models..', 0.9747090935707092)]\n",
      "[[[61.0, 364.0], [285.0, 364.0], [285.0, 374.0], [61.0, 374.0]], ('To aid further research, we use the Amazon mechan-', 0.9826070666313171)]\n",
      "[[[49.0, 376.0], [286.0, 376.0], [286.0, 386.0], [49.0, 386.0]], ('ical Turk (AMT) to re-annotate the aforementioned 4', 0.9840131998062134)]\n",
      "[[[48.0, 388.0], [287.0, 388.0], [287.0, 401.0], [48.0, 401.0]], ('datasets, which amount to 6837 word images in total.', 0.9847704768180847)]\n",
      "[[[48.0, 399.0], [287.0, 400.0], [287.0, 413.0], [48.0, 412.0]], ('Each word image is annotated by 3 workers, and we', 0.9940709471702576)]\n",
      "[[[48.0, 412.0], [287.0, 412.0], [287.0, 425.0], [48.0, 425.0]], ('manually check and correct images where the 3 an-', 0.9807734489440918)]\n",
      "[[[49.0, 426.0], [127.0, 426.0], [127.0, 436.0], [49.0, 436.0]], ('notationsdiffer.', 0.8888248801231384)]\n",
      "[[[122.0, 424.0], [287.0, 425.0], [287.0, 436.0], [122.0, 435.0]], ('The annotated datasets are released', 0.9614172577857971)]\n",
      "[[[46.0, 435.0], [287.0, 437.0], [287.0, 450.0], [46.0, 447.0]], ('via GitHub at https://github.com/Jyouhou/', 0.9704374670982361)]\n",
      "[[[49.0, 449.0], [330.0, 449.0], [330.0, 459.0], [49.0, 459.0]], ('Case-Sensitive-Scene-Text-Recognition-Datasets.', 0.987173855304718)]\n",
      "[[[49.0, 468.0], [109.0, 468.0], [109.0, 479.0], [49.0, 479.0]], ('B.1 Samples', 0.9922935962677002)]\n",
      "[[[60.0, 485.0], [286.0, 485.0], [286.0, 498.0], [60.0, 498.0]], ('We select some samples from the 4 datasets to demon-', 0.9974356889724731)]\n",
      "[[[47.0, 496.0], [194.0, 498.0], [194.0, 511.0], [47.0, 509.0]], ('strate the new annotations in Fig. 6.', 0.9882925152778625)]\n",
      "[[[48.0, 518.0], [192.0, 518.0], [192.0, 531.0], [48.0, 531.0]], ('B.2 Benchmark Performances', 0.9993060827255249)]\n",
      "[[[61.0, 537.0], [284.0, 537.0], [284.0, 546.0], [61.0, 546.0]], ('As we are encouraging case-sensitive (also with punctua', 0.9991858005523682)]\n",
      "[[[47.0, 558.0], [287.0, 559.0], [287.0, 572.0], [47.0, 571.0]], ('like to provide benchmark performances on those widely', 0.9998530745506287)]\n",
      "[[[49.0, 549.0], [287.0, 549.0], [287.0, 559.0], [49.0, 559.0]], ('tion marks) evaluation for scene text recognition, we would', 0.9979208707809448)]\n",
      "[[[49.0, 573.0], [287.0, 573.0], [287.0, 583.0], [49.0, 583.0]], ('used datasets.We evaluate two implementations of the', 0.9874305725097656)]\n",
      "[[[48.0, 584.0], [287.0, 584.0], [287.0, 597.0], [48.0, 597.0]], ('ASTER models, by Long et al. and Baek er af respec-', 0.95820552110672)]\n",
      "[[[48.0, 595.0], [214.0, 595.0], [214.0, 608.0], [48.0, 608.0]], ('tively. Results are summarized in Tab. 7.', 0.9910298585891724)]\n",
      "[[[60.0, 606.0], [286.0, 607.0], [286.0, 620.0], [60.0, 619.0]], ('The two benchmark implementations perform compara-', 0.9887709021568298)]\n",
      "[[[48.0, 619.0], [288.0, 619.0], [288.0, 632.0], [48.0, 632.0]], ('bly, with Baeks better on straight text and Longs better at', 0.9809600114822388)]\n",
      "[[[49.0, 633.0], [286.0, 633.0], [286.0, 643.0], [49.0, 643.0]], ('curved text. Compared with evaluation with fower case +', 0.9950405955314636)]\n",
      "[[[49.0, 644.0], [287.0, 644.0], [287.0, 654.0], [49.0, 654.0]], ('digits, the performance drops considerably for both models', 0.9921355843544006)]\n",
      "[[[49.0, 656.0], [287.0, 656.0], [287.0, 666.0], [49.0, 666.0]], ('when we evaluate with all symbols. These results indicate', 0.9984021782875061)]\n",
      "[[[61.0, 675.0], [194.0, 675.0], [194.0, 685.0], [61.0, 685.0]], ('https://github.con/Jyouhou/', 0.9657115340232849)]\n",
      "[[[49.0, 685.0], [209.0, 685.0], [209.0, 695.0], [49.0, 695.0]], ('ICDARz019-ArT-Recognition-Alchemy', 0.9525253772735596)]\n",
      "[[[59.0, 692.0], [193.0, 693.0], [193.0, 706.0], [59.0, 705.0]], ('*https://github.con/clovaai/', 0.959469735622406)]\n",
      "[[[48.0, 703.0], [200.0, 704.0], [200.0, 715.0], [48.0, 714.0]], ('deep-text-recognicion-benchnark', 0.9751423597335815)]\n",
      "[2024/10/01 22:22:20] ppocr DEBUG: dt_boxes num : 89, elapsed : 2.0809240341186523\n",
      "[2024/10/01 22:22:21] ppocr DEBUG: cls num  : 89, elapsed : 0.6519079208374023\n",
      "[2024/10/01 22:25:39] ppocr DEBUG: rec_res num  : 89, elapsed : 198.5230438709259\n",
      "[[[307.0, 72.0], [425.0, 72.0], [425.0, 85.0], [307.0, 85.0]], ('5.1.2Experiment Results', 0.9974668622016907)]\n",
      "[[[307.0, 97.0], [548.0, 97.0], [548.0, 111.0], [307.0, 111.0]], ('Experiment results are summarized in Tab. 7. First, we', 0.9747073650360107)]\n",
      "[[[306.0, 109.0], [548.0, 108.0], [548.0, 121.0], [306.0, 122.0]], ('compare our method with previous synthetic datasets. We', 0.9944857358932495)]\n",
      "[[[308.0, 123.0], [546.0, 123.0], [546.0, 133.0], [308.0, 133.0]], ('have to limit the size of training datasets to 1M/ since.', 0.9727500081062317)]\n",
      "[[[308.0, 134.0], [547.0, 134.0], [547.0, 147.0], [308.0, 147.0]], ('VISD only publishes 1M word images. Our synthetic data', 0.9912510514259338)]\n",
      "[[[307.0, 145.0], [546.0, 146.0], [546.0, 158.0], [307.0, 157.0]], ('achieves consistent improvements on all datasets.Espe-', 0.987369954586029)]\n",
      "[[[48.0, 156.0], [288.0, 156.0], [288.0, 169.0], [48.0, 169.0]], ('Figure 5: Results of ablation tests: (a) ablating viewfinder', 0.9882282614707947)]\n",
      "[[[305.0, 156.0], [545.0, 156.0], [545.0, 169.0], [305.0, 169.0]], ('cially, it surpasses other synthetic datasets by a consider-', 0.9890758395195007)]\n",
      "[[[47.0, 168.0], [280.0, 169.0], [280.0, 182.0], [47.0, 181.0]], ('module; (b) ablating environment randomization module.', 0.9951839447021484)]\n",
      "[[[307.0, 168.0], [547.0, 169.0], [547.0, 182.0], [307.0, 181.0]], ('able margin on datasets with diverse text styles and complex', 0.9908605217933655)]\n",
      "[[[306.0, 180.0], [545.0, 180.0], [545.0, 193.0], [306.0, 193.0]], ('backgrounds such as SVTP (+2.4%). The experiments ver-', 0.9762158393859863)]\n",
      "[[[48.0, 204.0], [287.0, 204.0], [287.0, 217.0], [48.0, 217.0]], ('random lighting conditions realize different real-world vari-', 0.9841779470443726)]\n",
      "[[[309.0, 195.0], [546.0, 195.0], [546.0, 205.0], [309.0, 205.0]], ('ify the effectiveness of our synthesis method in scene text', 0.9990286231040955)]\n",
      "[[[307.0, 207.0], [484.0, 207.0], [484.0, 217.0], [307.0, 217.0]], ('recognition especially in the complex cases.', 0.999947190284729)]\n",
      "[[[49.0, 218.0], [234.0, 218.0], [234.0, 228.0], [49.0, 228.0]], ('ations, which we also attribute as a key factor.', 0.9896178245544434)]\n",
      "[[[319.0, 220.0], [547.0, 220.0], [547.0, 233.0], [319.0, 233.0]], ('Since small scale experiments are not very helpful in', 0.9895458817481995)]\n",
      "[[[48.0, 243.0], [265.0, 243.0], [265.0, 255.0], [48.0, 255.0]], ('5. Experiments on Scene Text Recognition', 0.9996684193611145)]\n",
      "[[[308.0, 234.0], [546.0, 234.0], [546.0, 244.0], [308.0, 244.0]], ('how researchers should utilize these datasets, we further', 0.984186053276062)]\n",
      "[[[306.0, 244.0], [547.0, 245.0], [547.0, 257.0], [306.0, 256.0]], ('train models on combinations of Synth90K, SynthText, and', 0.9873238205909729)]\n",
      "[[[59.0, 262.0], [288.0, 262.0], [288.0, 275.0], [59.0, 275.0]], ('In addition to the superior performances in training scene', 0.9909811615943909)]\n",
      "[[[307.0, 256.0], [546.0, 257.0], [546.0, 268.0], [307.0, 267.0]], ('ours. We first limit the total number of training images to', 0.9923202991485596)]\n",
      "[[[48.0, 274.0], [289.0, 274.0], [289.0, 287.0], [48.0, 287.0]], ('text detection models, we also verify its effectiveness in the', 0.9807828664779663)]\n",
      "[[[307.0, 268.0], [547.0, 268.0], [547.0, 281.0], [307.0, 281.0]], ('9M. When we train on a combination of all 3 synthetic', 0.9748746156692505)]\n",
      "[[[48.0, 286.0], [170.0, 287.0], [170.0, 298.0], [48.0, 297.0]], ('task of scene text recognition.', 0.9707608819007874)]\n",
      "[[[308.0, 280.0], [547.0, 280.0], [547.0, 293.0], [308.0, 293.0]], ('datasets, with 3M each, the model performs better than the', 0.9936041831970215)]\n",
      "[[[307.0, 292.0], [546.0, 292.0], [546.0, 305.0], [307.0, 305.0]], ('model trained on 4.5M  2 datasets only. We further ob-', 0.963310718536377)]\n",
      "[[[48.0, 309.0], [208.0, 309.0], [208.0, 322.0], [48.0, 322.0]], ('5.1. Recognizing Latin Scene Text', 0.9819073677062988)]\n",
      "[[[309.0, 306.0], [544.0, 306.0], [544.0, 316.0], [309.0, 316.0]], ('serve that training on 3M  3 synthetic datasets is com-', 0.9877870082855225)]\n",
      "[[[76.0, 325.0], [115.0, 328.0], [114.0, 342.0], [75.0, 339.0]], ('Settings', 0.9775261878967285)]\n",
      "[[[307.0, 317.0], [544.0, 317.0], [544.0, 327.0], [307.0, 327.0]], ('parable to training on the whole Synth90K and SynthText,', 0.9946207404136658)]\n",
      "[[[49.0, 329.0], [79.0, 329.0], [79.0, 340.0], [49.0, 340.0]], ('5.1.1', 0.9007384181022644)]\n",
      "[[[307.0, 328.0], [547.0, 329.0], [547.0, 342.0], [307.0, 341.0]], ('while using much fewer training data. This result suggests', 0.9871450662612915)]\n",
      "[[[48.0, 346.0], [287.0, 346.0], [287.0, 359.0], [48.0, 359.0]], ('Model We select a widely accepted baseline method,', 0.9682563543319702)]\n",
      "[[[306.0, 341.0], [546.0, 341.0], [546.0, 353.0], [306.0, 353.0]], ('that the best practice is to combine the proposed synthetic', 0.9907719492912292)]\n",
      "[[[47.0, 359.0], [287.0, 360.0], [287.0, 373.0], [47.0, 372.0]], ('ASTER [39], and adopt the implementation that ranks top-', 0.984955906867981)]\n",
      "[[[306.0, 352.0], [418.0, 352.0], [418.0, 365.0], [306.0, 365.0]], ('dataset with previous ones.', 0.9889126420021057)]\n",
      "[[[48.0, 372.0], [288.0, 372.0], [288.0, 385.0], [48.0, 385.0]], ('1 on the ICDAR 2019 ArT competition on curved scene text', 0.987846314907074)]\n",
      "[[[48.0, 384.0], [288.0, 383.0], [288.0, 396.0], [48.0, 397.0]], ('recognition (Latin) by [20]. The models are trained with a', 0.9906794428825378)]\n",
      "[[[308.0, 380.0], [500.0, 380.0], [500.0, 393.0], [308.0, 393.0]], ('5.2. Recognizing Multilingual Scene Text', 0.9904667139053345)]\n",
      "[[[47.0, 396.0], [288.0, 396.0], [288.0, 409.0], [47.0, 409.0]], ('batch size of 512. A total of 95 symbols are recognized,', 0.9908655285835266)]\n",
      "[[[48.0, 410.0], [287.0, 410.0], [287.0, 420.0], [48.0, 420.0]], ('including an End-of-Sentence mark, 52 case sensitive al-', 0.9988681674003601)]\n",
      "[[[309.0, 404.0], [330.0, 404.0], [330.0, 412.0], [309.0, 412.0]], ('5.2.1', 0.9978954195976257)]\n",
      "[[[334.0, 403.0], [372.0, 403.0], [372.0, 414.0], [334.0, 414.0]], ('Settings', 0.9901514053344727)]\n",
      "[[[49.0, 422.0], [277.0, 422.0], [277.0, 432.0], [49.0, 432.0]], ('phabets, 10 digits, and 32 printable punctuation symbols.', 0.9889453649520874)]\n",
      "[[[48.0, 433.0], [287.0, 433.0], [287.0, 446.0], [48.0, 446.0]], ('Training Datasets From the 600K English synthetic im-', 0.9947285652160645)]\n",
      "[[[307.0, 426.0], [546.0, 426.0], [546.0, 439.0], [307.0, 439.0]], ('Although MLT 2017 has been widely used as a benchmark', 0.9996664524078369)]\n",
      "[[[48.0, 445.0], [287.0, 445.0], [287.0, 457.0], [48.0, 457.0]], ('ages, we obtain a total number of 12M word-level image', 0.9993340969085693)]\n",
      "[[[308.0, 440.0], [546.0, 440.0], [546.0, 449.0], [308.0, 449.0]], ('for detection, the task of recognizing multilingual scene text', 0.9995525479316711)]\n",
      "[[[48.0, 456.0], [288.0, 456.0], [288.0, 469.0], [48.0, 469.0]], ('regions to make our training dataset. Also note that, our', 0.9878071546554565)]\n",
      "[[[308.0, 450.0], [547.0, 450.0], [547.0, 463.0], [308.0, 463.0]], ('still remains largely untouched, mainly due to lack of a', 0.9952095746994019)]\n",
      "[[[48.0, 468.0], [289.0, 468.0], [289.0, 481.0], [48.0, 481.0]], ('synthetic dataset provide character level annotations, which', 0.9954919815063477)]\n",
      "[[[308.0, 462.0], [545.0, 461.0], [545.0, 474.0], [308.0, 475.0]], ('proper training dataset. To pave the way for future research,', 0.993747353553772)]\n",
      "[[[48.0, 480.0], [237.0, 480.0], [237.0, 493.0], [48.0, 493.0]], ('will be useful in some recognition algorithms.', 0.980058491230011)]\n",
      "[[[308.0, 474.0], [547.0, 474.0], [547.0, 487.0], [308.0, 487.0]], ('we also generate a multilingual version with 600K images', 0.986902117729187)]\n",
      "[[[48.0, 493.0], [287.0, 493.0], [287.0, 506.0], [48.0, 506.0]], ('Evaluation Datasets We evaluate models trained on dif-', 0.9956595301628113)]\n",
      "[[[49.0, 507.0], [287.0, 507.0], [287.0, 517.0], [49.0, 517.0]], ('ferent synthetic datasets on several widely used real image', 0.9998960494995117)]\n",
      "[[[306.0, 500.0], [544.0, 500.0], [544.0, 510.0], [306.0, 510.0]], ('Arabic, Bangia, Chinese, English, French, German, Hindi,', 0.9622498154640198)]\n",
      "[[[49.0, 519.0], [287.0, 519.0], [287.0, 529.0], [49.0, 529.0]], ('datasets:IIIT [25], SVT [45], ICDAR 2015 (IC15) [13],', 0.9544116854667664)]\n",
      "[[[307.0, 511.0], [545.0, 511.0], [545.0, 521.0], [307.0, 521.0]], ('Italian, Japanese, and Korean. Text contents are sampled', 0.9892991185188293)]\n",
      "[[[48.0, 530.0], [218.0, 530.0], [218.0, 543.0], [48.0, 543.0]], ('SVTP [32], CUTE [34], and Total-Texr[4].', 0.9577473402023315)]\n",
      "[[[307.0, 524.0], [510.0, 524.0], [510.0, 534.0], [307.0, 534.0]], ('from corpus extracted from the Wikimedia dump.', 0.9793853163719177)]\n",
      "[[[60.0, 543.0], [288.0, 543.0], [288.0, 555.0], [60.0, 555.0]], ('Some of these datasets, however, have incomplete an-', 0.991438627243042)]\n",
      "[[[307.0, 537.0], [545.0, 537.0], [545.0, 549.0], [307.0, 549.0]], ('Model We use the same model and implementation as Sec-', 0.9971280097961426)]\n",
      "[[[48.0, 554.0], [289.0, 554.0], [289.0, 567.0], [48.0, 567.0]], ('notations, including IIIT, SVT, SVTP, CUTE. While the', 0.9869527220726013)]\n",
      "[[[306.0, 549.0], [547.0, 549.0], [547.0, 562.0], [306.0, 562.0]], ('tion 5.1, except that the symbols to recognize are expanded', 0.9958102703094482)]\n",
      "[[[47.0, 565.0], [287.0, 566.0], [287.0, 579.0], [47.0, 578.0]], ('word images in these datasets contain punctuation symbols,', 0.993982195854187)]\n",
      "[[[308.0, 563.0], [517.0, 563.0], [517.0, 573.0], [308.0, 573.0]], ('to all characters that appear in the generated dataset.', 0.9751924872398376)]\n",
      "[[[48.0, 577.0], [287.0, 577.0], [287.0, 590.0], [48.0, 590.0]], ('digits, upper-case and lower-case characters, the aforemen-', 0.9920446276664734)]\n",
      "[[[307.0, 576.0], [548.0, 576.0], [548.0, 589.0], [307.0, 589.0]], ('Training and Evaluation Data We crop from the proposed', 0.9997977614402771)]\n",
      "[[[49.0, 590.0], [285.0, 590.0], [285.0, 600.0], [49.0, 600.0]], ('tioned datasets, in their current forms, only provide case', 0.9952835440635681)]\n",
      "[[[307.0, 588.0], [547.0, 588.0], [547.0, 601.0], [307.0, 601.0]], ('multilingual dataset. We discard images with widths shorter', 0.9847042560577393)]\n",
      "[[[49.0, 602.0], [285.0, 602.0], [285.0, 612.0], [49.0, 612.0]], ('insensitive annotations and ignore all punctuation symbols', 0.995205283164978)]\n",
      "[[[307.0, 600.0], [548.0, 600.0], [548.0, 613.0], [307.0, 613.0]], ('than 32 pixels as they are too blurry, and obtain 4.1M word', 0.9923457503318787)]\n",
      "[[[49.0, 615.0], [287.0, 615.0], [287.0, 625.0], [49.0, 625.0]], ('In order for more comprehensive evaluation of scene text', 0.9795581102371216)]\n",
      "[[[308.0, 614.0], [546.0, 614.0], [546.0, 624.0], [308.0, 624.0]], ('images in total. We compare with the multilingual version', 0.9808647632598877)]\n",
      "[[[49.0, 627.0], [287.0, 627.0], [287.0, 637.0], [49.0, 637.0]], ('recognition, we re-annotate these 4 datasets in a case-', 0.9906278848648071)]\n",
      "[[[306.0, 625.0], [544.0, 625.0], [544.0, 635.0], [306.0, 635.0]], ('of SynthText provided by MLT 2019 competition that con', 0.9757155179977417)]\n",
      "[[[49.0, 638.0], [286.0, 638.0], [286.0, 647.0], [49.0, 647.0]], ('sensitive way and also include punctuation symbols.We', 0.9864665865898132)]\n",
      "[[[308.0, 638.0], [544.0, 638.0], [544.0, 647.0], [308.0, 647.0]], ('tains a total number 1.2M images. For evaluation, we ran-', 0.9810096621513367)]\n",
      "[[[48.0, 649.0], [288.0, 649.0], [288.0, 662.0], [48.0, 662.0]], ('also release the new annotations and we believe that they', 0.9949110746383667)]\n",
      "[[[307.0, 648.0], [545.0, 648.0], [545.0, 661.0], [307.0, 661.0]], ('domly split 1500 images for each language (including sym-', 0.9788294434547424)]\n",
      "[[[48.0, 661.0], [288.0, 662.0], [288.0, 673.0], [48.0, 672.0]], ('will become better benchmarks for scene text recognition', 0.9992815852165222)]\n",
      "[[[307.0, 660.0], [546.0, 659.0], [546.0, 672.0], [307.0, 673.0]], ('bols and mixed) from the training set of MLT 2019. The', 0.9954760670661926)]\n",
      "[[[48.0, 673.0], [101.0, 674.0], [100.0, 685.0], [48.0, 684.0]], ('in the future.', 0.997209370136261)]\n",
      "[[[308.0, 674.0], [475.0, 674.0], [475.0, 684.0], [308.0, 684.0]], ('rest of the training set is used for training,', 0.9835272431373596)]\n",
      "[[[59.0, 692.0], [195.0, 693.0], [195.0, 706.0], [59.0, 705.0]], ('Shttps://glthub.con/Jyouhou/', 0.9463998675346375)]\n",
      "[[[48.0, 703.0], [209.0, 704.0], [209.0, 715.0], [48.0, 714.0]], ('ICDAR2019-ArT-Recognicion-Alchemy', 0.9445436596870422)]\n",
      "[[[317.0, 701.0], [454.0, 703.0], [454.0, 716.0], [317.0, 714.0]], ('httpa://dumps.wikimadia.org', 0.9204043745994568)]\n",
      "[2024/10/01 22:25:43] ppocr DEBUG: dt_boxes num : 170, elapsed : 2.5239620208740234\n",
      "[2024/10/01 22:25:44] ppocr DEBUG: cls num  : 170, elapsed : 1.1727006435394287\n",
      "[2024/10/01 22:28:14] ppocr DEBUG: rec_res num  : 170, elapsed : 149.39654779434204\n",
      "[[[118.0, 72.0], [162.0, 72.0], [162.0, 82.0], [118.0, 82.0]], ('Iraining Data', 0.9853467345237732)]\n",
      "[[[119.0, 79.0], [161.0, 81.0], [161.0, 92.0], [119.0, 90.0]], ('ST (1.2M)', 0.9798759818077087)]\n",
      "[[[240.0, 81.0], [258.0, 81.0], [258.0, 92.0], [240.0, 92.0]], ('50.5', 0.9818994998931885)]\n",
      "[[[272.0, 81.0], [289.0, 81.0], [289.0, 92.0], [272.0, 92.0]], ('17.7', 0.9684677124023438)]\n",
      "[[[304.0, 81.0], [322.0, 81.0], [322.0, 92.0], [304.0, 92.0]], ('43.9', 0.9889505505561829)]\n",
      "[[[335.0, 81.0], [353.0, 81.0], [353.0, 92.0], [335.0, 92.0]], ('15.7', 0.9813385009765625)]\n",
      "[[[366.0, 81.0], [385.0, 81.0], [385.0, 93.0], [366.0, 93.0]], ('21.2', 0.9893273115158081)]\n",
      "[[[401.0, 81.0], [419.0, 81.0], [419.0, 93.0], [401.0, 93.0]], ('55.7', 0.9867123961448669)]\n",
      "[[[429.0, 73.0], [488.0, 73.0], [488.0, 80.0], [429.0, 80.0]], ('SymbolsMixed', 0.9974597096443176)]\n",
      "[[[108.0, 90.0], [169.0, 90.0], [169.0, 101.0], [108.0, 101.0]], ('UnreaIfext (1.2M)', 0.9539847373962402)]\n",
      "[[[213.0, 83.0], [230.0, 83.0], [230.0, 91.0], [213.0, 91.0]], ('34.6', 0.9922195672988892)]\n",
      "[[[304.0, 89.0], [321.0, 89.0], [321.0, 101.0], [304.0, 101.0]], ('44.8', 0.997839093208313)]\n",
      "[[[335.0, 89.0], [353.0, 89.0], [353.0, 100.0], [335.0, 100.0]], ('30.3', 0.9897353649139404)]\n",
      "[[[366.0, 89.0], [385.0, 89.0], [385.0, 101.0], [366.0, 101.0]], ('21.7', 0.9942774772644043)]\n",
      "[[[401.0, 89.0], [418.0, 89.0], [418.0, 101.0], [401.0, 101.0]], ('54.6', 0.9988884925842285)]\n",
      "[[[435.0, 81.0], [453.0, 81.0], [453.0, 93.0], [435.0, 93.0]], ('44.7', 0.9850949048995972)]\n",
      "[[[436.0, 89.0], [452.0, 89.0], [452.0, 101.0], [436.0, 101.0]], ('16.7', 0.8993484973907471)]\n",
      "[[[467.0, 90.0], [485.0, 87.0], [487.0, 98.0], [469.0, 101.0]], ('25.0', 0.9003407955169678)]\n",
      "[[[470.0, 81.0], [485.0, 81.0], [485.0, 93.0], [470.0, 93.0]], (' 9.8', 0.8605984449386597)]\n",
      "[[[500.0, 81.0], [517.0, 81.0], [517.0, 93.0], [500.0, 93.0]], ('6K', 0.7346737384796143)]\n",
      "[[[213.0, 91.0], [228.0, 91.0], [228.0, 99.0], [213.0, 99.0]], ('42.2', 0.9995128512382507)]\n",
      "[[[241.0, 91.0], [257.0, 91.0], [257.0, 99.0], [241.0, 99.0]], ('50.3', 0.9989340305328369)]\n",
      "[[[273.0, 91.0], [288.0, 91.0], [288.0, 99.0], [273.0, 99.0]], ('16.5', 0.9949242472648621)]\n",
      "[[[436.0, 98.0], [452.0, 98.0], [452.0, 111.0], [436.0, 111.0]], ('25.6', 0.996456503868103)]\n",
      "[[[500.0, 89.0], [516.0, 89.0], [516.0, 101.0], [500.0, 101.0]], ('36.5', 0.9969083666801453)]\n",
      "[[[103.0, 100.0], [176.0, 100.0], [176.0, 111.0], [103.0, 111.0]], ('UwrealText (fuli, 4.5M)', 0.8969516754150391)]\n",
      "[[[212.0, 101.0], [230.0, 101.0], [230.0, 109.0], [212.0, 109.0]], ('44.3', 0.9753226041793823)]\n",
      "[[[241.0, 101.0], [256.0, 101.0], [256.0, 109.0], [241.0, 109.0]], ('51.1', 0.9955915212631226)]\n",
      "[[[273.0, 100.0], [288.0, 100.0], [288.0, 110.0], [273.0, 110.0]], ('19.7', 0.8634262084960938)]\n",
      "[[[304.0, 100.0], [321.0, 100.0], [321.0, 111.0], [304.0, 111.0]], ('47.9', 0.8618850708007812)]\n",
      "[[[305.0, 108.0], [323.0, 110.0], [321.0, 122.0], [303.0, 119.0]], ('611', 0.9971955418586731)]\n",
      "[[[335.0, 100.0], [352.0, 100.0], [352.0, 111.0], [335.0, 111.0]], ('33.1', 0.9151612520217896)]\n",
      "[[[367.0, 99.0], [384.0, 99.0], [384.0, 111.0], [367.0, 111.0]], ('24.2', 0.9979429244995117)]\n",
      "[[[402.0, 99.0], [418.0, 99.0], [418.0, 111.0], [402.0, 111.0]], ('57.3', 0.9982364177703857)]\n",
      "[[[500.0, 99.0], [516.0, 99.0], [516.0, 111.0], [500.0, 111.0]], ('39.5', 0.9984886646270752)]\n",
      "[[[87.0, 118.0], [191.0, 119.0], [191.0, 130.0], [87.0, 129.0]], ('MLT19-train (90K) + ST (1.2M)', 0.998489499092102)]\n",
      "[[[108.0, 110.0], [171.0, 110.0], [171.0, 120.0], [108.0, 120.0]], ('MLT19-train (90K)', 0.9988770484924316)]\n",
      "[[[211.0, 110.0], [230.0, 110.0], [230.0, 121.0], [211.0, 121.0]], ('64.3', 0.9937793016433716)]\n",
      "[[[241.0, 111.0], [257.0, 111.0], [257.0, 119.0], [241.0, 119.0]], ('47.2', 0.99977707862854)]\n",
      "[[[271.0, 110.0], [288.0, 110.0], [288.0, 121.0], [271.0, 121.0]], ('46.9', 0.9960339069366455)]\n",
      "[[[304.0, 118.0], [320.0, 118.0], [320.0, 130.0], [304.0, 130.0]], ('50.7', 0.986512303352356)]\n",
      "[[[335.0, 110.0], [353.0, 110.0], [353.0, 121.0], [335.0, 121.0]], ('46.9', 0.9921217560768127)]\n",
      "[[[367.0, 109.0], [385.0, 109.0], [385.0, 121.0], [367.0, 121.0]], ('23.3', 0.9942470192909241)]\n",
      "[[[367.0, 118.0], [385.0, 118.0], [385.0, 130.0], [367.0, 130.0]], (' 33.9', 0.8634341359138489)]\n",
      "[[[402.0, 110.0], [419.0, 110.0], [419.0, 121.0], [402.0, 121.0]], ('39.1', 0.9975371956825256)]\n",
      "[[[402.0, 117.0], [418.0, 117.0], [418.0, 130.0], [402.0, 130.0]], ('64.5', 0.9904319643974304)]\n",
      "[[[436.0, 109.0], [453.0, 109.0], [453.0, 121.0], [436.0, 121.0]], ('35.9', 0.9772461652755737)]\n",
      "[[[437.0, 118.0], [452.0, 118.0], [452.0, 132.0], [437.0, 132.0]], ('45.5', 0.9207932949066162)]\n",
      "[[[470.0, 117.0], [485.0, 117.0], [485.0, 129.0], [470.0, 129.0]], ('10.3', 0.9915956854820251)]\n",
      "[[[471.0, 111.0], [484.0, 111.0], [484.0, 120.0], [471.0, 120.0]], ('3.6', 0.7699437737464905)]\n",
      "[[[500.0, 109.0], [516.0, 109.0], [516.0, 121.0], [500.0, 121.0]], ('45.7', 0.8991890549659729)]\n",
      "[[[241.0, 120.0], [256.0, 120.0], [256.0, 128.0], [241.0, 128.0]], ('62.0', 0.9980483055114746)]\n",
      "[[[273.0, 119.0], [288.0, 119.0], [288.0, 129.0], [273.0, 129.0]], ('48.9', 0.9985829591751099)]\n",
      "[[[306.0, 127.0], [319.0, 130.0], [317.0, 137.0], [304.0, 134.0]], ('47.7', 0.7699354887008667)]\n",
      "[[[336.0, 119.0], [351.0, 119.0], [351.0, 129.0], [336.0, 129.0]], ('47.7', 0.9980431199073792)]\n",
      "[[[500.0, 118.0], [515.0, 118.0], [515.0, 130.0], [500.0, 130.0]], ('54.7', 0.8935703039169312)]\n",
      "[[[73.0, 129.0], [205.0, 129.0], [205.0, 139.0], [73.0, 139.0]], ('MLT19-train (90K) + UnrealText (1.2M)', 0.9987233281135559)]\n",
      "[[[336.0, 130.0], [350.0, 130.0], [350.0, 137.0], [336.0, 137.0]], ('64.0', 0.9713936448097229)]\n",
      "[[[369.0, 129.0], [381.0, 129.0], [381.0, 136.0], [369.0, 136.0]], ('35.7', 0.9121072292327881)]\n",
      "[[[437.0, 129.0], [449.0, 129.0], [449.0, 136.0], [437.0, 136.0]], ('44.3', 0.8281256556510925)]\n",
      "[[[501.0, 127.0], [514.0, 130.0], [512.0, 137.0], [500.0, 134.0]], ('575', 0.591933012008667)]\n",
      "[[[47.0, 143.0], [548.0, 144.0], [548.0, 157.0], [47.0, 156.0]], ('Table 4: Multilingual scene text recognition results (word level accuracy). Latin aggregates English, French, German, and', 0.9824314713478088)]\n",
      "[[[49.0, 157.0], [283.0, 157.0], [283.0, 167.0], [49.0, 167.0]], ('Italian, as they are all marked as Latin in the MLT dataset.', 0.987904965877533)]\n",
      "[[[79.0, 187.0], [122.0, 189.0], [121.0, 200.0], [78.0, 198.0]], ('Training Data', 0.9546782374382019)]\n",
      "[[[150.0, 188.0], [293.0, 188.0], [293.0, 198.0], [150.0, 198.0]], ('IIIT  SVT  ICI5  SVTP  CUTE Total', 0.8687873482704163)]\n",
      "[[[306.0, 186.0], [380.0, 188.0], [379.0, 202.0], [306.0, 200.0]], ('7. Conclusion', 0.9915366768836975)]\n",
      "[[[78.0, 198.0], [123.0, 198.0], [123.0, 208.0], [78.0, 208.0]], ('90K [10] (1M)', 0.9777871370315552)]\n",
      "[[[81.0, 206.0], [120.0, 206.0], [120.0, 217.0], [81.0, 217.0]], ('ST [6] (1M)', 0.994734525680542)]\n",
      "[[[152.0, 199.0], [170.0, 199.0], [170.0, 207.0], [152.0, 207.0]], ('51.6', 0.9475288987159729)]\n",
      "[[[153.0, 207.0], [167.0, 207.0], [167.0, 214.0], [153.0, 214.0]], ('53.5', 0.9983519911766052)]\n",
      "[[[175.0, 207.0], [189.0, 207.0], [189.0, 214.0], [175.0, 214.0]], ('30.3', 0.9971395134925842)]\n",
      "[[[273.0, 207.0], [286.0, 207.0], [286.0, 214.0], [273.0, 214.0]], ('31.I', 0.8845221996307373)]\n",
      "[[[274.0, 199.0], [288.0, 199.0], [288.0, 206.0], [274.0, 206.0]], ('30.5', 0.9968546628952026)]\n",
      "[[[75.0, 215.0], [125.0, 215.0], [125.0, 226.0], [75.0, 226.0]], ('VISD [50] (1M)', 0.9928025007247925)]\n",
      "[[[153.0, 216.0], [168.0, 216.0], [168.0, 224.0], [153.0, 224.0]], ('53.9', 0.9988866448402405)]\n",
      "[[[175.0, 216.0], [188.0, 216.0], [188.0, 224.0], [175.0, 224.0]], ('37.1', 0.9982722401618958)]\n",
      "[[[318.0, 207.0], [547.0, 208.0], [547.0, 221.0], [318.0, 220.0]], ('In this paper, we introduce a scene text image synthesis', 0.9995003342628479)]\n",
      "[[[75.0, 222.0], [126.0, 223.0], [126.0, 234.0], [74.0, 233.0]], ('UmralText (1M)', 0.8150140047073364)]\n",
      "[[[153.0, 224.0], [168.0, 224.0], [168.0, 232.0], [153.0, 232.0]], ('54.8', 0.9706061482429504)]\n",
      "[[[175.0, 225.0], [189.0, 225.0], [189.0, 232.0], [175.0, 232.0]], ('40.3', 0.9478486776351929)]\n",
      "[[[198.0, 225.0], [211.0, 225.0], [211.0, 232.0], [198.0, 232.0]], ('39.1', 0.9587938785552979)]\n",
      "[[[222.0, 225.0], [236.0, 225.0], [236.0, 232.0], [222.0, 232.0]], ('39,6', 0.9875353574752808)]\n",
      "[[[250.0, 225.0], [263.0, 225.0], [263.0, 232.0], [250.0, 232.0]], ('31.6', 0.974945068359375)]\n",
      "[[[274.0, 224.0], [287.0, 224.0], [287.0, 232.0], [274.0, 232.0]], ('32.1', 0.9935454726219177)]\n",
      "[[[307.0, 222.0], [546.0, 222.0], [546.0, 232.0], [307.0, 232.0]], ('engine that renders images with 3D graphics engines, where', 0.9987775087356567)]\n",
      "[[[69.0, 233.0], [132.0, 233.0], [132.0, 243.0], [69.0, 243.0]], ('ST+90K(4.53  2)', 0.9823195934295654)]\n",
      "[[[151.0, 241.0], [169.0, 241.0], [169.0, 251.0], [151.0, 251.0]], ('81.6', 0.9673730731010437)]\n",
      "[[[153.0, 234.0], [170.0, 234.0], [170.0, 242.0], [153.0, 242.0]], ('80.5', 0.973220705986023)]\n",
      "[[[173.0, 241.0], [191.0, 241.0], [191.0, 251.0], [173.0, 251.0]], ('71.9', 0.9907205104827881)]\n",
      "[[[195.0, 241.0], [213.0, 241.0], [213.0, 251.0], [195.0, 251.0]], ('61.8', 0.9967172741889954)]\n",
      "[[[197.0, 235.0], [212.0, 235.0], [212.0, 242.0], [197.0, 242.0]], ('58.4', 0.9946880340576172)]\n",
      "[[[219.0, 241.0], [238.0, 241.0], [238.0, 251.0], [219.0, 251.0]], ('61.7', 0.9886282682418823)]\n",
      "[[[221.0, 234.0], [237.0, 234.0], [237.0, 242.0], [221.0, 242.0]], ('60.0', 0.995841383934021)]\n",
      "[[[246.0, 241.0], [263.0, 238.0], [265.0, 249.0], [248.0, 252.0]], ('67,7', 0.9632405042648315)]\n",
      "[[[306.0, 232.0], [546.0, 233.0], [546.0, 246.0], [306.0, 245.0]], ('text instances and scenes are rendered as a whole. In exper-', 0.9818235039710999)]\n",
      "[[[54.0, 242.0], [155.0, 242.0], [155.0, 251.0], [54.0, 251.0]], ('ST+90K+UareaIText(3M  3) |', 0.9337738752365112)]\n",
      "[[[78.0, 250.0], [124.0, 250.0], [124.0, 260.0], [78.0, 260.0]], ('ST+90K(16M)', 0.9997199773788452)]\n",
      "[[[273.0, 242.0], [289.0, 242.0], [289.0, 249.0], [273.0, 249.0]], ('45.7', 0.9967546463012695)]\n",
      "[[[306.0, 245.0], [546.0, 245.0], [546.0, 257.0], [306.0, 257.0]], ('iments, we verify the effectiveness of the proposed engine.', 0.9819700121879578)]\n",
      "[[[49.0, 264.0], [286.0, 265.0], [286.0, 278.0], [49.0, 277.0]], ('Table 5: Results on English datasets (word level accuracy).', 0.987865686416626)]\n",
      "[[[306.0, 255.0], [546.0, 255.0], [546.0, 268.0], [306.0, 268.0]], ('in both scene text detection and recognition models. We', 0.9872246980667114)]\n",
      "[[[307.0, 268.0], [547.0, 268.0], [547.0, 281.0], [307.0, 281.0]], ('also study key components of the proposed engine. We be-', 0.9955738186836243)]\n",
      "[[[306.0, 279.0], [546.0, 279.0], [546.0, 292.0], [306.0, 292.0]], ('lieve our work will be a solid stepping stone towards better', 0.9948230981826782)]\n",
      "[[[307.0, 292.0], [394.0, 292.0], [394.0, 305.0], [307.0, 305.0]], ('synthesis algorithms.', 0.9912589192390442)]\n",
      "[[[49.0, 306.0], [79.0, 306.0], [79.0, 317.0], [49.0, 317.0]], ('5.2.2', 0.946966826915741)]\n",
      "[[[75.0, 306.0], [166.0, 306.0], [166.0, 319.0], [75.0, 319.0]], ('Experiment Results', 0.9998545050621033)]\n",
      "[[[308.0, 315.0], [405.0, 315.0], [405.0, 328.0], [308.0, 328.0]], ('Acknowledgement', 0.999489963054657)]\n",
      "[[[49.0, 334.0], [288.0, 334.0], [288.0, 344.0], [49.0, 344.0]], ('Experiment results are shown in Tab. 4. When we only use', 0.9930182695388794)]\n",
      "[[[320.0, 329.0], [545.0, 329.0], [545.0, 342.0], [320.0, 342.0]], ('This research was supported by National Key R&D Pro-', 0.9962868094444275)]\n",
      "[[[48.0, 345.0], [288.0, 345.0], [288.0, 357.0], [48.0, 357.0]], ('synthetic data and control the number of images to 1.2M,', 0.9989822506904602)]\n",
      "[[[306.0, 341.0], [469.0, 340.0], [469.0, 352.0], [306.0, 353.0]], ('gram of China (No. 2017YFA0700800)', 0.9862270355224609)]\n",
      "[[[47.0, 356.0], [288.0, 356.0], [288.0, 369.0], [47.0, 369.0]], ('ours result in a considerable improvement of 1.6% in over-', 0.9928495287895203)]\n",
      "[[[47.0, 367.0], [288.0, 368.0], [288.0, 381.0], [47.0, 380.0]], ('all accuracy, and significant improvements on some scripts,', 0.9963371157646179)]\n",
      "[[[307.0, 362.0], [366.0, 364.0], [365.0, 375.0], [307.0, 373.0]], ('References', 0.9950265884399414)]\n",
      "[[[48.0, 380.0], [289.0, 380.0], [289.0, 393.0], [48.0, 393.0]], ('e.g. Latin (+7.6%) and Mixed (+21.6%). Using the whole', 0.9766556620597839)]\n",
      "[[[312.0, 381.0], [546.0, 381.0], [546.0, 394.0], [312.0, 394.0]], ('[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-', 0.9874000549316406)]\n",
      "[[[49.0, 394.0], [286.0, 394.0], [286.0, 404.0], [49.0, 404.0]], ('training set of 4.1M images further improves overall accu-', 0.9845511317253113)]\n",
      "[[[327.0, 394.0], [545.0, 394.0], [545.0, 404.0], [327.0, 404.0]], ('tendra Malik. Contour detection and hierarchical image seg-', 0.9766406416893005)]\n",
      "[[[48.0, 404.0], [288.0, 404.0], [288.0, 417.0], [48.0, 417.0]], ('racy to 39.5%. When we train models on combinations of', 0.9819745421409607)]\n",
      "[[[328.0, 405.0], [545.0, 405.0], [545.0, 415.0], [328.0, 415.0]], ('mentation. IEEE transactions on pattern analysis and ma-', 0.9952812194824219)]\n",
      "[[[49.0, 418.0], [288.0, 418.0], [288.0, 428.0], [49.0, 428.0]], ('synthetic data and our training split of MLT19, as shown', 0.9920511245727539)]\n",
      "[[[328.0, 416.0], [474.0, 416.0], [474.0, 426.0], [328.0, 426.0]], ('chine intelligence, 33(5):898916, 2011', 0.9811114072799683)]\n",
      "[[[48.0, 430.0], [286.0, 430.0], [286.0, 440.0], [48.0, 440.0]], ('in the bottom of Tab. 4, we can still observe a considerable', 0.9871273040771484)]\n",
      "[[[312.0, 426.0], [547.0, 426.0], [547.0, 439.0], [312.0, 439.0]], ('[2] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun,', 0.9825107455253601)]\n",
      "[[[47.0, 441.0], [288.0, 440.0], [288.0, 452.0], [47.0, 453.0]], ('margin of our method over SynthText by 3.2% in overall ac-.', 0.9776585698127747)]\n",
      "[[[325.0, 436.0], [545.0, 437.0], [545.0, 449.0], [325.0, 448.0]], ('and Hwalsuk Lee. Character region awareness for text detec-', 0.9969094395637512)]\n",
      "[[[48.0, 452.0], [289.0, 452.0], [289.0, 465.0], [48.0, 465.0]], ('curacy. The experiment results demonstrate that our method', 0.9997679591178894)]\n",
      "[[[326.0, 447.0], [547.0, 447.0], [547.0, 460.0], [326.0, 460.0]], ('tion. In Proceedings of the IEEE Conference on Computer', 0.9909278154373169)]\n",
      "[[[47.0, 464.0], [289.0, 464.0], [289.0, 477.0], [47.0, 477.0]], ('is also superior in multilingual scene text recognition, and.', 0.9768027067184448)]\n",
      "[[[327.0, 458.0], [547.0, 458.0], [547.0, 471.0], [327.0, 471.0]], ('Vision and Pattern Recognition (CVPR), pages 93659374,', 0.9971068501472473)]\n",
      "[[[48.0, 476.0], [287.0, 476.0], [287.0, 489.0], [48.0, 489.0]], ('we believe this result will become a stepping stone to fur-', 0.9872472882270813)]\n",
      "[[[312.0, 480.0], [546.0, 480.0], [546.0, 493.0], [312.0, 493.0]], ('[3] Zhanzhan Cheng, Xuyang Liu, Fan Bai, Yi Niu, Shiliang Pu,', 0.9930537343025208)]\n",
      "[[[328.0, 471.0], [348.0, 471.0], [348.0, 479.0], [328.0, 479.0]], ('2019', 0.9990489482879639)]\n",
      "[[[47.0, 488.0], [105.0, 489.0], [105.0, 500.0], [47.0, 499.0]], ('ther research.', 0.9990967512130737)]\n",
      "[[[327.0, 492.0], [546.0, 492.0], [546.0, 505.0], [327.0, 505.0]], ('and Shuigeng Zhou.Arbitrarily-oriented text recognition.', 0.9872889518737793)]\n",
      "[[[328.0, 504.0], [395.0, 504.0], [395.0, 514.0], [328.0, 514.0]], ('CVPR2018, 2017.', 0.9707102179527283)]\n",
      "[[[47.0, 523.0], [210.0, 522.0], [210.0, 536.0], [47.0, 537.0]], ('6. Limitation and Future Work', 0.9844474792480469)]\n",
      "[[[312.0, 515.0], [545.0, 515.0], [545.0, 528.0], [312.0, 528.0]], ('[4] Chee Kheng Chng and Chee Seng Chan. Total-text: A com-', 0.9866626858711243)]\n",
      "[[[327.0, 526.0], [546.0, 526.0], [546.0, 539.0], [327.0, 539.0]], ('prehensive dataset for scene text detection and recognition.', 0.9721131920814514)]\n",
      "[[[327.0, 537.0], [508.0, 537.0], [508.0, 549.0], [327.0, 549.0]], ('In Proc. ICDAR, volume 1, pages 935942, 2017.', 0.9860209226608276)]\n",
      "[[[312.0, 547.0], [547.0, 547.0], [547.0, 560.0], [312.0, 560.0]], ('[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing', 0.9928474426269531)]\n",
      "[[[48.0, 559.0], [288.0, 559.0], [288.0, 572.0], [48.0, 572.0]], ('into: (1) Overall, the engine is based on rules and human-', 0.996085524559021)]\n",
      "[[[328.0, 560.0], [546.0, 560.0], [546.0, 570.0], [328.0, 570.0]], ('Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and', 0.9894397258758545)]\n",
      "[[[49.0, 572.0], [288.0, 572.0], [288.0, 582.0], [49.0, 582.0]], ('selected parameters. The automation of the selection and', 0.977497935295105)]\n",
      "[[[328.0, 571.0], [546.0, 571.0], [546.0, 581.0], [328.0, 581.0]], ('Yoshua Bengio. Generative adversarial nets. In Proc. NIPS,', 0.9933879971504211)]\n",
      "[[[49.0, 584.0], [287.0, 584.0], [287.0, 594.0], [49.0, 594.0]], ('search for these parameters can save human efforts and help', 0.9989974498748779)]\n",
      "[[[311.0, 591.0], [546.0, 592.0], [546.0, 605.0], [311.0, 604.0]], ('[6] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman.', 0.986922025680542)]\n",
      "[[[327.0, 582.0], [417.0, 580.0], [417.0, 591.0], [327.0, 593.0]], ('pages 26722680, 2014.', 0.9936875700950623)]\n",
      "[[[48.0, 593.0], [288.0, 593.0], [288.0, 606.0], [48.0, 606.0]], ('adapt to different scenarios. (2) While rendering small text', 0.9993144869804382)]\n",
      "[[[48.0, 608.0], [288.0, 608.0], [288.0, 618.0], [48.0, 618.0]], ('can help training detectors, the low image quality of the', 0.9981454014778137)]\n",
      "[[[328.0, 604.0], [546.0, 604.0], [546.0, 614.0], [328.0, 614.0]], ('Synthetic data for text localisation in natural images. In Proc.', 0.9998161792755127)]\n",
      "[[[49.0, 619.0], [286.0, 619.0], [286.0, 629.0], [49.0, 629.0]], ('small text makes recognizers harder to train and harms the', 0.9869276881217957)]\n",
      "[[[328.0, 614.0], [445.0, 614.0], [445.0, 627.0], [328.0, 627.0]], ('CVPR, pages 23152324, 2016.', 0.9945265650749207)]\n",
      "[[[48.0, 631.0], [289.0, 631.0], [289.0, 644.0], [48.0, 644.0]], ('performance. Designing a method to mark the illegible ones', 0.9789465069770813)]\n",
      "[[[312.0, 626.0], [546.0, 626.0], [546.0, 639.0], [312.0, 639.0]], ('[7] Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao,', 0.9782862663269043)]\n",
      "[[[48.0, 642.0], [287.0, 642.0], [287.0, 654.0], [48.0, 654.0]], ('as difficalt and excluding them from loss calculation may.', 0.9818129539489746)]\n",
      "[[[326.0, 647.0], [547.0, 647.0], [547.0, 660.0], [326.0, 660.0]], ('alignment and attention. In Proc. CVPR, pages 50205029,', 0.9940409660339355)]\n",
      "[[[328.0, 639.0], [546.0, 639.0], [546.0, 648.0], [328.0, 648.0]], ('and Changming Sun. An end-to-end textspotter with explicit', 0.9905989170074463)]\n",
      "[[[47.0, 653.0], [287.0, 653.0], [287.0, 666.0], [47.0, 666.0]], ('help mitigate this problem. (3) For multilingual scene text,', 0.9970542788505554)]\n",
      "[[[48.0, 665.0], [287.0, 665.0], [287.0, 678.0], [48.0, 678.0]], ('scripts except Latin have much fewer available fonts that we', 0.9824055433273315)]\n",
      "[[[311.0, 668.0], [546.0, 668.0], [546.0, 681.0], [311.0, 681.0]], ('[8] Stefan Hinterstoisser, Olivier Pauly, Hauke Heibel, Martina', 0.9876622557640076)]\n",
      "[[[327.0, 659.0], [350.0, 659.0], [350.0, 670.0], [327.0, 670.0]], ('2018.', 0.9992104768753052)]\n",
      "[[[49.0, 679.0], [286.0, 679.0], [286.0, 689.0], [49.0, 689.0]], ('have easy access to. To improve performance on more lan-', 0.9878588914871216)]\n",
      "[[[47.0, 691.0], [288.0, 690.0], [288.0, 703.0], [47.0, 704.0]], ('guages, researchers may consider learning-based methods', 0.9869220852851868)]\n",
      "[[[328.0, 682.0], [545.0, 682.0], [545.0, 692.0], [328.0, 692.0]], ('Marek, and Martin Bokeloh. An annotation saved is an an-', 0.9893880486488342)]\n",
      "[[[328.0, 693.0], [545.0, 693.0], [545.0, 703.0], [328.0, 703.0]], ('notation earned: Using fully synthetic training for object in-', 0.9907850623130798)]\n",
      "[[[48.0, 704.0], [201.0, 704.0], [201.0, 714.0], [48.0, 714.0]], ('to transfer Latin fonts to other scripts.', 0.9870771765708923)]\n",
      "[[[327.0, 704.0], [499.0, 704.0], [499.0, 714.0], [327.0, 714.0]], ('stance detection. CoRR, abs/1902.09967, 2019.', 0.9989738464355469)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from paddleocr import PaddleOCR, draw_ocr\n",
    "from PIL import Image\n",
    "\n",
    "ocr = PaddleOCR(use_angle_cls=True, lang='en')\n",
    "\n",
    "img_folder = './pdf_images'  \n",
    "font_path = './PaddleOCR/doc/fonts/Ubuntu-L.ttf'  \n",
    "output_folder = './output/annotations' \n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for img_file in os.listdir(img_folder):\n",
    "    if img_file.endswith('.png'): \n",
    "        image_path = os.path.join(img_folder, img_file)\n",
    "        \n",
    "        ocr_result = ocr.ocr(image_path, cls=True)\n",
    "       \n",
    "        for idx in range(len(ocr_result)):\n",
    "            res = ocr_result[idx]\n",
    "            for line in res:\n",
    "                print(line)\n",
    "       \n",
    "        result = ocr_result[0]\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        boxes = [elements[0] for elements in result]\n",
    "        txts = [elements[1][0] for elements in result]\n",
    "        scores = [elements[1][1] for elements in result]\n",
    "\n",
    "        im_show = draw_ocr(image, boxes, txts, scores, font_path=font_path)\n",
    "        im_show = Image.fromarray(im_show)\n",
    "\n",
    "        output_image_path = os.path.join(output_folder, f\"{os.path.basename(image_path).split('.')[0]}_annotation.png\")\n",
    "        im_show.save(output_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a47e49b",
   "metadata": {},
   "source": [
    "#### Process and extract table structure from `.png` images in `pdf_images` using PaddleOCR's PPStructure, save results as JSON and annotated images in `output/json` and `output/` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "166f2d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/10/01 21:26:05] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/Users/dhushanthankumararatnam/.paddleocr/whl/det/ch/ch_PP-OCRv4_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/Users/dhushanthankumararatnam/.paddleocr/whl/rec/ch/ch_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages/paddleocr/ppocr/utils/ppocr_keys_v1.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=False, cls_model_dir=None, cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir='/Users/dhushanthankumararatnam/.paddleocr/whl/table/ch_ppstructure_mobile_v2.0_SLANet_infer', merge_no_span_structure=True, table_char_dict_path='/Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages/paddleocr/ppocr/utils/dict/table_structure_dict_ch.txt', layout_model_dir='/Users/dhushanthankumararatnam/.paddleocr/whl/layout/picodet_lcnet_x1_0_fgd_layout_cdla_infer', layout_dict_path='/Users/dhushanthankumararatnam/miniconda3/envs/VisionEnv/lib/python3.11/site-packages/paddleocr/ppocr/utils/dict/layout_dict/layout_cdla_dict.txt', layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, ocr=True, recovery=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='ch', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n",
      "[2024/10/01 21:26:07] ppocr DEBUG: dt_boxes num : 104, elapsed : 0.3807969093322754\n",
      "[2024/10/01 21:26:31] ppocr DEBUG: rec_res num  : 104, elapsed : 23.27687978744507\n",
      "[2024/10/01 21:26:32] ppocr DEBUG: dt_boxes num : 99, elapsed : 0.3153958320617676\n",
      "[2024/10/01 21:26:52] ppocr DEBUG: rec_res num  : 99, elapsed : 20.51331901550293\n",
      "[2024/10/01 21:26:53] ppocr DEBUG: dt_boxes num : 86, elapsed : 0.3199172019958496\n",
      "[2024/10/01 21:27:09] ppocr DEBUG: rec_res num  : 86, elapsed : 15.999110221862793\n",
      "[2024/10/01 21:27:10] ppocr DEBUG: dt_boxes num : 59, elapsed : 0.31980276107788086\n",
      "[2024/10/01 21:27:23] ppocr DEBUG: rec_res num  : 59, elapsed : 13.381146669387817\n",
      "[2024/10/01 21:27:24] ppocr DEBUG: dt_boxes num : 93, elapsed : 0.32740187644958496\n",
      "[2024/10/01 21:27:42] ppocr DEBUG: rec_res num  : 93, elapsed : 17.459993839263916\n",
      "[2024/10/01 21:27:43] ppocr DEBUG: dt_boxes num : 181, elapsed : 0.3303699493408203\n",
      "[2024/10/01 21:28:06] ppocr DEBUG: rec_res num  : 181, elapsed : 23.477046012878418\n",
      "[2024/10/01 21:28:07] ppocr DEBUG: dt_boxes num : 83, elapse : 0.06384587287902832\n",
      "[2024/10/01 21:28:13] ppocr DEBUG: rec_res num  : 83, elapse : 6.379813194274902\n",
      "[2024/10/01 21:28:14] ppocr DEBUG: dt_boxes num : 127, elapsed : 0.36527276039123535\n",
      "[2024/10/01 21:28:37] ppocr DEBUG: rec_res num  : 127, elapsed : 22.090091943740845\n",
      "[2024/10/01 21:28:37] ppocr DEBUG: dt_boxes num : 35, elapse : 0.04203605651855469\n",
      "[2024/10/01 21:28:40] ppocr DEBUG: rec_res num  : 35, elapse : 2.76790714263916\n",
      "[2024/10/01 21:28:41] ppocr DEBUG: dt_boxes num : 114, elapsed : 0.32708001136779785\n",
      "[2024/10/01 21:29:02] ppocr DEBUG: rec_res num  : 114, elapsed : 21.273214101791382\n",
      "[2024/10/01 21:29:03] ppocr DEBUG: dt_boxes num : 68, elapsed : 0.3280220031738281\n",
      "[2024/10/01 21:29:16] ppocr DEBUG: rec_res num  : 68, elapsed : 12.765326023101807\n",
      "[2024/10/01 21:29:17] ppocr DEBUG: dt_boxes num : 86, elapsed : 0.3321981430053711\n",
      "[2024/10/01 21:29:30] ppocr DEBUG: rec_res num  : 86, elapsed : 13.577187061309814\n",
      "[2024/10/01 21:29:31] ppocr DEBUG: dt_boxes num : 70, elapsed : 0.344404935836792\n",
      "[2024/10/01 21:29:43] ppocr DEBUG: rec_res num  : 70, elapsed : 11.849762916564941\n",
      "[2024/10/01 21:29:44] ppocr DEBUG: dt_boxes num : 89, elapsed : 0.33736705780029297\n",
      "[2024/10/01 21:30:02] ppocr DEBUG: rec_res num  : 89, elapsed : 17.959954977035522\n",
      "[2024/10/01 21:30:03] ppocr DEBUG: dt_boxes num : 194, elapsed : 0.34899210929870605\n",
      "[2024/10/01 21:30:28] ppocr DEBUG: rec_res num  : 194, elapsed : 24.643811225891113\n",
      "[2024/10/01 21:30:28] ppocr DEBUG: dt_boxes num : 24, elapse : 0.030597209930419922\n",
      "[2024/10/01 21:30:30] ppocr DEBUG: rec_res num  : 24, elapse : 1.922868013381958\n",
      "[2024/10/01 21:30:31] ppocr DEBUG: dt_boxes num : 28, elapse : 0.025641918182373047\n",
      "[2024/10/01 21:30:33] ppocr DEBUG: rec_res num  : 28, elapse : 2.2612040042877197\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from paddleocr import PPStructure, draw_structure_result, save_structure_res\n",
    "from PIL import Image\n",
    "\n",
    "table_engine = PPStructure(show_log=True)\n",
    "\n",
    "save_folder = './output'\n",
    "img_folder = 'pdf_images'\n",
    "json_folder = './output/json'  \n",
    "font_path = 'PaddleOCR/doc/fonts/simfang.ttf' \n",
    "\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "if not os.path.exists(json_folder):\n",
    "    os.makedirs(json_folder)\n",
    "\n",
    "for img_file in os.listdir(img_folder):\n",
    "    if img_file.endswith('.png'): \n",
    "        img_path = os.path.join(img_folder, img_file)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        result = table_engine(img)\n",
    "\n",
    "        save_structure_res(result, save_folder, os.path.basename(img_path).split('.')[0])\n",
    "\n",
    "        for line in result:\n",
    "            line.pop('img')\n",
    "        \n",
    "        json_file_path = os.path.join(json_folder, f'{os.path.basename(img_path).split(\".\")[0]}.json')\n",
    "        with open(json_file_path, 'w') as json_file:\n",
    "            json.dump(result, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        im_show = draw_structure_result(image, result, font_path=font_path)\n",
    "        im_show = Image.fromarray(im_show)\n",
    "        im_show.save(os.path.join(save_folder, f'{os.path.basename(img_path).split(\".\")[0]}_result.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf95a66-b6a0-42c0-b1c2-90f459dcb248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0f793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
