Figure 2: The pipeline of the proposed synthesis method. The arrows indicate the order. For simplicity, we only show one text
region. From left to right: scene overview, diverse viewpoints, various lighting conditions (light color, intensity, shadows,
etc.), text region generation and text rendering.
In each step, we ﬁrst randomly change the pitch and yaw
values of the camera rotation, making the camera pointing
to a new direction. Then, we cast a ray from the camera lo-
cation towards the direction of the viewpoint. The ray stops
when it hits any object meshes or reaches a ﬁxed maximum
length. By design, the path from the current location to the
stopping position is free of any barrier, i.e. not inside of
any object meshes. Therefore, points along this ray path are
all valid. Finally, we randomly sample one point between
the 1
3-th and 2
3-th of this path, and set it as the new location
of the camera, which is non-trivial. The proposed random
walk algorithm can generate diverse camera viewpoints.
3.2.2
Auxiliary Camera Anchors
The proposed random walk algorithm, however, is inefﬁ-
cient in terms of exploration. Therefore, we manually select
a set of N camera anchors across the 3D scenes as start-
ing points. After every T steps, we reset the location of
the camera to a randomly sampled camera anchor. We set
N = 150-200 and T = 100. Note that the selection of cam-
era anchors requires only little carefulness. We only need to
ensure coverage over the space. It takes around 20 to 30 sec-
onds for each scene, which is trivial and not a bottleneck of
scalability. The manual but efﬁcient selection of camera is
compatible with the proposed random walk algorithm that
generates diverse viewpoints.
3.3. Environment Randomization
To produce real-world variations such as lighting condi-
tions, we randomly change the intensity, color, and direction
of all light sources in the scene. In addition to illuminations,
we also add fog conditions and randomly adjust its inten-
sity. The environment randomization proves to increase the
Figure 3:
In the ﬁrst row (1)-(4), we illustrate the
physically-constrained 3D random walk. For better visu-
alization, we use a camera object to represent the viewpoint
(marked with green boxes and arrows). In the second row,
we compare viewpoints from the proposed method with ran-
domly sampled viewpoints.
diversity of the generated images and results in stronger de-
tector performance. The proposed randomization can also
beneﬁt sim-to-real domain adaptation [41].
3.4. Text Region Generation
In real-world, text instances are usually embedded on
well-deﬁned surfaces, e.g. trafﬁc signs, to maintain good
legibility. Previous works ﬁnd suitable regions by using es-
timated scene information, such as gPb-UCM [1] in Syn-
thText [6] or saliency map in VISD [50] for approxima-
tion. However, these methods are imprecise and often fail
to ﬁnd appropriate regions. Therefore, we propose to ﬁnd
text regions by probing around object meshes in 3D world.
Since inspecting all object meshes is time-consuming, we
propose a 2-staged pipeline: (1) We retrieve ground truth
surface normal map to generate initial text region propos-
